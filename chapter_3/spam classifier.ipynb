{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "HAM_DIR = \"easy_ham\"\n",
    "SPAM_DIR = \"spam_2\"\n",
    "ham_filenames = [name for name in sorted(os.listdir(HAM_DIR)) if len(name)>20]\n",
    "spam_filenames = [name for name in sorted(os.listdir(SPAM_DIR)) if len(name)>20]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2551 1396\n"
     ]
    }
   ],
   "source": [
    "print(len(ham_filenames), len( spam_filenames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python email module to parse the emails\n",
    "import email\n",
    "import email.policy\n",
    "\n",
    "def load_email(directory, filename, spam_path=SPAM_PATH):\n",
    "    with open(os.path.join(directory,filename), \"rb\") as f:\n",
    "        return email.parser.BytesParser(policy=email.policy.default).parse(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "ham_emails = [load_email(\"easy_ham\", name) for name in ham_filenames ]\n",
    "spam_emails = [load_email(\"spam_2\", name) for name in spam_filenames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Martin A posted:\\nTassos Papadopoulos, the Greek sculptor behind the plan, judged that the\\n limestone of Mount Kerdylio, 70 miles east of Salonika and not far from the\\n Mount Athos monastic community, was ideal for the patriotic sculpture. \\n \\n As well as Alexander's granite features, 240 ft high and 170 ft wide, a\\n museum, a restored amphitheatre and car park for admiring crowds are\\nplanned\\n---------------------\\nSo is this mountain limestone or granite?\\nIf it's limestone, it'll weather pretty fast.\\n\\n------------------------ Yahoo! Groups Sponsor ---------------------~-->\\n4 DVDs Free +s&p Join Now\\nhttp://us.click.yahoo.com/pt6YBB/NXiEAA/mG3HAA/7gSolB/TM\\n---------------------------------------------------------------------~->\\n\\nTo unsubscribe from this group, send an email to:\\nforteana-unsubscribe@egroups.com\\n\\n \\n\\nYour use of Yahoo! Groups is subject to http://docs.yahoo.com/info/terms/\""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reading the content of email\n",
    "ham_emails[1].get_content().strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looking at email structure\n",
    "\n",
    "def get_email_structure(email):\n",
    "    if(isinstance(email, str)):\n",
    "        return email\n",
    "    payload = email.get_payload()\n",
    "    if(isinstance(payload, list)):\n",
    "        return \"multipart({})\".format(\", \".join([\n",
    "            get_email_structure(sub_mail)\n",
    "            for sub_mail in payload\n",
    "        ]))\n",
    "    else :\n",
    "        return email.get_content_type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def structures_counter(emails):\n",
    "    structures = Counter()\n",
    "    for email in emails:\n",
    "        structure = get_email_structure(email)\n",
    "        structures[structure] += 1\n",
    "    return structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('text/plain', 2453),\n",
       " ('multipart(text/plain, application/pgp-signature)', 72),\n",
       " ('multipart(text/plain, text/html)', 8),\n",
       " ('multipart(text/plain, text/plain)', 4),\n",
       " ('multipart(text/plain)', 3),\n",
       " ('multipart(text/plain, application/octet-stream)', 2),\n",
       " ('multipart(text/plain, text/enriched)', 1),\n",
       " ('multipart(text/plain, application/ms-tnef, text/plain)', 1),\n",
       " ('multipart(multipart(text/plain, text/plain, text/plain), application/pgp-signature)',\n",
       "  1),\n",
       " ('multipart(text/plain, video/mng)', 1),\n",
       " ('multipart(text/plain, multipart(text/plain))', 1),\n",
       " ('multipart(text/plain, application/x-pkcs7-signature)', 1),\n",
       " ('multipart(text/plain, multipart(text/plain, text/plain), text/rfc822-headers)',\n",
       "  1),\n",
       " ('multipart(text/plain, multipart(text/plain, text/plain), multipart(multipart(text/plain, application/x-pkcs7-signature)))',\n",
       "  1),\n",
       " ('multipart(text/plain, application/x-java-applet)', 1)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "structures_counter(ham_emails).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('text/plain', 597),\n",
       " ('text/html', 589),\n",
       " ('multipart(text/plain, text/html)', 114),\n",
       " ('multipart(text/html)', 29),\n",
       " ('multipart(text/plain)', 25),\n",
       " ('multipart(multipart(text/html))', 18),\n",
       " ('multipart(multipart(text/plain, text/html))', 5),\n",
       " ('multipart(text/plain, application/octet-stream, text/plain)', 3),\n",
       " ('multipart(text/html, text/plain)', 2),\n",
       " ('multipart(text/html, image/jpeg)', 2),\n",
       " ('multipart(multipart(text/plain), application/octet-stream)', 2),\n",
       " ('multipart(text/plain, application/octet-stream)', 2),\n",
       " ('multipart(text/plain, multipart(text/plain))', 1),\n",
       " ('multipart(multipart(text/plain, text/html), image/jpeg, image/jpeg, image/jpeg, image/jpeg, image/jpeg)',\n",
       "  1),\n",
       " ('multipart(multipart(text/plain, text/html), image/jpeg, image/jpeg, image/jpeg, image/jpeg, image/gif)',\n",
       "  1),\n",
       " ('text/plain charset=us-ascii', 1),\n",
       " ('multipart(multipart(text/html), image/gif)', 1),\n",
       " ('multipart(multipart(text/plain, text/html), application/octet-stream, application/octet-stream, application/octet-stream, application/octet-stream)',\n",
       "  1),\n",
       " ('multipart(multipart(text/plain, text/html), image/gif, image/jpeg)', 1),\n",
       " ('multipart/alternative', 1)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "structures_counter(spam_emails).most_common()\n",
    "\n",
    "# it seems spam has lot of html elements compared to ham"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Return-Path : <ilug-admin@linux.ie>\n",
      "Delivered-To : yyyy@localhost.netnoteinc.com\n",
      "Received : from localhost (localhost [127.0.0.1])\tby phobos.labs.netnoteinc.com (Postfix) with ESMTP id 9E1F5441DD\tfor <jm@localhost>; Tue,  6 Aug 2002 06:48:09 -0400 (EDT)\n",
      "Received : from phobos [127.0.0.1]\tby localhost with IMAP (fetchmail-5.9.0)\tfor jm@localhost (single-drop); Tue, 06 Aug 2002 11:48:09 +0100 (IST)\n",
      "Received : from lugh.tuatha.org (root@lugh.tuatha.org [194.125.145.45]) by    dogma.slashnull.org (8.11.6/8.11.6) with ESMTP id g72LqWv13294 for    <jm-ilug@jmason.org>; Fri, 2 Aug 2002 22:52:32 +0100\n",
      "Received : from lugh (root@localhost [127.0.0.1]) by lugh.tuatha.org    (8.9.3/8.9.3) with ESMTP id WAA31224; Fri, 2 Aug 2002 22:50:17 +0100\n",
      "Received : from bettyjagessar.com (w142.z064000057.nyc-ny.dsl.cnc.net    [64.0.57.142]) by lugh.tuatha.org (8.9.3/8.9.3) with ESMTP id WAA31201 for    <ilug@linux.ie>; Fri, 2 Aug 2002 22:50:11 +0100\n",
      "X-Authentication-Warning : lugh.tuatha.org: Host w142.z064000057.nyc-ny.dsl.cnc.net    [64.0.57.142] claimed to be bettyjagessar.com\n",
      "Received : from 64.0.57.142 [202.63.165.34] by bettyjagessar.com    (SMTPD32-7.06 EVAL) id A42A7FC01F2; Fri, 02 Aug 2002 02:18:18 -0400\n",
      "Message-Id : <1028311679.886@0.57.142>\n",
      "Date : Fri, 02 Aug 2002 23:37:59 +0530\n",
      "To : ilug@linux.ie\n",
      "From : Start Now <startnow2002@hotmail.com>\n",
      "MIME-Version : 1.0\n",
      "Content-Type : text/plain; charset=\"US-ASCII\"; format=\"flowed\"\n",
      "Subject : [ILUG] STOP THE MLM INSANITY\n",
      "Sender : ilug-admin@linux.ie\n",
      "Errors-To : ilug-admin@linux.ie\n",
      "X-Mailman-Version : 1.1\n",
      "Precedence : bulk\n",
      "List-Id : Irish Linux Users' Group <ilug.linux.ie>\n",
      "X-Beenthere : ilug@linux.ie\n"
     ]
    }
   ],
   "source": [
    "# looking at email headers\n",
    "\n",
    "for header, value in spam_emails[0].items():\n",
    "    print(header, \":\", value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[ILUG] STOP THE MLM INSANITY'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lot of headers look fishy\n",
    "# but we can limit ourseleves to subject\n",
    "\n",
    "spam_emails[0][\"Subject\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### splitting to train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = np.array(ham_emails +spam_emails)\n",
    "y = np.array([0] * len(ham_emails) + [1] * len(spam_emails))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using regex to parse html\n",
    "# we could have used beautiful soup but no we to take the scourge\n",
    "\n",
    "import re\n",
    "from html import unescape\n",
    "\n",
    "def html_to_plain_text(html):\n",
    "    text = re.sub('<head.*?>.*?</head>', '', html, flags=re.M | re.I | re.S)\n",
    "    # remove head section\n",
    "    \n",
    "    text = re.sub('<style.*?>.*?</style>', '', html, flags=re.M | re.I | re.S)\n",
    "    # remove style\n",
    "    \n",
    "    text = re.sub('<a\\s.*?>', ' HYPERLINK ',text, flags= re.M | re.S | re.I)\n",
    "    # replace links with hyperlink\n",
    "    \n",
    "    text = re.sub('<.*?>', '',text, flags=re.M | re.S)\n",
    "    # remove all html tags\n",
    "    \n",
    "    text = re.sub(r'(\\s*\\n)+', '\\n', text,flags=re.M | re.S)\n",
    "    # replace multiple new line with single newline\n",
    "    \n",
    "    text = re.sub(r'(\\s*\\t)+', ' ', text,flags=re.M | re.S)\n",
    "    # replace multiple tabs with single space\n",
    "    \n",
    "    return unescape(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# important line\n",
    "# if statement and\n",
    "# conditional array selector\n",
    "\n",
    "html_spam_emails = [email for email in X_train[y_train==1]\n",
    "                    if get_email_structure(email) == \"text/html\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html><body onload=\"window.open('http://202.101.163.34:81/ultimatehgh_run/')\" bgColor=\"#CCFF66\" topmargin=1 onMouseOver=\"window.status=''; return true\" oncontextmenu=\"return false\" ondragstart=\"return false\" onselectstart=\"return false\">\n",
      "<div align=\"center\">Hello, jm@netnoteinc.com<BR><BR></div><div align=\"center\"></div><p align=\"center\"><b><font face=\"Arial\" size=\"4\">Human Growth Hormone Therapy</font></b></p>\n",
      "<p align=\"center\"><b><font face=\"Arial\" size=\"4\">Lose weight while building lean muscle mass<br>and reversing the ravages of aging all at once.</font><font face=\"Arial\" size=\"3\"><br>\n",
      "</font></b><font face=\"Arial\" size=\"3\"> <br>\n",
      "As seen on NBC, CBS, and CNN, and even Oprah! The health<br>\n",
      "discovery that actually reverses aging while burning fat,<br>\n",
      "without dieting or exercise! This proven discovery has even<br>\n",
      "been reported on by the New England Journal of Medicine.<br>\n",
      "Forget aging and dieting forever! And it's Guaranteed!</font></p>\n",
      "<center><table width=\"481\"><tr><td height=\" ...\n"
     ]
    }
   ],
   "source": [
    "sample_html_spam = html_spam_emails[8]\n",
    "print(sample_html_spam.get_content().strip()[:1000], \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hello, jm@netnoteinc.comHuman Growth Hormone Therapy\n",
      "Lose weight while building lean muscle massand reversing the ravages of aging all at once.\n",
      "As seen on NBC, CBS, and CNN, and even Oprah! The health\n",
      "discovery that actually reverses aging while burning fat,\n",
      "without dieting or exercise! This proven discovery has even\n",
      "been reported on by the New England Journal of Medicine.\n",
      "Forget aging and dieting forever! And it's Guaranteed!\n",
      "Lose WeightBuild Muscle ToneReverse Aging\n",
      "Increased LibidoDuration Of Penile ErectionHealthier Bones\n",
      "Improved MemoryImproved skinNew Hair GrowthWrinkle Disappearance\n",
      " HYPERLINK Visit\n",
      "  Our Web Site and Learn The Facts : Click Here\n",
      "   HYPERLINK OR\n",
      "  Here\n",
      "  You are receiving this email as a subscriber\n",
      "  to the Opt-In America Mailing List.\n",
      "To remove yourself from all related maillists,just  HYPERLINK Click Here\n",
      " ...\n"
     ]
    }
   ],
   "source": [
    "# after applying our function to create plaintext\n",
    "print(html_to_plain_text(sample_html_spam.get_content())[:1000], \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a email function to return content as plaintext\n",
    "# regardless of content\n",
    "\n",
    "def email_to_text(email):\n",
    "    html = None\n",
    "    for part in email.walk():\n",
    "        ctype = part.get_content_type()\n",
    "        if not ctype in (\"text/plain\", \"text/html\"):\n",
    "            continue\n",
    "        try:\n",
    "            content = part.get_content()\n",
    "        except:\n",
    "            content = str(part.get_payload())\n",
    "        if ctype == \"text/plain\":\n",
    "            return content\n",
    "        else :\n",
    "            html = content\n",
    "    if html:\n",
    "        return html_to_plain_text(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hello, jm@netnoteinc.comHuman Growth Hormone Therapy\n",
      "Lose weight while building lean muscle massand ...\n"
     ]
    }
   ],
   "source": [
    "print(email_to_text(sample_html_spam)[:100], \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computations => comput\n",
      "Computation => comput\n",
      "Computing => comput\n",
      "Computed => comput\n",
      "Compute => comput\n",
      "Compulsive => compuls\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "stemmer = nltk.PorterStemmer()\n",
    "for word in (\"Computations\", \"Computation\", \"Computing\", \"Computed\", \"Compute\", \"Compulsive\" ):\n",
    "    print(word, \"=>\", stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting urlextract\n",
      "  Downloading https://files.pythonhosted.org/packages/06/db/23b47f32d990dea1d9852ace16d551a0003bdfc8be33094cfd208757466e/urlextract-0.14.0-py3-none-any.whl\n",
      "Collecting appdirs (from urlextract)\n",
      "  Downloading https://files.pythonhosted.org/packages/56/eb/810e700ed1349edde4cbdc1b2a21e28cdf115f9faf263f6bbf8447c1abf3/appdirs-1.4.3-py2.py3-none-any.whl\n",
      "Requirement already satisfied: idna in c:\\installs\\anaconda\\envs\\tf_gpu\\lib\\site-packages (from urlextract) (2.8)\n",
      "Collecting uritools (from urlextract)\n",
      "  Downloading https://files.pythonhosted.org/packages/eb/1a/5995c0a000ef116111b9af9303349ba97ec2446d2c9a79d2df028a3e3b19/uritools-3.0.0-py3-none-any.whl\n",
      "Installing collected packages: appdirs, uritools, urlextract\n",
      "Successfully installed appdirs-1.4.3 uritools-3.0.0 urlextract-0.14.0\n"
     ]
    }
   ],
   "source": [
    "#!pip install urlextract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['github.com/sas/sa']\n"
     ]
    }
   ],
   "source": [
    "# need to replace URLs with \"URL\" word\n",
    "import urlextract\n",
    "\n",
    "url_extractor = urlextract.URLExtract()\n",
    "print(url_extractor.find_urls(\"Will it detect github.com/sas/sa ?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# putting it all into a transformer for converting emails\n",
    "# to word counter\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class EmailToWordCounterTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, strip_headers=True, lower_case=True, remove_punctuation=True,\n",
    "                 replace_urls=True, replace_numbers=True, stemming=True):\n",
    "        self.strip_headers = strip_headers\n",
    "        self.lower_case = lower_case\n",
    "        self.remove_punctuation = remove_punctuation\n",
    "        self.replace_urls = replace_urls\n",
    "        self.replace_numbers = replace_numbers\n",
    "        self.stemming = stemming\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        X_transformed = []\n",
    "        for email in X:\n",
    "            text = email_to_text(email) or \"\"\n",
    "            \n",
    "            if self.lower_case:\n",
    "                text = text.lower()\n",
    "                \n",
    "            if self.replace_urls and url_extractor is not None:\n",
    "                urls = list(set(url_extractor.find_urls(text)))\n",
    "                urls.sort(key=lambda url: len(url), reverse=True)\n",
    "                for url in urls:\n",
    "                    text = text.replace(url, \" URL \")\n",
    "                    \n",
    "            if self.replace_numbers:\n",
    "                text = re.sub(r'\\d+(?:\\.\\d*(?:[eE]\\d+))?', 'NUMBER', text)\n",
    "                \n",
    "            if self.remove_punctuation:\n",
    "                text = re.sub(r'\\W+', ' ', text, flags=re.M)    \n",
    "            \n",
    "            word_counts = Counter(text.split())\n",
    "            \n",
    "            if self.stemming and stemmer is not None:\n",
    "                stemmed_word_counts = Counter()\n",
    "                for word, count in word_counts.items():\n",
    "                    stemmed_word = stemmer.stem(word)\n",
    "                    stemmed_word_counts[stemmed_word] += count\n",
    "                word_counts = stemmed_word_counts\n",
    "            X_transformed.append(word_counts)\n",
    "            \n",
    "        return np.array(X_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<email.message.EmailMessage object at 0x000001704514EFC8>\n",
      " <email.message.EmailMessage object at 0x000001704489E408>\n",
      " <email.message.EmailMessage object at 0x0000017044FABE48>]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([Counter({'to': 8, 'number': 8, 'you': 7, 'pleas': 7, 'servic': 5, 'the': 5, 'and': 5, 'if': 4, 'collect': 3, 'for': 3, 'we': 3, 'a': 3, 'your': 3, 'thi': 3, 'of': 3, 'in': 3, 'more': 3, 'inform': 3, 'fax': 3, 'telephon': 3, 'debt': 2, 'provid': 2, 'busi': 2, 'charg': 2, 'account': 2, 'are': 2, 'not': 2, 'us': 2, 'can': 2, 'repli': 2, 'remov': 2, 'mail': 2, 'email': 2, 'call': 2, 'time': 2, 'at': 2, 'profession': 1, 'effect': 1, 'avail': 1, 'last': 1, 'seventeen': 1, 'year': 1, 'nation': 1, 'credit': 1, 'system': 1, 'inc': 1, 'ha': 1, 'been': 1, 'top': 1, 'flight': 1, 'over': 1, 'institut': 1, 'healthcar': 1, 'onli': 1, 'low': 1, 'flat': 1, 'fee': 1, 'less': 1, 'than': 1, 'per': 1, 'all': 1, 'proce': 1, 'forward': 1, 'directli': 1, 'agenc': 1, 'wish': 1, 'will': 1, 'report': 1, 'unpaid': 1, 'experian': 1, 'formerli': 1, 'trw': 1, 'transunion': 1, 'equifax': 1, 'there': 1, 'is': 1, 'no': 1, 'import': 1, 'let': 1, 'know': 1, 'be': 1, 'simpli': 1, 'debt_collector': 1, 'btamail': 1, 'net': 1, 'cn': 1, 'with': 1, 'follow': 1, 'instruct': 1, 'subject': 1, 'field': 1, 'me': 1, 'from': 1, 'list': 1, 'snailmail': 1, 'have': 1, 'repres': 1, 'indic': 1, 'best': 1, 'ani': 1, 'necessari': 1, 'address': 1, 'text': 1, 'prefer': 1, 'alway': 1, 'dure': 1, 'normal': 1, 'hour': 1, 'ext': 1, 'thank': 1, 'p': 1, 's': 1, 'need': 1, 'our': 1, 'retain': 1, 'messag': 1, 'futur': 1, 'use': 1, 'or': 1, 'past': 1, 'it': 1, 'on': 1, 'friend': 1}),\n",
       "       Counter({'log': 15, 'nonspam': 13, 'the': 5, 'i': 4, 'number': 4, 'of': 3, 'on': 3, 'in': 3, 'that': 3, 'url': 3, 'think': 2, 'and': 2, 'should': 2, 'get': 2, 'justin': 2, 'a': 2, 't': 2, 'includ': 2, 'from': 2, 'your': 2, 'be': 2, 'it': 2, 's': 2, 'date': 2, 'as': 2, 'spamassassin': 2, 'talk': 2, 'list': 2, 'with': 1, 'confus': 1, 'all': 1, 'we': 1, 'give': 1, 'these': 1, 'folk': 1, 'until': 1, 'end': 1, 'day': 1, 'to': 1, 'fix': 1, 'upload': 1, 'relat': 1, 'note': 1, 'didn': 1, 'spamtrap': 1, 'stuff': 1, 'corpu': 1, 'directori': 1, 'server': 1, 'my': 1, 'but': 1, 'definit': 1, 'there': 1, 'don': 1, 'know': 1, 'if': 1, 'you': 1, 'd': 1, 'or': 1, 'not': 1, 'kelsey': 1, 'mailtrap': 1, 'c': 1, 'thursday': 1, 'august': 1, 'at': 1, 'am': 1, 'mason': 1, 'wrote': 1, 'file': 1, 'are': 1, 'out': 1, 'far': 1, 'can': 1, 'tell': 1, 'they': 1, 'have': 1, 'no': 1, 'comment': 1, 'will': 1, 'ignor': 1, 'danielp': 1, 'danielr': 1, 'duncan': 1, 'jame': 1, 'laager': 1, 'messagelab': 1, 'msquadrat': 1, 'olivi': 1, 'oliviern': 1, 'quinlan': 1, 'rodbegbi': 1, 'sean': 1, 'theo': 1, 'thi': 1, 'email': 1, 'is': 1, 'sponsor': 1, 'by': 1, 'osdn': 1, 'tire': 1, 'same': 1, 'old': 1, 'cell': 1, 'phone': 1, 'new': 1, 'here': 1, 'for': 1, 'free': 1, '_______________________________________________': 1, 'mail': 1, 'sourceforg': 1, 'net': 1}),\n",
       "       Counter({'hyperlink': 7})], dtype=object)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# trying out the transformer\n",
    "X_few = X_train[:3]\n",
    "X_few_wordcounts = EmailToWordCounterTransformer().fit_transform(X_few)\n",
    "X_few_wordcounts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### word to vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter()\n"
     ]
    }
   ],
   "source": [
    "print(Counter())\n",
    "#print(word_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "class WordCounterToVectorTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, vocabulary_size=1000):\n",
    "        self.vocabulary_size = vocabulary_size\n",
    "    def fit(self, X, y=None):\n",
    "        total_count = Counter()\n",
    "        \n",
    "        for word_count in X:\n",
    "            for word, count in word_count.items():\n",
    "                total_count[word] += min(count, 10)\n",
    "                # what is the logic behind 10\n",
    "                \n",
    "        most_common = total_count.most_common()[:self.vocabulary_size]\n",
    "        self.most_common_ = most_common\n",
    "        self.vocabulary_ = {word: index + 1 for index, (word, count) in enumerate(most_common)}\n",
    "        # returns an object with most common words and vocabulary \n",
    "        return self\n",
    "        \n",
    "    def transform(self, X, y=None):\n",
    "        rows = []\n",
    "        cols = []\n",
    "        data = []\n",
    "        for row, word_count in enumerate(X):\n",
    "            for word, count in word_count.items():\n",
    "                rows.append(row)\n",
    "                cols.append(self.vocabulary_.get(word, 0))\n",
    "                data.append(count)\n",
    "        return csr_matrix((data, (rows, cols)), shape=(len(X), self.vocabulary_size + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3x11 sparse matrix of type '<class 'numpy.int32'>'\n",
       "\twith 18 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_transformer = WordCounterToVectorTransformer(vocabulary_size=10)\n",
    "X_few_vectors = vocab_transformer.fit_transform(X_few_wordcounts)\n",
    "# calls fit then transform\n",
    "\n",
    "X_few_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[161,   8,   5,   0,   0,   8,   7,   5,   7,   0,   3],\n",
       "       [133,   4,   5,  15,  13,   1,   1,   2,   0,   0,   3],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   7,   0]],\n",
       "      dtype=int32)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_few_vectors.toarray()\n",
    "\n",
    "# 133 means there are 133 new words not part of vocaublary in second email\n",
    "# 9 means the firstword in vocabulary is used 8 times inthis email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'number': 1,\n",
       " 'the': 2,\n",
       " 'log': 3,\n",
       " 'nonspam': 4,\n",
       " 'to': 5,\n",
       " 'you': 6,\n",
       " 'and': 7,\n",
       " 'pleas': 8,\n",
       " 'hyperlink': 9,\n",
       " 'of': 10}"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_transformer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('number', 12),\n",
       " ('the', 10),\n",
       " ('log', 10),\n",
       " ('nonspam', 10),\n",
       " ('to', 9),\n",
       " ('you', 8),\n",
       " ('and', 7),\n",
       " ('pleas', 7),\n",
       " ('hyperlink', 7),\n",
       " ('of', 6)]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_transformer.most_common_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline and tansformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "preprocess_pipeline = Pipeline([\n",
    "    (\"email_to_wordcount\", EmailToWordCounterTransformer()),\n",
    "    (\"wordcount_to_vector\", WordCounterToVectorTransformer())\n",
    "])\n",
    "\n",
    "X_train_transformed = preprocess_pipeline.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.1s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  ................................................................\n",
      "[CV] .................................... , score=0.980, total=   0.1s\n",
      "[CV]  ................................................................\n",
      "[CV] .................................... , score=0.988, total=   0.1s\n",
      "[CV]  ................................................................\n",
      "[CV] .................................... , score=0.981, total=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.2s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.982893935108757"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "log_clf = LogisticRegression(solver=\"liblinear\", random_state=42)\n",
    "score = cross_val_score(log_clf, X_train_transformed, y_train, cv=3, verbose=3 )\n",
    "score.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### precision and recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:  0.9808429118773946\n",
      "recall:  0.9624060150375939\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "X_test_transformed = preprocess_pipeline.transform(X_test)\n",
    "\n",
    "log_clf = LogisticRegression(solver=\"liblinear\", random_state=42)\n",
    "log_clf.fit(X_train_transformed, y_train)\n",
    "\n",
    "y_pred = log_clf.predict(X_test_transformed)\n",
    "\n",
    "print(\"Precision: \", precision_score(y_test, y_pred))\n",
    "print(\"recall: \", recall_score(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf_gpu)",
   "language": "python",
   "name": "tf_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
