{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedded reber grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a function that generates strings based on a grammar\n",
    "# grammar will b the list of possible transitions for each state\n",
    "# A transition specifiesthe string to output (or a grammar to generate it)\n",
    "# and the next state\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "default_reber_grammar = [\n",
    "    [(\"B\", 1)],           # (state 0) =B=>(state 1)\n",
    "    [(\"T\", 2), (\"P\", 3)], # (state 1) =T=>(state 2) or =P=>(state 3)\n",
    "    [(\"S\", 2), (\"X\", 4)], # (state 2) =S=>(state 2) or =X=>(state 4)\n",
    "    [(\"T\", 3), (\"V\", 5)], # and so on...\n",
    "    [(\"X\", 3), (\"S\", 6)],\n",
    "    [(\"P\", 4), (\"V\", 6)],\n",
    "    [(\"E\", None)]]        # (state 6) =E=>(terminal state)\n",
    "\n",
    "embedded_reber_grammar = [\n",
    "    [(\"B\", 1)],\n",
    "    [(\"T\", 2), (\"P\", 3)],\n",
    "    [(default_reber_grammar, 4)],\n",
    "    [(default_reber_grammar, 5)],\n",
    "    [(\"T\", 6)],\n",
    "    [(\"P\", 6)],\n",
    "    [(\"E\", None)]]\n",
    "\n",
    "def generate_string(grammar):\n",
    "    state = 0\n",
    "    output = []\n",
    "    while state is not None:\n",
    "        index = np.random.randint(len(grammar[state]))\n",
    "        production, state = grammar[state][index]\n",
    "        if isinstance(production, list):\n",
    "            production = generate_string(grammar=production)\n",
    "        output.append(production)\n",
    "    return \"\".join(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BTXXTTVPXTVPXTTVPSE BPVPSE BTXSE BPVVE BPVVE BTSXSE BPTVPXTTTVVE BPVVE BTXSE BTXXVPSE BPTTTTTTTTVVE BTXSE BPVPSE BTXSE BPTVPSE BTXXTVPSE BPVVE BPVVE BPVVE BPTTVVE BPVVE BPVVE BTXXVVE BTXXVVE BTXXVPXVVE "
     ]
    }
   ],
   "source": [
    "# generating weber grammar\n",
    "for _ in range(25):\n",
    "    print(generate_string(default_reber_grammar), end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BTBPVVETE BTBTSSSSSSSXXVVETE BPBTSSSXXTTTTVPSEPE BTBPTTVVETE BPBTXXTVVEPE BTBTXSETE BPBTSSSSSXXTTVPXVPXTTTVVEPE BPBTSSXXTVPSEPE BPBPTTTTTTTVPSEPE BTBTSXSETE BPBPTVPXVVEPE BPBPVVEPE BPBPTVVEPE BTBPTTVPXTTVPSETE BTBTSSXSETE BTBTXXTTVVETE BPBTSXSEPE BPBPTVPSEPE BTBPVVETE BPBTXXTTTVPXTVVEPE BPBPTTVPXTVVEPE BTBPVVETE BPBPTVPXVPXTVVEPE BTBPVVETE BPBTSXSEPE "
     ]
    }
   ],
   "source": [
    "#generating fromm embedded reber grammar\n",
    "for _ in range(25):\n",
    "    print(generate_string(embedded_reber_grammar), end= \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a function to generate strings that dont respect the grammar\n",
    "\n",
    "def generate_corrupted_string(grammar, chars=\"BEPSTVX\"):\n",
    "    good_string = generate_string(grammar)\n",
    "    index = np.random.randint(len(good_string))\n",
    "    good_char = good_string[index]\n",
    "    bad_char = np.random.choice(sorted(set(chars) - set(good_char)))\n",
    "    return good_string[:index] + bad_char + good_string[index + 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BTTTXXVVETE BPBTXXSPXTVVEPE BTBTXSPTE BPTTSXXTVPXVVEPE PPBPVPSEPE BTBPTVETE BPTTSSSSSXSEPE BPBSVPSEPE BTBPVVESE BPBTXSEPS BEBTXSETE XPBTXXTVPSEPE BTBPVVEPE BTXPTVVETE BTBPVXETE BVBTXSETE BPTTXXVPXVPSEPE BTBPXVPSETE STBPTTVPXVPXTVPSETE BPBPTVPSESE BPBPVEEPE ETBTXSETE BTBTXSVTE BPBTXXVPSEPP BTBTXXVPSETS "
     ]
    }
   ],
   "source": [
    "# few corrupted strings\n",
    "\n",
    "for _ in range(25):\n",
    "    print(generate_corrupted_string(embedded_reber_grammar), end = \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "int32\n"
     ]
    }
   ],
   "source": [
    "# its not possible to feeda string directly to RNN \n",
    "# we need to convert it to a sequence of vectors \n",
    "# each vector will represent a single letter,using one hot encoding\n",
    "# lets write a function for one hot encoding\n",
    "\n",
    "def one_hot_encoding_sample(string):\n",
    "    one_hot_vector = {}\n",
    "    for i,char in enumerate(string):\n",
    "        word = []\n",
    "        for j in range(len(string)):\n",
    "            word.append(0)\n",
    "        word[i] = 1\n",
    "        one_hot_vector[char] = word\n",
    "    return one_hot_vector\n",
    "\n",
    "reber = one_hot_encoding_sample(\"BEPSTVX\")\n",
    "#print(reber)\n",
    "\n",
    "#print(reber['B'])\n",
    "def converting_string(reber, string):\n",
    "    output = []\n",
    "    for i in string:\n",
    "        output.append(reber[i])\n",
    "    return np.array(output).astype(np.int32)\n",
    "\n",
    "sample = converting_string(reber,\"BTBTXSETE\")\n",
    "print(sample.dtype)\n",
    "# I am using this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_to_one_hot_vectors(string, n_steps, chars=\"BEPSTVX\"):\n",
    "    char_to_index = {char: index for index, char in enumerate(chars)}\n",
    "    output = np.zeros((n_steps, len(chars)), dtype=np.int32)\n",
    "    for index, char in enumerate(string):\n",
    "        output[index, char_to_index[char]] = 1.\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 1, 0, 0],\n",
       "       [1, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 1, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 1],\n",
       "       [0, 0, 0, 1, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 1, 0, 0],\n",
       "       [0, 1, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string_to_one_hot_vectors(\"BTBTXSETE\",12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset(size):\n",
    "    good_strings = [generate_string(embedded_reber_grammar)\n",
    "                    for _ in range(size // 2)]\n",
    "    bad_strings = [generate_corrupted_string(embedded_reber_grammar)\n",
    "                   for _ in range(size - size // 2)]\n",
    "    all_strings = good_strings + bad_strings\n",
    "    n_steps = max([len(string) for string in all_strings])\n",
    "    X = np.array([string_to_one_hot_vectors(string, n_steps)\n",
    "                  for string in all_strings])\n",
    "    seq_length = np.array([len(string) for string in all_strings])\n",
    "    y = np.array([[1] for _ in range(len(good_strings))] +\n",
    "                 [[0] for _ in range(len(bad_strings))])\n",
    "    rnd_idx = np.random.permutation(size)\n",
    "    return X[rnd_idx], seq_length[rnd_idx], y[rnd_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, l_train, y_train = generate_dataset(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0 0 0 0 0]\n",
      " [0 0 0 0 1 0 0]\n",
      " [1 0 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0]\n",
      " [0 0 0 0 0 1 0]\n",
      " [0 0 1 0 0 0 0]\n",
      " [0 0 0 0 0 0 1]\n",
      " [0 0 0 0 1 0 0]\n",
      " [0 0 0 0 1 0 0]\n",
      " [0 0 0 0 1 0 0]\n",
      " [0 0 0 0 1 0 0]\n",
      " [0 0 0 0 1 0 0]\n",
      " [0 0 0 0 1 0 0]\n",
      " [0 0 0 0 1 0 0]\n",
      " [0 0 0 0 1 0 0]\n",
      " [0 0 0 0 0 1 0]\n",
      " [0 0 1 0 0 0 0]\n",
      " [0 0 0 0 0 0 1]\n",
      " [0 0 0 0 0 1 0]\n",
      " [0 0 0 0 0 1 0]\n",
      " [0 0 0 1 0 0 0]\n",
      " [0 0 0 0 1 0 0]\n",
      " [0 1 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0621 12:56:45.853755  2640 deprecation.py:323] From <ipython-input-14-008d4fb6ff16>:15: GRUCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.GRUCell, and will be replaced by that in Tensorflow 2.0.\n",
      "W0621 12:56:45.855701  2640 deprecation.py:323] From <ipython-input-14-008d4fb6ff16>:17: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "W0621 12:56:45.930532  2640 deprecation.py:323] From C:\\installs\\Anaconda\\envs\\tf_gpu\\lib\\site-packages\\tensorflow_core\\python\\ops\\rnn_cell_impl.py:559: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.add_weight` method instead.\n",
      "W0621 12:56:45.941471  2640 deprecation.py:506] From C:\\installs\\Anaconda\\envs\\tf_gpu\\lib\\site-packages\\tensorflow_core\\python\\ops\\rnn_cell_impl.py:565: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0621 12:56:45.955441  2640 deprecation.py:506] From C:\\installs\\Anaconda\\envs\\tf_gpu\\lib\\site-packages\\tensorflow_core\\python\\ops\\rnn_cell_impl.py:575: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0621 12:56:45.977374  2640 deprecation.py:323] From C:\\installs\\Anaconda\\envs\\tf_gpu\\lib\\site-packages\\tensorflow_core\\python\\ops\\rnn.py:244: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W0621 12:56:45.997341  2640 deprecation.py:323] From <ipython-input-14-008d4fb6ff16>:19: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.Dense instead.\n",
      "W0621 12:56:45.997341  2640 deprecation.py:323] From C:\\installs\\Anaconda\\envs\\tf_gpu\\lib\\site-packages\\tensorflow_core\\python\\layers\\core.py:187: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "possible_chars = \"BEPSTVX\"\n",
    "n_inputs = len(possible_chars)\n",
    "n_neurons = 30\n",
    "n_outputs = 1\n",
    "\n",
    "learning_rate = 0.02\n",
    "momentum = 0.95\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, None, n_inputs], name=\"X\")\n",
    "seq_length = tf.placeholder(tf.int32, [None], name=\"seq_length\")\n",
    "y = tf.placeholder(tf.float32, [None, 1], name=\"y\")\n",
    "\n",
    "gru_cell = tf.nn.rnn_cell.GRUCell(num_units=n_neurons)\n",
    "outputs, states = tf.nn.dynamic_rnn(gru_cell, X, dtype=tf.float32,\n",
    "                                    sequence_length=seq_length)\n",
    "\n",
    "logits = tf.layers.dense(states, n_outputs, name=\"logits\")\n",
    "y_pred = tf.cast(tf.greater(logits, 0.), tf.float32, name=\"y_pred\")\n",
    "y_proba = tf.nn.sigmoid(logits, name=\"y_proba\")\n",
    "\n",
    "xentropy = tf.nn.sigmoid_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate,\n",
    "                                       momentum=momentum,\n",
    "                                       use_nesterov=True)\n",
    "training_op = optimizer.minimize(loss)\n",
    "\n",
    "correct = tf.equal(y_pred, y, name=\"correct\")\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating a validation set to track progress during trainig\n",
    "\n",
    "X_val, l_val, y_val = generate_dataset(5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0  Train loss: 0.6639, accuracy: 64.00%  Validation accuracy: 60.42%\n",
      "   1  Train loss: 0.6460, accuracy: 72.00%  Validation accuracy: 58.40%\n",
      "   2  Train loss: 0.6041, accuracy: 80.00%  Validation accuracy: 71.60%\n",
      "   3  Train loss: 0.5973, accuracy: 56.00%  Validation accuracy: 53.76%\n",
      "   4  Train loss: 0.5723, accuracy: 82.00%  Validation accuracy: 75.50%\n",
      "   5  Train loss: 0.3514, accuracy: 88.00%  Validation accuracy: 85.64%\n",
      "   6  Train loss: 0.3306, accuracy: 94.00%  Validation accuracy: 91.64%\n",
      "   7  Train loss: 0.2649, accuracy: 94.00%  Validation accuracy: 94.64%\n",
      "   8  Train loss: 0.4476, accuracy: 88.00%  Validation accuracy: 85.76%\n",
      "   9  Train loss: 0.2015, accuracy: 98.00%  Validation accuracy: 92.28%\n",
      "  10  Train loss: 0.0286, accuracy: 100.00%  Validation accuracy: 96.30%\n",
      "  11  Train loss: 0.0654, accuracy: 100.00%  Validation accuracy: 98.66%\n",
      "  12  Train loss: 0.0221, accuracy: 100.00%  Validation accuracy: 99.16%\n",
      "  13  Train loss: 0.0475, accuracy: 100.00%  Validation accuracy: 99.20%\n",
      "  14  Train loss: 0.0843, accuracy: 94.00%  Validation accuracy: 94.90%\n",
      "  15  Train loss: 0.6484, accuracy: 66.00%  Validation accuracy: 67.34%\n",
      "  16  Train loss: 0.5116, accuracy: 72.00%  Validation accuracy: 75.78%\n",
      "  17  Train loss: 0.2029, accuracy: 92.00%  Validation accuracy: 90.78%\n",
      "  18  Train loss: 0.0725, accuracy: 100.00%  Validation accuracy: 95.44%\n",
      "  19  Train loss: 0.3885, accuracy: 84.00%  Validation accuracy: 82.30%\n",
      "  20  Train loss: 0.3476, accuracy: 86.00%  Validation accuracy: 92.62%\n",
      "  21  Train loss: 0.1787, accuracy: 94.00%  Validation accuracy: 96.78%\n",
      "  22  Train loss: 0.1799, accuracy: 96.00%  Validation accuracy: 95.12%\n",
      "  23  Train loss: 0.0992, accuracy: 98.00%  Validation accuracy: 96.46%\n",
      "  24  Train loss: 0.2756, accuracy: 94.00%  Validation accuracy: 97.68%\n",
      "  25  Train loss: 0.0929, accuracy: 100.00%  Validation accuracy: 98.14%\n",
      "  26  Train loss: 0.0907, accuracy: 98.00%  Validation accuracy: 94.64%\n",
      "  27  Train loss: 0.1091, accuracy: 98.00%  Validation accuracy: 99.44%\n",
      "  28  Train loss: 0.2337, accuracy: 96.00%  Validation accuracy: 95.36%\n",
      "  29  Train loss: 0.1915, accuracy: 96.00%  Validation accuracy: 99.00%\n",
      "  30  Train loss: 0.0026, accuracy: 100.00%  Validation accuracy: 99.84%\n",
      "  31  Train loss: 0.0046, accuracy: 100.00%  Validation accuracy: 99.94%\n",
      "  32  Train loss: 0.0010, accuracy: 100.00%  Validation accuracy: 99.94%\n",
      "  33  Train loss: 0.0010, accuracy: 100.00%  Validation accuracy: 99.46%\n",
      "  34  Train loss: 0.0032, accuracy: 100.00%  Validation accuracy: 99.94%\n",
      "  35  Train loss: 0.0013, accuracy: 100.00%  Validation accuracy: 99.96%\n",
      "  36  Train loss: 0.0006, accuracy: 100.00%  Validation accuracy: 100.00%\n",
      "  37  Train loss: 0.0004, accuracy: 100.00%  Validation accuracy: 100.00%\n",
      "  38  Train loss: 0.0004, accuracy: 100.00%  Validation accuracy: 100.00%\n",
      "  39  Train loss: 0.0003, accuracy: 100.00%  Validation accuracy: 100.00%\n",
      "  40  Train loss: 0.0003, accuracy: 100.00%  Validation accuracy: 100.00%\n",
      "  41  Train loss: 0.0003, accuracy: 100.00%  Validation accuracy: 100.00%\n",
      "  42  Train loss: 0.0003, accuracy: 100.00%  Validation accuracy: 100.00%\n",
      "  43  Train loss: 0.0003, accuracy: 100.00%  Validation accuracy: 100.00%\n",
      "  44  Train loss: 0.0002, accuracy: 100.00%  Validation accuracy: 100.00%\n",
      "  45  Train loss: 0.0002, accuracy: 100.00%  Validation accuracy: 100.00%\n",
      "  46  Train loss: 0.0002, accuracy: 100.00%  Validation accuracy: 100.00%\n",
      "  47  Train loss: 0.0002, accuracy: 100.00%  Validation accuracy: 100.00%\n",
      "  48  Train loss: 0.0002, accuracy: 100.00%  Validation accuracy: 100.00%\n",
      "  49  Train loss: 0.0002, accuracy: 100.00%  Validation accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 50\n",
    "batch_size = 50\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        X_batches = np.array_split(X_train, len(X_train) // batch_size)\n",
    "        l_batches = np.array_split(l_train, len(l_train) // batch_size)\n",
    "        y_batches = np.array_split(y_train, len(y_train) // batch_size)\n",
    "        for X_batch, l_batch, y_batch in zip(X_batches, l_batches, y_batches):\n",
    "            loss_val, _ = sess.run(\n",
    "                [loss, training_op],\n",
    "                feed_dict={X: X_batch, seq_length: l_batch, y: y_batch})\n",
    "        acc_train = accuracy.eval(feed_dict={X: X_batch, seq_length: l_batch, y: y_batch})\n",
    "        acc_val = accuracy.eval(feed_dict={X: X_val, seq_length: l_val, y: y_val})\n",
    "        print(\"{:4d}  Train loss: {:.4f}, accuracy: {:.2f}%  Validation accuracy: {:.2f}%\".format(\n",
    "            epoch, loss_val, 100 * acc_train, 100 * acc_val))\n",
    "        saver.save(sess, \"./my_reber_classifier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.4046186e-06]\n",
      " [9.9911445e-01]]\n"
     ]
    }
   ],
   "source": [
    "test_strings = [\n",
    "    \"BPBTSSSSSSSXXTTVPXVPXTTTTTVVETE\",\n",
    "    \"BPBTSSSSSSSXXTTVPXVPXTTTTTVVEPE\"\n",
    "]\n",
    "\n",
    "l_test = np.array([len(s) for s in test_strings])\n",
    "max_length = l_test.max()\n",
    "X_test = [string_to_one_hot_vectors(s, n_steps=max_length) for s in test_strings]\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"./my_reber_classifier\")\n",
    "    y_proba_val = y_proba.eval(feed_dict={X:X_test, seq_length: l_test})\n",
    "print(y_proba_val)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf_gpu)",
   "language": "python",
   "name": "tf_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
