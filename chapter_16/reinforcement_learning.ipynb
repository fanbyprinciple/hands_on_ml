{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To support both python 2 and python 3\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "# To plot pretty figures and animations\n",
    "%matplotlib nbagg\n",
    "import matplotlib\n",
    "import matplotlib.animation as animation\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"rl\"\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True):\n",
    "    path = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID, fig_id + \".png\")\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format='png', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open ai introductinoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('MsPacman-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the environemnt is initialised by reset method\n",
    "obs = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(210, 160, 3)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# observation vary depending on the nevironemnt\n",
    "# in this case it is an RGB image represented as a 3D nUmpy array\n",
    "# of shape [width, height, channels]\n",
    "\n",
    "obs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# an environemnt can be visualized by calling its render() method\n",
    "# in this examplewe will set the mode=\"rgb_array\"\n",
    "\n",
    "img = env.render(mode=\"rgb_array\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython import display\n",
    "%matplotlib inline\n",
    "plt.figure(figsize=(5,4))\n",
    "for i in range(25):\n",
    "    plt.imshow(img)\n",
    "    display.display(plt.gcf())    \n",
    "    display.clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(img == obs).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to plot th eenvironment\n",
    "def plot_environment(env, figsize=(5,4)):\n",
    "    plt.close()\n",
    "    plt.figure(figsize=figsize)\n",
    "    img = env.render(mode=\"rgb_array\")\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    display.display(plt.gcf())\n",
    "    display.clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(9)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space\n",
    "# Discrete(9) means that the possible actions are integers 0 through 8, which represents the 9 possible positions of the joystick (0=center, 1=up, 2=right, 3=left, 4=down, 5=upper-right, 6=upper-left, 7=lower-right, 8=lower-left)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "for step in range(110):\n",
    "    env.step(3)\n",
    "for step in range(40):\n",
    "    env.step(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot_environment(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the step function\n",
    "obs, reward , done, info =env.step(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(210, 160, 3)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "done\n",
    "# when game is over done returns true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ale.lives': 3}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info\n",
    "# its an environemtn specific dictionary that can provide some extra info about the\n",
    "# internal state of environment,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# playing one full game with 3 lives\n",
    "frames = []\n",
    "n_max_steps = 1000\n",
    "n_change_steps = 10\n",
    "\n",
    "obs = env.reset()\n",
    "\n",
    "for step in range(n_max_steps):\n",
    "    img = env.render(mode=\"rgb_array\")\n",
    "    frames.append(img)\n",
    "    if step % n_change_steps == 0:\n",
    "        action = env.action_space.sample()\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    if done:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to show the animation\n",
    "def update_scene(num,frames,patch):\n",
    "    patch.set_data(frames[num])\n",
    "    return patch\n",
    "\n",
    "def plot_animation(frames, repeat=False, interval=40):\n",
    "    plt.close()\n",
    "    fig = plt.figure()\n",
    "    for f in frames:\n",
    "        plt.imshow(f)\n",
    "        plt.axis('off')\n",
    "        display.display(plt.gcf())\n",
    "        display.clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot_animation(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cartpole rendering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trying a differne tpproach for rendering \n",
    "# answer taken from\n",
    "# https://stackoverflow.com/questions/52726475/display-openai-gym-in-jupyter-notebook-only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "from IPython import display\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "obs = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.01911864,  0.00549671, -0.01746714,  0.00916598])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageDraw\n",
    "\n",
    "try:\n",
    "    from pyglet.gl import gl_info\n",
    "    openai_cart_pole_rendering=True\n",
    "except Exception:\n",
    "    openai_cart_pole_rendering=False\n",
    "\n",
    "def render_cart_pole(env, obs):\n",
    "    if openai_cart_pole_rendering:\n",
    "        return env.render(mode=\"rgb_array\")\n",
    "    else:\n",
    "        # rendering forcart pole environment\n",
    "        # so badass ageron\n",
    "        print(\"self rendering\")\n",
    "        img_w = 600\n",
    "        img_h = 400\n",
    "        cart_w = img_w //12\n",
    "        cart_h = img_h //15\n",
    "        pole_len = img_h //3.5\n",
    "        ple_w = img_w // 80 + 1\n",
    "        x_width = 2\n",
    "        max_ang = 0.2\n",
    "        bg_col = (255,255,255)\n",
    "        cart_col = 0x000000\n",
    "        pole_col = 0x669acc\n",
    "        \n",
    "        pos,vel, ang, ang_vel = obs\n",
    "        img = Image.new('RGB', (img_w, img_h), bg_col)\n",
    "        draw = ImageDraw.Draw(img)\n",
    "        cart_x = pos * img_w // x_width + img_w // x_width\n",
    "        cart_y = img_h * 95 // 100\n",
    "        top_pole_x = cart_x + pole_len * np.sin(ang)\n",
    "        top_pole_y = cart_y - cart_h // 2 - pole_len * np.cos(ang)\n",
    "        draw.line((0, cart_y, img_w, cart_y), fill=0)\n",
    "        draw.rectangle((cart_x, cart_y - cart_h // 2, top_pole_x,top_pole_y ), fill=pole_col, width=pole_w)\n",
    "        draw.line((cart_x, cart_y - cart_h//2, top_pole_x, top_pole_y ), fill=pole_col, width=pole_w)\n",
    "        return np.array(img)\n",
    "\n",
    "def plot_cart_pole(env,obs):\n",
    "    plt.close()\n",
    "    image = render_cart_pole(env, obs)\n",
    "    plt.imshow(image)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVQAAADnCAYAAABBu67aAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAADqklEQVR4nO3c0U3CUBiAUWu6hHOwhnPgTDCHaziHY9Q3Y6RSgh+2knMSEugNyf/QfLkppcM0TQ8A/N7j2gMA3AtBBYgIKkBEUAEiggoQGRfW3QIAcGqYO2iHChARVICIoAJEBBUgIqgAEUEFiAgqQERQASKCChARVICIoAJEBBUgIqgAEUEFiAgqQERQASKCChARVICIoAJEBBUgIqgAEUEFiAgqQERQASKCChARVICIoAJEBBUgIqgAEUEFiAgqQERQASKCChARVICIoAJEBBUgIqgAEUEFiAgqQERQASKCChARVICIoAJEBBUgIqgAEUEFiAgqQERQASKCChARVICIoAJEBBUgIqgAEUEFiAgqQERQASKCChARVICIoAJEBBUgIqgAEUEFiAgqQERQASKCChARVICIoAJEBBUgIqgAEUEFiAgqQERQASKCChARVICIoAJEBBUgIqgAEUEFiAgqQERQASKCChARVICIoAJEBBUgIqgAEUEFiAgqQERQASKCChARVICIoAJEBBUgIqgAkXHtAeCct+PLybHd/rDCJLDMDhUgIqgAEUEFiAgqQERQASKCyr8z98s/bIGgAkQEFSAiqAARQQWICCpARFABIoIKEBFUgIigAkQEFSAiqAARQQWICCpARFABIoIKEBFUgIigAkQEFSAiqAARQQWICCpARFABIoIKEBFUgIigAkQEFSAiqAARQQWICCpARFABIoIKEBFUNm23P6w9AlxMUAEiggoQEVSAiKACRAQVICKoABFBBYgIKkBEUAEiggoQEVSAiKACRAQVICKoABFBBYgIKkBEUAEiggoQEVSAiKACRAQVICKoABFBZRXDMFz8usX34RYEFSAyrj0AXOL1ff/5/vnpuOIk8DM7VDbva0znPsNWCCpARFABIoLK5n2/ZuoaKls1TNN0bv3sIlzrL29nWjjH4RqzJ7AdKkBEUAEiggoQEVSAiKACRAQVICKoABFBBYgIKkDE4/tYhX8vcY/sUAEiggoQEVSAiKACRAQVICKoABFBBYgIKkBEUAEiggoQEVSAiKACRAQVICKoABFBBYgIKkBEUAEiggoQEVSAiKACRAQVICKoABFBBYgIKkBEUAEiggoQEVSAiKACRAQVICKoABFBBYgIKkBEUAEiggoQEVSAiKACRAQVICKoABFBBYgIKkBEUAEiggoQEVSAiKACRAQVIDIurA9/MgXAHbBDBYgIKkBEUAEiggoQEVSAiKACRD4AmqsfEUQ28RAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_cart_pole(env, obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# the other way of rendering cartpole\n",
    "\n",
    "for i in range(25):\n",
    "   plt.imshow(env.render(mode='rgb_array'))\n",
    "   display.display(plt.gcf())    \n",
    "   display.clear_output(wait=True)\n",
    "   env.step(env.action_space.sample()) # take a random action\n",
    "\n",
    "# doesntwork that well because it leaves a dead kernel \n",
    "# when we close the other window\n",
    "# so dont close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(2)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.5, 599.5, 399.5, -0.5)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVQAAADnCAYAAABBu67aAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAED0lEQVR4nO3c0W0aQRRAUW9EE6kjLiN14DbcBtSRMpw6UsbmL0ocZOzkmp1ZnyPxAQvSfKCrYR6wrOt6B8D/+7T1AgD2QlABIoIKEBFUgIigAkQOV677CgDA35ZLD9qhAkQEFSAiqAARQQWICCpARFABIoIKEBFUgIigAkQEFSAiqAARQQWICCpARFABIoIKEBFUgIigAkQEFSAiqAARQQWICCpARFABIoIKEBFUgIigAkQEFSAiqAARQQWICCpARFABIoIKEBFUgIigAkQEFSAiqAARQQWICCpARFABIoIKEBFUgIigAkQEFSAiqAARQQWICCpARFABIoIKEBFUgIigAkQEFSAiqAARQQWICCpARFABIoIKEBFUgIigAkQEFSAiqAARQQWICCpARFABIoIKEBFUgIigAkQEFSAiqAARQQWICCpARFABIoIKEBFUgIigAkQEFSAiqAARQQWICCpARFABIoIKEBFUgIigAkQEFSAiqAARQQWICCpARFABIoIKEBFUgIigAkQEFSAiqAARQQWICCpARFABIoetFwCv9f388Mf9L8fTRiuBy+xQmdbzwMLWBBUgIqhMwW6UGQgqQERQASKCyrRM+RmNoAJEBJXhGUgxC0EFiAgqQERQmZKBFCMSVICIoDI0AylmIqgAEUEFiAgqQERQmY4JP6MSVIZlIMVsBBUgIqgAEUEFiAgqUzGQYmSCChARVIZkws+MBBUgIqgAEUFlGgZSjE5QASKCynAuDaTsTpmBoAJEBBUgIqgAEUEFiAgqQzGQYmaCChARVIbh9/vMTlABIoIKEBFUgIigMjQTfmYiqAzBQIo9EFSAiKACRAQVICKoDMtAitkIKkBEUNmcCT97IagAEUFlSM5PmZGgAkQEFSAiqGzKQIo9EVSAiKACRASV4ZjwMytBBYgIKpsxkGJvBBUgIqgAEUElsyzLm26X3D+c//m1sDVBBYgctl4AH9PT6Xj37cfx1/2vn88brgYay7quL11/8SL87i0fxR8fny48dv/q119538J7u/hm95Gfm3s6Ha8/CSYkqAARQWUTz89MnaGyB85Qydzy60zOUNmYM1SA9ySoABFBBYgIKkBEUAEiggoQEVSAiKACRAQVIOLv+8j49RIfnR0qQERQASKCChARVICIoAJEBBUgIqgAEUEFiAgqQERQASKCChARVICIoAJEBBUgIqgAEUEFiAgqQERQASKCChARVICIoAJEBBUgIqgAEUEFiAgqQERQASKCChARVICIoAJEBBUgIqgAEUEFiAgqQERQASKCChARVICIoAJEBBUgIqgAEUEFiAgqQERQASKCChARVICIoAJEDleuLzdZBcAO2KECRAQVICKoABFBBYgIKkBEUAEiPwGNdEtvQ/Ob1QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# for cartpole each observation is a 1D array containing 4 floats\n",
    "# representing the carts horizontal position, angle of pole and the horizontal and angular velocity\n",
    "\n",
    "# to use render to return image as anarray we can use\n",
    "# env.render(\"rgb_array\")\n",
    "\n",
    "# cartpole has only two possible actions accelerate towards left or towards right\n",
    "# accelerating left till pole falls\n",
    "obs = env.reset()\n",
    "while True:\n",
    "    obs, reward, done, info = env.step(0)\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "plt.close()\n",
    "img = render_cart_pole(env, obs)\n",
    "plt.imshow(img)\n",
    "plt.axis(\"off\")\n",
    "\n",
    "# notice that the game fails when it tilts toot muc\n",
    "# and not when it actually falls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVQAAADnCAYAAABBu67aAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAEJUlEQVR4nO3c0W0TQRRAURu5CeqAMqgjaSNtxHVQBqmDMswfAhLFRLl45y3nSPlwNrHmw76afePkeLlcDgC834etFwCwF4IKEBFUgIigAkQEFSByunLdRwAAnju+9E07VICIoAJEBBUgIqgAEUEFiAgqQERQASKCChARVICIoAJEBBUgIqgAEUEFiAgqQERQASKCChARVICIoAJEBBUgIqgAEUEFiAgqQERQASKCChARVICIoAJEBBUgIqgAEUEFiAgqQERQASKCChARVICIoAJEBBUgIqgAEUEFiAgqQERQASKCChARVICIoAJEBBUgIqgAEUEFiAgqQERQASKCChARVICIoAJEBBUgIqgAEUEFiAgqQERQASKCChARVICIoAJEBBUgIqgAEUEFiAgqQERQASKCChARVICIoAJEBBUgIqgAEUEFiAgqQERQASKCChARVICIoAJEBBUgIqgAEUEFiAgqQERQASKCChARVICIoAJEBBUgIqgAEUEFiAgqQERQASKCChARVICIoAJEBBUgIqgAEUEFiAgqQERQWdLT+f7wdL7fehnwJqetFwCHw0E82QU7VJbw6e5x6yXAuwkqQERQWZpRAJMIKkBEUFmGOSrTCSpARFBZnjkqUwgqS3Hbz2SCChARVICIoDKCOSoTCCrLMUdlKkEFiAgqY7jtZ3WCChARVICIoLIkB1NMJKiMYo7KygQVICKoABFBZVnmqEwjqAARQWUcB1OsSlABIoIKEBFUluZgikkEFSAiqIzkYIoVCSpARFBZnjkqUwgqQERQGcscldUIKkBEUAEigsoIDqaYQFABIoLKaA6mWImgMobbflYnqIxnl8oqBBUgIqgAEUFlFHNUViaoABFBZRccTLECQQWICCpARFAZx8EUqxJUdsMcla0JKkBEUAEigspI5qisSFABIoLKrjiYYkuCChARVICIoDKWgylWI6gAEUFldxxMsRVBZVnH4/Hq13t+92+eB95CUBnt8/156yXAT6etFwCVr9/vfnkktNyeHSq78HtMD4eHh28brYT/maAyntt+ViGo7Na3x7vrPwQhQWUXvnw8P3ts58qtHS+Xy2vXX70I/9ItP8505X0Af3rxxWmHChARVICIoAJEBBUgIqgAEUEFiAgqQERQASKCChDx7/tYlr9eYho7VICIoAJEBBUgIqgAEUEFiAgqQERQASKCChARVICIoAJEBBUgIqgAEUEFiAgqQERQASKCChARVICIoAJEBBUgIqgAEUEFiAgqQERQASKCChARVICIoAJEBBUgIqgAEUEFiAgqQERQASKCChARVICIoAJEBBUgIqgAEUEFiAgqQERQASKCChARVICIoAJEBBUgIqgAEUEFiJyuXD/eZBUAO2CHChARVICIoAJEBBUgIqgAEUEFiPwAEipNSKVUrZgAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# lets pushto right\n",
    "obs = env.reset()\n",
    "while True:\n",
    "    obs, reward, done, info = env.step(1)\n",
    "    if done:\n",
    "        break\n",
    "plot_cart_pole(env, obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A simple policy\n",
    "If tilting to left push right otherwise left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = []\n",
    "\n",
    "n_max_steps = 1000\n",
    "n_change_steps = 10\n",
    "\n",
    "obs = env.reset()\n",
    "for step in range(n_max_steps):\n",
    "    img = render_cart_pole(env,obs)\n",
    "    frames.append(img)\n",
    "#     plt.imshow(img)\n",
    "#     display.display(plt.gcf())    \n",
    "#     display.clear_output(wait=True)\n",
    "    # hard coded policy\n",
    "    position, velocity, angle, angular_velocity = obs\n",
    "    if angle<0:\n",
    "        action = 0\n",
    "    else:\n",
    "        action =1\n",
    "    \n",
    "    obs, reward, done , info = env.step(action)\n",
    "    if done:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVQAAADnCAYAAABBu67aAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAEB0lEQVR4nO3b3Y3aQBhAURzRROpIykgdbE1LHSkjqWPLIG+Rsj9sVnvx2MM5kh+wBZoHuBrrM8vlcjkA8HlfRi8AYBaCChARVICIoAJEBBUgcnznukcAAF5aXjtphwoQEVSAiKACRAQVICKoABFBBYgIKkBEUAEiggoQEVSAiKACRAQVICKoABFBBYgIKkBEUAEiggoQEVSAiKACRAQVICKoABFBBYgIKkBEUAEiggoQEVSAiKACRAQVICKoABFBBYgIKkBEUAEiggoQEVSAiKACRAQVICKoABFBBYgIKkBEUAEiggoQEVSAiKACRAQVICKoABFBBYgIKkBEUAEiggoQEVSAiKACRAQVICKoABFBBYgIKkBEUAEiggoQEVSAiKACRAQVICKoABFBBYgIKkBEUAEiggoQEVSAiKACRAQVICKoABFBBYgIKkBEUAEiggoQEVSAiKACRAQVICKoABFBBYgIKkBEUAEiggoQEVSAiKACRAQVICKoABFBBYgIKkBEUAEiggoQEVSAiKACRAQVICKoABFBBYgcRy8APuL3+eGf199Oj4NWAi/ZobIbz2P61jkYRVDZDbtRtk5QASKCyu657WcrBBUgIqgAEUFlVwym2DJBBYgIKkBEUJmCST9bIKgAEUEFiAgqu2PSz1YJKkBEUJmGwRSjCSpARFABIoLKLhlMsUWCChARVICIoDIVk35GElSAiKACRASV3TLpZ2sEFSAiqEzHYIpRBBUgIqgAEUFl1wym2BJBBYgIKlMymGIEQWX33PazFYIKEBFUpuW2n7UJKkBEUAEigsoUDKbYAkEFiAgqQERQmZpJP2sSVICIoAJEBJVpmPQzmqACRASV6RlMsRZBBYgIKkBEUJmKwRQjCSpARFABIoLKXTDpZw2CChARVICIoDIdk35GEVSAiKByNwymuDVBBYgIKkBEUJmSwRQjCCq7syzLfx2fee+1z4C3CCp35dfjafQSmJigMq3vD+fD4XA4/Hw6/T3glgSVqT2PqKhyS4LK3XHbz60IKkBEUJnaj6/nq6+htFwul2vXr16EEdZ8nOmd3wf369UvoR0qQERQASKCChARVICIoAJEBBUgIqgAEUEFiAgqQOQ4egHwUf69xFbZoQJEBBUgIqgAEUEFiAgqQERQASKCChARVICIoAJEBBUgIqgAEUEFiAgqQERQASKCChARVICIoAJEBBUgIqgAEUEFiAgqQERQASKCChARVICIoAJEBBUgIqgAEUEFiAgqQERQASKCChARVICIoAJEBBUgIqgAEUEFiAgqQERQASKCChARVICIoAJEBBUgIqgAEUEFiAgqQOT4zvVllVUATMAOFSAiqAARQQWICCpARFABIoIKEPkD2PdJq7Q0AngAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_animation(frames)\n",
    "#system is unstable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"dense_3/Elu:0\", shape=(?, 1), dtype=float32)\n",
      "Tensor(\"concat_1:0\", shape=(?, 2), dtype=float32)\n",
      "log:  Tensor(\"Log_2:0\", shape=(?, 2), dtype=float32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nin thsi particular env, the apst actions and observation can be safely ignored\\nsince each observation contains the environment full state. If there were some hidden state\\nthen you may need to consider past actions and observations in order to try ot\\ninfer the hidden state of the environemtn for example if environment only reveealed the position and not the velocit\\nwe would have to consider past and presnt.\\n\\nanother exampleis if observations are noisy\\n\\nwe are picking one random action \\nYou may wonder why we are picking a random action \\nbased on the probability given by the policy network,\\nrather than just picking the action with the highest probability. \\nThis approach lets the agent find the right balance between exploring new actions and exploiting the actions \\nthat are known to work well. \\nHere's an analogy: suppose you go to a restaurant for the first time, \\nand all the dishes look equally appealing so you randomly pick one. If it turns out to be good, you can increase the probability to order it next time, but you shouldn't increase that probability to 100%, or else you will never try out the other dishes, some of which may be even better than the one you tried.\\n\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets create a neral network that will take observation as inputs and output the action to take for\n",
    "# each observation. To choose an action the network will first estimate\n",
    "# a probability for each action then select an action randomly according to etimated probabilities\n",
    "# in case of cart pole environment\n",
    "# there are two action left or right so we onyl need one output neuton\n",
    "# it will output a p probability of p and 1 - p will be the probability for the right\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# specify the network architechture\n",
    "n_inputs = 4 # == env.observation_space.shape[0]\n",
    "n_hidden = 4 # it a simple task, we dont need more than this\n",
    "n_outputs = 1 # only outputs the probability of accelerating left\n",
    "initializer = tf.variance_scaling_initializer()\n",
    "\n",
    "# build the network\n",
    "X = tf.placeholder(tf.float32, shape=[None,n_inputs])\n",
    "hidden = tf.layers.dense(X, n_hidden, activation=tf.nn.elu,\n",
    "                         kernel_initializer = initializer)\n",
    "outputs = tf.layers.dense(hidden, n_outputs, activation=tf.nn.elu,\n",
    "                          kernel_initializer=initializer)\n",
    "# if there were more twopossible actions we would have used softmax\n",
    "\n",
    "# 3 select a random action based on estimated probailities\n",
    "p_left_and_right = tf.concat(axis=1, values=[outputs, 1- outputs])\n",
    "# print(outputs)\n",
    "# print(p_left_and_right)\n",
    "# print(\"log: \", tf.log(p_left_and_right))\n",
    "\n",
    "\"\"\"\n",
    "Lastly, we call the multinomial() function to pick a random action. This func‐\n",
    "tion independently samples one (or more) integers, given the log probability of\n",
    "each integer. For example, if you call it with the array [np.log(0.5),\n",
    "np.log(0.2), np.log(0.3)] and with num_samples=5, then it will output five\n",
    "integers, each of which will have a 50% probability of being 0, 20% of being 1,\n",
    "and 30% of being 2. In our case we just need one integer representing the action\n",
    "to take. Since the outputs tensor only contains the probability of going left, we\n",
    "must first concatenate 1-outputs to it to have a tensor containing the probability\n",
    "of both left and right actions. Note that if there were more than two possible\n",
    "actions, the neural network would have to output one probability per action so\n",
    "you would not need the concatenation step\n",
    "\"\"\"\n",
    "\n",
    "action = tf.multinomial(tf.log(p_left_and_right), num_samples=1)\n",
    "\n",
    "init = tf.global_variables_initializer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly playing onegame\n",
    "# without training the network\n",
    "\n",
    "n_max_steps = 1000\n",
    "frames = []\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    obs = env.reset()\n",
    "    for step in range(n_max_steps):\n",
    "        img = render_cart_pole(env, obs)\n",
    "        frames.append(img)\n",
    "        action_val = action.eval(feed_dict={X: obs.reshape(1,n_inputs )})\n",
    "        # this line choses the action with most probability at 0,0\n",
    "        \n",
    "        obs, reward, done, info = env.step(action_val[0][0])\n",
    "        if done:\n",
    "            break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVQAAADnCAYAAABBu67aAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAADp0lEQVR4nO3b7UnDUBiAUSNZwjlcwznqGq5h53AN53CM+MsPtLRiH3treg4ESi6U90d4uBeSaVmWKwCOdz16AIC1EFSAiKACRAQVICKoAJH5wLpXAAC+m3bdtEMFiAgqQERQASKCChARVICIoAJEBBUgIqgAEUEFiAgqQERQASKCChARVICIoAJEBBUgIqgAEUEFiAgqQERQASKCChARVICIoAJEBBUgIqgAEUEFiAgqQERQASKCChARVICIoAJEBBUgIqgAEUEFiAgqQERQASKCChARVICIoAJEBBUgIqgAEUEFiAgqQERQASKCChARVICIoAJEBBUgIqgAEUEFiAgqQERQASKCChARVICIoAJEBBUgIqgAEUEFiAgqQERQASKCChARVICIoAJEBBUgIqgAEUEFiAgqQERQASKCChARVICIoAJEBBUgIqgAEUEFiAgqQERQASKCChARVICIoAJEBBUgIqgAEUEFiAgqQERQASKCChARVICIoAJEBBUgIqgAEUEFiAgqQERQASKCChARVIDIPHoALtfz9n7n/dvN44kngYYdKkBEUAEiggoQEVSAiKACRAQVICKoABFBBYgIKkBEUAEiggoQEVSAiKACRAQVICKoABFBBYgIKkBEUAEiggoQEVSAiKACRAQVICKoABFBBYgIKkBEUAEiggoQEVSAiKACRAQVICKoABFBBYgIKkBEUAEiggoQEVSAiKACRAQVICKoABFBBYgIKkBEUAEiggoQEVSAiKACRAQVICKoABFBBYgIKkBEUAEiggoQEVSAiKACRAQVICKoABFBBYgIKkBEUAEiggoQEVSAiKCSmqbpx9ex/wHnRlABIvPoAbhsTy+b9993N9uBk8DxBJWz8RFXYeV/cuRnmM+7U1gDQWUYR3zWxpGfs/EW2IexY8CvTcuy7FvfuwhfnfJ1pgPPLvylnQ+6Iz9ARFABIoIKEBFUgIigAkQEFSAiqAARQQWICCpAxKenpHy9xCWzQwWICCpARFABIoIKEBFUgIigAkQEFSAiqAARQQWICCpARFABIoIKEBFUgIigAkQEFSAiqAARQQWICCpARFABIoIKEBFUgIigAkQEFSAiqAARQQWICCpARFABIoIKEBFUgIigAkQEFSAiqAARQQWICCpARFABIoIKEBFUgIigAkQEFSAiqAARQQWICCpARFABIoIKEBFUgMh8YH06yRQAK2CHChARVICIoAJEBBUgIqgAEUEFiLwCHM0dLDq63/wAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_animation(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training theneural network\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "tf.reset_default_graph()\n",
    "tf.set_random_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "n_inputs = 4\n",
    "n_hidden = 4\n",
    "n_outputs = 1\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "initializer = tf.variance_scaling_initializer()\n",
    "\n",
    "X = tf.placeholder(tf.float32,shape=[None, n_inputs])\n",
    "y = tf.placeholder(tf.float32, shape=[None, n_outputs])\n",
    "\n",
    "hidden = tf.layers.dense(X, n_hidden, activation=tf.nn.elu, kernel_initializer=initializer )\n",
    "logits = tf.layers.dense(hidden,n_outputs)\n",
    "outputs =tf.nn.sigmoid(logits)\n",
    "# probability of 0 is left\n",
    "\n",
    "p_left_and_right = tf.concat(axis=1, values=[outputs, 1-outputs])\n",
    "action = tf.multinomial(tf.log(p_left_and_right), num_samples=1)\n",
    "# wtf is this p_left_and_right\n",
    "\n",
    "cross_entropy = tf.nn.sigmoid_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "training_op = optimizer.minimize(cross_entropy)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_environments = 10\n",
    "n_iterations = 1000\n",
    "\n",
    "envs = [gym.make(\"CartPole-v0\") for _ in range(n_environments)]\n",
    "observations = [env.reset() for env in envs]\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for iteration in range(n_iterations):\n",
    "        target_probas = np.array([([1.] if obs[2] < 0 else [0.]) for obs in observations]) \n",
    "        # if angle<0 we want proba(left)=1., or else proba(left)=0.\n",
    "        action_val, _ = sess.run([action, training_op], feed_dict={X: np.array(observations), y: target_probas})\n",
    "        for env_index, env in enumerate(envs):\n",
    "            obs, reward, done, info = env.step(action_val[env_index][0])\n",
    "            observations[env_index] = obs if not done else env.reset()\n",
    "    saver.save(sess, \"./my_policy_net_basic.ckpt\")\n",
    "\n",
    "for env in envs:\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_policy_net(model_path, aciton, X, n_max_steps=1000):\n",
    "    frames = []\n",
    "    env = gym.make(\"CartPole-v0\")\n",
    "    obs = env.reset()\n",
    "    with tf.Session() as sess:\n",
    "        saver.restore(sess, model_path)\n",
    "        for step in range(n_max_steps):\n",
    "            img = render_cart_pole(env, obs)\n",
    "            frames.append(img)\n",
    "            action_val = action.eval(feed_dict={X:obs.reshape(1, n_inputs )})\n",
    "            obs, reward, done, info = env.step(action_val[0][0])\n",
    "            if done:\n",
    "                break\n",
    "    env.close()\n",
    "    return frames\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVQAAADnCAYAAABBu67aAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAEDklEQVR4nO3cwU1bQRRA0f8jN5E6QhmpA9qgDagjZZA6UoazygKCYhA3/jPDOTuwkWaBrt73G9jP5/MGwMd9OfoAAKsQVICIoAJEBBUgIqgAkdOF110BAPjb/to3TagAEUEFiAgqQERQASKCChARVICIoAJEBBUgIqgAEUEFiAgqQERQASKCChARVICIoAJEBBUgIqgAEUEFiAgqQERQASKCChARVICIoAJEBBUgIqgAEUEFiAgqQERQASKCChARVICIoAJEBBUgIqgAEUEFiAgqQERQASKCChARVICIoAJEBBUgIqgAEUEFiAgqQERQASKCChARVICIoAJEBBUgIqgAEUEFiAgqQERQASKCChARVICIoAJEBBUgIqgAEUEFiAgqQERQASKCChARVICIoAJEBBUgIqgAEUEFiAgqQERQASKCChARVICIoAJEBBUgIqgAEUEFiAgqQERQASKCChARVICIoAJEBBUgIqgAEUEFiAgqQERQASKCChARVICIoAJEBBUgIqgAEUEFiAgqQERQASKCChARVIDI6egDwCU/H++eff3t9uGgk8C/mVAZ2suYwsgEFSAiqEzH1MqoBBUgIqgAEUFlaDb6zERQASKCChARVKZk08+IBBUgIqgMz2KKWQgqQERQmZbPURmNoAJEBBUgIqgAEUFlCjb9zEBQASKCyjRem1Jt+hmJoAJEBBUgIqhMz2M/oxBUgIigAkQElam4j8rIBBUgIqgswWKKEQgqQERQASKCynQsphiVoAJEBBUgIqgsw6afowkqQERQASKCypRs+hmRoAJEBJWlWExxJEEFiAgqQERQmZbFFKMRVICIoLIciymOIqgAEUEFiAgqU7OYYiSCChARVJZkMcURBJXpeexnFIIKEBFUluWxn2sTVICIoAJEBJUlWEwxAkEFiAgqQERQWZpNP9ckqAARQQWInI4+AFyy7/ub3/v0cPuhn9+2bTufz+96P/xhQgWImFBZzo9fz6fUp4dtu7l7POg0fCYmVJb3MrDwvwgqSxFPjiSoLOX+/uboI/CJCSrL+/7V56dcx37hioj7IxzuvdeePsq1Kd7g1V9KEypARFABIoIKEBFUgIigAkQEFSAiqAARQQWICCpAxL/vY3j+colZmFABIoIKEBFUgIigAkQEFSAiqAARQQWICCpARFABIoIKEBFUgIigAkQEFSAiqAARQQWICCpARFABIoIKEBFUgIigAkQEFSAiqAARQQWICCpARFABIoIKEBFUgIigAkQEFSAiqAARQQWICCpARFABIoIKEBFUgIigAkQEFSAiqAARQQWICCpARFABIoIKEBFUgIigAkQEFSByuvD6fpVTACzAhAoQEVSAiKACRAQVICKoABFBBYj8Bg2XRwuZYfwnAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "frames =render_policy_net(\"./my_policy_net_basic.ckpt\", action, X)\n",
    "plot_animation(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "we need to define the target probabilities y\n",
    "If an action is good we must increaseits probability\n",
    "and if its bad then decrease its probability. But how do we know if\n",
    "an action is good or bad\n",
    "The problem is that most actions hae delayed effects so \n",
    "if we win and lose points in a game\n",
    "it is not clear whcic actions have delayed effects so when we los and win it si\n",
    "not clear that which actions cnotributed to us winning so theis is called credit assignment problem\n",
    "\n",
    "The policy gradients algorithm takckles this problem by first playing\n",
    "multiple games then making the action isngood games sloghltr more likely while actions in bad\n",
    "games are made slightly less likely\n",
    "fiirst we plat then we go back and thngk what we did right and bad\n",
    "\"\"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.reset_default_graph()\n",
    "tf.set_random_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "n_inputs = 4 # no of states of the environment\n",
    "n_hidden =4\n",
    "n_outputs = 1 #only one action selected\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "initializer = tf.variance_scaling_initializer()\n",
    "\n",
    "X = tf.placeholder(tf.float32,shape=[None, n_inputs])\n",
    "\n",
    "hidden = tf.layers.dense(X, n_hidden,activation=tf.nn.elu,\n",
    "                         kernel_initializer=initializer)\n",
    "logits = tf.layers.dense(hidden, n_outputs)\n",
    "outputs = tf.nn.sigmoid(logits)\n",
    "p_left_and_right = tf.concat(axis=1,values=[outputs, 1-outputs])\n",
    "action  = tf.multinomial(tf.log(p_left_and_right), num_samples=1)\n",
    "\n",
    "y = 1. - tf.to_float(action)\n",
    "# since we are saying the left is the desired action\n",
    "\n",
    "\n",
    "cross_entropy = tf.nn.sigmoid_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "\n",
    "# we are using compute gradients instead of minimizing loss\n",
    "# compute _gradients returns a list of gradient vecot/ variable pairs (one pair per traineable variable)\n",
    "grads_and_vars = optimizer.compute_gradients(cross_entropy)\n",
    "\n",
    "# putting all the gradients in a list\n",
    "gradients = [grad for grad, variable in grads_and_vars]\n",
    "\n",
    "# during the execution part the algoritm will run the policy and at each step evaluate theier values\n",
    "# after a number of episodes it will tweak the values\n",
    "# and then compute the mean of tweaked gradients as explained earlier\n",
    "# and wwe compute thte mean of tweaked gradients\n",
    "gradients_placeholders = []\n",
    "grads_and_vars_feed = []\n",
    "\n",
    "# we need one placeholder  per gradient vector\n",
    "for grad, variable in grads_and_vars:\n",
    "    gradient_placeholder = tf.placeholder(tf.float32, shape=grad.get_shape())\n",
    "    gradient_placeholders.append(gradient_placeholder)\n",
    "    grads_and_vars_feed.append((gradient_placeholder, variable))\n",
    "\n",
    "# apply _gradients to do the training op\n",
    "# instead of giving it the original vectors we will give it updated vectors\n",
    "training_op = optimizer.apply_gradients(grads_and_vars_feed)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to return the total discounted rewards \n",
    "# given raw rewards\n",
    "def discount_rewards(rewards, discount_rate):\n",
    "    discounted_rewards = np.zeros(len(rewards))\n",
    "    cumulative_rewards = 0\n",
    "    for step in reversed(range(len(rewards))):\n",
    "        cumulative_rewards = rewards[step] + cumulative_rewards * discount_rate\n",
    "        discounted_rewards[step] = cumulative_rewards\n",
    "    return discounted_rewards\n",
    "\n",
    "# discount and normalize rewards as the name says\n",
    "def discount_and_normalize_rewards(all_rewards, discount_rate):\n",
    "    all_discounted_rewards = [discount_rewards(rewards, discount_rate) for rewards in all_rewards]\n",
    "    flat_rewards = np.concatenate(all_discounted_rewards)\n",
    "    reward_mean = flat_rewards.mean()\n",
    "    reward_std = flat_rewards.std()\n",
    "    return [(discounted_rewards -reward_mean) / reward_std for discounted_rewards in all_discounted_rewards]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-22., -40., -50.])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "discount_rewards([10,0,-50],discount_rate=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([-0.28435071, -0.86597718, -1.18910299]),\n",
       " array([1.26665318, 1.0727777 ])]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "discount_and_normalize_rewards([[10,0,-50], [10,20]], discount_rate=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Iteration: 0"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-87-ea5d8dfaeedc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mvar_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient_placeholder\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgradient_placeholders\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m             mean_gradients = np.mean([reward * all_gradients[game_index][step][var_index]\n\u001b[1;32m---> 33\u001b[1;33m                                       \u001b[1;32mfor\u001b[0m \u001b[0mgame_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrewards\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_rewards\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m                                           for step, reward in enumerate(rewards)], axis=0)\n\u001b[0;32m     35\u001b[0m             \u001b[0mfeed_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mgradient_placeholder\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmean_gradients\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-87-ea5d8dfaeedc>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     32\u001b[0m             mean_gradients = np.mean([reward * all_gradients[game_index][step][var_index]\n\u001b[0;32m     33\u001b[0m                                       \u001b[1;32mfor\u001b[0m \u001b[0mgame_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrewards\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_rewards\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m                                           for step, reward in enumerate(rewards)], axis=0)\n\u001b[0m\u001b[0;32m     35\u001b[0m             \u001b[0mfeed_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mgradient_placeholder\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmean_gradients\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v0\")\n",
    "\n",
    "n_games_per_update = 10\n",
    "n_max_steps = 1000\n",
    "n_iterations = 250\n",
    "save_iterations = 10\n",
    "discount_rate = 0.95\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for iteration in range(n_iterations):\n",
    "        print(\"\\rIteration: {}\".format(iteration), end=\"\")\n",
    "        all_rewards = []\n",
    "        all_gradients = []\n",
    "        for game in range(n_games_per_update):\n",
    "            current_rewards = []\n",
    "            current_gradients = []\n",
    "            obs = env.reset()\n",
    "            for step in range(n_max_steps):\n",
    "                action_val, gradients_val = sess.run([action, gradients], feed_dict={X: obs.reshape(1, n_inputs)})\n",
    "                obs, reward, done, info = env.step(action_val[0][0])\n",
    "                current_rewards.append(reward)\n",
    "                current_gradients.append(gradients_val)\n",
    "                if done:\n",
    "                    break\n",
    "            all_rewards.append(current_rewards)\n",
    "            all_gradients.append(current_gradients)\n",
    "\n",
    "        all_rewards = discount_and_normalize_rewards(all_rewards, discount_rate=discount_rate)\n",
    "        feed_dict = {}\n",
    "        for var_index, gradient_placeholder in enumerate(gradient_placeholders):\n",
    "            mean_gradients = np.mean([reward * all_gradients[game_index][step][var_index]\n",
    "                                      for game_index, rewards in enumerate(all_rewards)\n",
    "                                          for step, reward in enumerate(rewards)], axis=0)\n",
    "            feed_dict[gradient_placeholder] = mean_gradients\n",
    "        sess.run(training_op, feed_dict=feed_dict)\n",
    "        if iteration % save_iterations == 0:\n",
    "            saver.save(sess, \"./my_policy_net_pg.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = render_policy_net(\"./my_policy_net_pg.ckpt\", action,X, n_max_steps=1000 )\n",
    "plot_animation(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transition_probabilities = [\n",
    "    [0.7, 0.2, 0.0,0.1],\n",
    "    [0.0,0.0,0.9,0.1],\n",
    "    [0.0,1.0,0.0,0.0],\n",
    "    [0.0,0.0,0.0,1.0],\n",
    "]\n",
    "\n",
    "n_max_steps = 50\n",
    "\n",
    "def print_sequence(start_state=0):\n",
    "    current_state = start_state\n",
    "    print(\"States:\", end=\" \")\n",
    "    for step in range(n_maX_steps):\n",
    "        print(current_state, end=\" \")\n",
    "        if current_state == 3:\n",
    "            break\n",
    "        current_state = np.random.choice(range(4), p=transition_probabilities[current_state])\n",
    "        else:\n",
    "            print(\"...\",end=\"\")\n",
    "        print()\n",
    "\n",
    "for _ in range(10):\n",
    "    print_sequence()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov decision process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transition_probabilities = [\n",
    "    [[0.7,0.3,0.0], [1.0,0.0,0.0], [0.8,0.2,0.0]],\n",
    "    [[0.0,1.0,0.0], None, [0.0,0.0,1.0]],\n",
    "    [None,[0.8,0.1,0.1], None],\n",
    "]\n",
    "\n",
    "rewards = [\n",
    "    [[+10,0,0],[0,0,0],[0,0,0]],\n",
    "    [[0,0,0],[0,0,0],[0,0,-50]],\n",
    "    [[0,0,0],[+40,0,0],[0,0,0]],\n",
    "]\n",
    "\n",
    "possible_actions = [[0,1,2],[0,2],[1]]\n",
    "\n",
    "def policy_fire(state):\n",
    "    return [0,2,1][state]\n",
    "\n",
    "def policy_random(state):\n",
    "    return np.random.choice(possible_actions[state])\n",
    "\n",
    "def policy_safe(state):\n",
    "    retunr [0,0,1][state]\n",
    "\n",
    "def MDPEnvironement(object):\n",
    "    def __init__(self, start_stae=0):\n",
    "        self.start_state = start_state\n",
    "        self.reset()\n",
    "    def reset(self):\n",
    "        self.total_rewards = 0\n",
    "        self.state = self.start_state\n",
    "    def step(self,action):\n",
    "        next_state = np.random.choice(range(3), p=transition_probabilities[self.state][action])\n",
    "        reward = rewards[self.state][action][next_state]\n",
    "        self.state = next_state\n",
    "        self.total_rewards += reward\n",
    "        return self.state, reward\n",
    "    \n",
    "def run_rpisode(policy, n_steps,start_state=0, display=True):\n",
    "    env = MDPEnvironment()\n",
    "    if display:\n",
    "        print(\"states (+rewards): \", end=\" \")\n",
    "    for step in range(n_steps):\n",
    "        if display:\n",
    "            if step == 10:\n",
    "                print(\"...\", end=\" \")\n",
    "            elif step < 10:\n",
    "                print(env.state, end=\" \")\n",
    "        action = policy(env.state)\n",
    "        state, reward = env.step(action)\n",
    "        if display and step <10:\n",
    "            if reward:\n",
    "                print(\"({})\".format(reward), end=\" \")\n",
    "    if display:\n",
    "        print(\"Total rewards = \", env.total_rewards)\n",
    "    return env.total_rewards\n",
    "\n",
    "for policy in (policy_fire, policy_random,policy_safe):\n",
    "    all_totals = []\n",
    "    print(policy.__name__)\n",
    "    for episode in range(1000):\n",
    "        all_totals.append(run_episode(policy, n_steps=100, display=(episode<5)))\n",
    "    print(\"Summary: mean={:.1f},std={:1f}, min={}, max={}\".format(np.mean(all_totals), np.std(all_totals), np.min(all_totals),np.max(all_totals) ))\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf_gpu)",
   "language": "python",
   "name": "tf_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
