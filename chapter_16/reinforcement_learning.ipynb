{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To support both python 2 and python 3\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "# To plot pretty figures and animations\n",
    "%matplotlib nbagg\n",
    "import matplotlib\n",
    "import matplotlib.animation as animation\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"rl\"\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True):\n",
    "    path = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID, fig_id + \".png\")\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format='png', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open ai introductinoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('MsPacman-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the environemnt is initialised by reset method\n",
    "obs = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(210, 160, 3)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# observation vary depending on the nevironemnt\n",
    "# in this case it is an RGB image represented as a 3D nUmpy array\n",
    "# of shape [width, height, channels]\n",
    "\n",
    "obs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# an environemnt can be visualized by calling its render() method\n",
    "# in this examplewe will set the mode=\"rgb_array\"\n",
    "\n",
    "img = env.render(mode=\"rgb_array\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython import display\n",
    "%matplotlib inline\n",
    "plt.figure(figsize=(5,4))\n",
    "# for i in range(25):\n",
    "#     plt.imshow(img)\n",
    "#     display.display(plt.gcf())    \n",
    "#     display.clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(img == obs).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to plot th eenvironment\n",
    "def plot_environment(env, figsize=(5,4)):\n",
    "    plt.close()\n",
    "    plt.figure(figsize=figsize)\n",
    "    img = env.render(mode=\"rgb_array\")\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    display.display(plt.gcf())\n",
    "    display.clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(9)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space\n",
    "# Discrete(9) means that the possible actions are integers 0 through 8, which represents the 9 possible positions of the joystick (0=center, 1=up, 2=right, 3=left, 4=down, 5=upper-right, 6=upper-left, 7=lower-right, 8=lower-left)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "for step in range(110):\n",
    "    env.step(3)\n",
    "for step in range(40):\n",
    "    env.step(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot_environment(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the step function\n",
    "obs, reward , done, info =env.step(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(210, 160, 3)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "done\n",
    "# when game is over done returns true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ale.lives': 3}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info\n",
    "# its an environemtn specific dictionary that can provide some extra info about the\n",
    "# internal state of environment,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# playing one full game with 3 lives\n",
    "frames = []\n",
    "n_max_steps = 1000\n",
    "n_change_steps = 10\n",
    "\n",
    "obs = env.reset()\n",
    "\n",
    "for step in range(n_max_steps):\n",
    "    img = env.render(mode=\"rgb_array\")\n",
    "    frames.append(img)\n",
    "    if step % n_change_steps == 0:\n",
    "        action = env.action_space.sample()\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    if done:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to show the animation\n",
    "from IPython import display\n",
    "%matplotlib inline\n",
    "def update_scene(num,frames,patch):\n",
    "    patch.set_data(frames[num])\n",
    "    return patch\n",
    "\n",
    "def plot_animation(frames, repeat=False, interval=40):\n",
    "    plt.close()\n",
    "    fig = plt.figure()\n",
    "    for f in frames:\n",
    "        plt.imshow(f)\n",
    "        plt.axis('off')\n",
    "        display.display(plt.gcf())\n",
    "        display.clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot_animation(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cartpole rendering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trying a differne tpproach for rendering \n",
    "# answer taken from\n",
    "# https://stackoverflow.com/questions/52726475/display-openai-gym-in-jupyter-notebook-only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "from IPython import display\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "obs = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.01911864,  0.00549671, -0.01746714,  0.00916598])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageDraw\n",
    "\n",
    "try:\n",
    "    from pyglet.gl import gl_info\n",
    "    openai_cart_pole_rendering=True\n",
    "except Exception:\n",
    "    openai_cart_pole_rendering=False\n",
    "\n",
    "def render_cart_pole(env, obs):\n",
    "    if openai_cart_pole_rendering:\n",
    "        return env.render(mode=\"rgb_array\")\n",
    "    else:\n",
    "        # rendering forcart pole environment\n",
    "        # so badass ageron\n",
    "        print(\"self rendering\")\n",
    "        img_w = 600\n",
    "        img_h = 400\n",
    "        cart_w = img_w //12\n",
    "        cart_h = img_h //15\n",
    "        pole_len = img_h //3.5\n",
    "        ple_w = img_w // 80 + 1\n",
    "        x_width = 2\n",
    "        max_ang = 0.2\n",
    "        bg_col = (255,255,255)\n",
    "        cart_col = 0x000000\n",
    "        pole_col = 0x669acc\n",
    "        \n",
    "        pos,vel, ang, ang_vel = obs\n",
    "        img = Image.new('RGB', (img_w, img_h), bg_col)\n",
    "        draw = ImageDraw.Draw(img)\n",
    "        cart_x = pos * img_w // x_width + img_w // x_width\n",
    "        cart_y = img_h * 95 // 100\n",
    "        top_pole_x = cart_x + pole_len * np.sin(ang)\n",
    "        top_pole_y = cart_y - cart_h // 2 - pole_len * np.cos(ang)\n",
    "        draw.line((0, cart_y, img_w, cart_y), fill=0)\n",
    "        draw.rectangle((cart_x, cart_y - cart_h // 2, top_pole_x,top_pole_y ), fill=pole_col, width=pole_w)\n",
    "        draw.line((cart_x, cart_y - cart_h//2, top_pole_x, top_pole_y ), fill=pole_col, width=pole_w)\n",
    "        return np.array(img)\n",
    "\n",
    "def plot_cart_pole(env,obs):\n",
    "    plt.close()\n",
    "    image = render_cart_pole(env, obs)\n",
    "    plt.imshow(image)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVQAAADnCAYAAABBu67aAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAADqklEQVR4nO3c0U3CUBiAUWu6hHOwhnPgTDCHaziHY9Q3Y6RSgh+2knMSEugNyf/QfLkppcM0TQ8A/N7j2gMA3AtBBYgIKkBEUAEiggoQGRfW3QIAcGqYO2iHChARVICIoAJEBBUgIqgAEUEFiAgqQERQASKCChARVICIoAJEBBUgIqgAEUEFiAgqQERQASKCChARVICIoAJEBBUgIqgAEUEFiAgqQERQASKCChARVICIoAJEBBUgIqgAEUEFiAgqQERQASKCChARVICIoAJEBBUgIqgAEUEFiAgqQERQASKCChARVICIoAJEBBUgIqgAEUEFiAgqQERQASKCChARVICIoAJEBBUgIqgAEUEFiAgqQERQASKCChARVICIoAJEBBUgIqgAEUEFiAgqQERQASKCChARVICIoAJEBBUgIqgAEUEFiAgqQERQASKCChARVICIoAJEBBUgIqgAEUEFiAgqQERQASKCChARVICIoAJEBBUgIqgAEUEFiAgqQERQASKCChARVICIoAJEBBUgIqgAkXHtAeCct+PLybHd/rDCJLDMDhUgIqgAEUEFiAgqQERQASKCyr8z98s/bIGgAkQEFSAiqAARQQWICCpARFABIoIKEBFUgIigAkQEFSAiqAARQQWICCpARFABIoIKEBFUgIigAkQEFSAiqAARQQWICCpARFABIoIKEBFUgIigAkQEFSAiqAARQQWICCpARFABIoIKEBFUNm23P6w9AlxMUAEiggoQEVSAiKACRAQVICKoABFBBYgIKkBEUAEiggoQEVSAiKACRAQVICKoABFBBYgIKkBEUAEiggoQEVSAiKACRAQVICKoABFBZRXDMFz8usX34RYEFSAyrj0AXOL1ff/5/vnpuOIk8DM7VDbva0znPsNWCCpARFABIoLK5n2/ZuoaKls1TNN0bv3sIlzrL29nWjjH4RqzJ7AdKkBEUAEiggoQEVSAiKACRAQVICKoABFBBYgIKkDE4/tYhX8vcY/sUAEiggoQEVSAiKACRAQVICKoABFBBYgIKkBEUAEiggoQEVSAiKACRAQVICKoABFBBYgIKkBEUAEiggoQEVSAiKACRAQVICKoABFBBYgIKkBEUAEiggoQEVSAiKACRAQVICKoABFBBYgIKkBEUAEiggoQEVSAiKACRAQVICKoABFBBYgIKkBEUAEiggoQEVSAiKACRAQVIDIurA9/MgXAHbBDBYgIKkBEUAEiggoQEVSAiKACRD4AmqsfEUQ28RAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot_cart_pole(env, obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# the other way of rendering cartpole\n",
    "\n",
    "for i in range(25):\n",
    "   plt.imshow(env.render(mode='rgb_array'))\n",
    "   display.display(plt.gcf())    \n",
    "   display.clear_output(wait=True)\n",
    "   env.step(env.action_space.sample()) # take a random action\n",
    "\n",
    "# doesntwork that well because it leaves a dead kernel \n",
    "# when we close the other window\n",
    "# so dont close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(2)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.5, 599.5, 399.5, -0.5)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVQAAADnCAYAAABBu67aAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAED0lEQVR4nO3c0W0aQRRAUW9EE6kjLiN14DbcBtSRMpw6UsbmL0ocZOzkmp1ZnyPxAQvSfKCrYR6wrOt6B8D/+7T1AgD2QlABIoIKEBFUgIigAkQOV677CgDA35ZLD9qhAkQEFSAiqAARQQWICCpARFABIoIKEBFUgIigAkQEFSAiqAARQQWICCpARFABIoIKEBFUgIigAkQEFSAiqAARQQWICCpARFABIoIKEBFUgIigAkQEFSAiqAARQQWICCpARFABIoIKEBFUgIigAkQEFSAiqAARQQWICCpARFABIoIKEBFUgIigAkQEFSAiqAARQQWICCpARFABIoIKEBFUgIigAkQEFSAiqAARQQWICCpARFABIoIKEBFUgIigAkQEFSAiqAARQQWICCpARFABIoIKEBFUgIigAkQEFSAiqAARQQWICCpARFABIoIKEBFUgIigAkQEFSAiqAARQQWICCpARFABIoIKEBFUgIigAkQEFSAiqAARQQWICCpARFABIoIKEBFUgIigAkQEFSAiqAARQQWICCpARFABIoetFwCv9f388Mf9L8fTRiuBy+xQmdbzwMLWBBUgIqhMwW6UGQgqQERQASKCyrRM+RmNoAJEBJXhGUgxC0EFiAgqQERQmZKBFCMSVICIoDI0AylmIqgAEUEFiAgqQERQmY4JP6MSVIZlIMVsBBUgIqgAEUEFiAgqUzGQYmSCChARVIZkws+MBBUgIqgAEUFlGgZSjE5QASKCynAuDaTsTpmBoAJEBBUgIqgAEUEFiAgqQzGQYmaCChARVIbh9/vMTlABIoIKEBFUgIigMjQTfmYiqAzBQIo9EFSAiKACRAQVICKoDMtAitkIKkBEUNmcCT97IagAEUFlSM5PmZGgAkQEFSAiqGzKQIo9EVSAiKACRASV4ZjwMytBBYgIKpsxkGJvBBUgIqgAEUElsyzLm26X3D+c//m1sDVBBYgctl4AH9PT6Xj37cfx1/2vn88brgYay7quL11/8SL87i0fxR8fny48dv/q119538J7u/hm95Gfm3s6Ha8/CSYkqAARQWUTz89MnaGyB85Qydzy60zOUNmYM1SA9ySoABFBBYgIKkBEUAEiggoQEVSAiKACRAQVIOLv+8j49RIfnR0qQERQASKCChARVICIoAJEBBUgIqgAEUEFiAgqQERQASKCChARVICIoAJEBBUgIqgAEUEFiAgqQERQASKCChARVICIoAJEBBUgIqgAEUEFiAgqQERQASKCChARVICIoAJEBBUgIqgAEUEFiAgqQERQASKCChARVICIoAJEBBUgIqgAEUEFiAgqQERQASKCChARVICIoAJEDleuLzdZBcAO2KECRAQVICKoABFBBYgIKkBEUAEiPwGNdEtvQ/Ob1QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# for cartpole each observation is a 1D array containing 4 floats\n",
    "# representing the carts horizontal position, angle of pole and the horizontal and angular velocity\n",
    "\n",
    "# to use render to return image as anarray we can use\n",
    "# env.render(\"rgb_array\")\n",
    "\n",
    "# cartpole has only two possible actions accelerate towards left or towards right\n",
    "# accelerating left till pole falls\n",
    "obs = env.reset()\n",
    "while True:\n",
    "    obs, reward, done, info = env.step(0)\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "plt.close()\n",
    "img = render_cart_pole(env, obs)\n",
    "plt.imshow(img)\n",
    "plt.axis(\"off\")\n",
    "\n",
    "# notice that the game fails when it tilts toot muc\n",
    "# and not when it actually falls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVQAAADnCAYAAABBu67aAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAEJUlEQVR4nO3c0W0TQRRAURu5CeqAMqgjaSNtxHVQBqmDMswfAhLFRLl45y3nSPlwNrHmw76afePkeLlcDgC834etFwCwF4IKEBFUgIigAkQEFSByunLdRwAAnju+9E07VICIoAJEBBUgIqgAEUEFiAgqQERQASKCChARVICIoAJEBBUgIqgAEUEFiAgqQERQASKCChARVICIoAJEBBUgIqgAEUEFiAgqQERQASKCChARVICIoAJEBBUgIqgAEUEFiAgqQERQASKCChARVICIoAJEBBUgIqgAEUEFiAgqQERQASKCChARVICIoAJEBBUgIqgAEUEFiAgqQERQASKCChARVICIoAJEBBUgIqgAEUEFiAgqQERQASKCChARVICIoAJEBBUgIqgAEUEFiAgqQERQASKCChARVICIoAJEBBUgIqgAEUEFiAgqQERQASKCChARVICIoAJEBBUgIqgAEUEFiAgqQERQASKCChARVICIoAJEBBUgIqgAEUEFiAgqQERQASKCChARVICIoAJEBBUgIqgAEUEFiAgqQERQWdLT+f7wdL7fehnwJqetFwCHw0E82QU7VJbw6e5x6yXAuwkqQERQWZpRAJMIKkBEUFmGOSrTCSpARFBZnjkqUwgqS3Hbz2SCChARVICIoDKCOSoTCCrLMUdlKkEFiAgqY7jtZ3WCChARVICIoLIkB1NMJKiMYo7KygQVICKoABFBZVnmqEwjqAARQWUcB1OsSlABIoIKEBFUluZgikkEFSAiqIzkYIoVCSpARFBZnjkqUwgqQERQGcscldUIKkBEUAEigsoIDqaYQFABIoLKaA6mWImgMobbflYnqIxnl8oqBBUgIqgAEUFlFHNUViaoABFBZRccTLECQQWICCpARFAZx8EUqxJUdsMcla0JKkBEUAEigspI5qisSFABIoLKrjiYYkuCChARVICIoDKWgylWI6gAEUFldxxMsRVBZVnH4/Hq13t+92+eB95CUBnt8/156yXAT6etFwCVr9/vfnkktNyeHSq78HtMD4eHh28brYT/maAyntt+ViGo7Na3x7vrPwQhQWUXvnw8P3ts58qtHS+Xy2vXX70I/9ItP8505X0Af3rxxWmHChARVICIoAJEBBUgIqgAEUEFiAgqQERQASKCChDx7/tYlr9eYho7VICIoAJEBBUgIqgAEUEFiAgqQERQASKCChARVICIoAJEBBUgIqgAEUEFiAgqQERQASKCChARVICIoAJEBBUgIqgAEUEFiAgqQERQASKCChARVICIoAJEBBUgIqgAEUEFiAgqQERQASKCChARVICIoAJEBBUgIqgAEUEFiAgqQERQASKCChARVICIoAJEBBUgIqgAEUEFiJyuXD/eZBUAO2CHChARVICIoAJEBBUgIqgAEUEFiPwAEipNSKVUrZgAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# lets pushto right\n",
    "obs = env.reset()\n",
    "while True:\n",
    "    obs, reward, done, info = env.step(1)\n",
    "    if done:\n",
    "        break\n",
    "#plot_cart_pole(env, obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A simple policy\n",
    "If tilting to left push right otherwise left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = []\n",
    "\n",
    "n_max_steps = 1000\n",
    "n_change_steps = 10\n",
    "\n",
    "obs = env.reset()\n",
    "for step in range(n_max_steps):\n",
    "    img = render_cart_pole(env,obs)\n",
    "    frames.append(img)\n",
    "#     plt.imshow(img)\n",
    "#     display.display(plt.gcf())    \n",
    "#     display.clear_output(wait=True)\n",
    "    # hard coded policy\n",
    "    position, velocity, angle, angular_velocity = obs\n",
    "    if angle<0:\n",
    "        action = 0\n",
    "    else:\n",
    "        action =1\n",
    "    \n",
    "    obs, reward, done , info = env.step(action)\n",
    "    if done:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVQAAADnCAYAAABBu67aAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAEB0lEQVR4nO3b3Y3aQBhAURzRROpIykgdbE1LHSkjqWPLIG+Rsj9sVnvx2MM5kh+wBZoHuBrrM8vlcjkA8HlfRi8AYBaCChARVICIoAJEBBUgcnznukcAAF5aXjtphwoQEVSAiKACRAQVICKoABFBBYgIKkBEUAEiggoQEVSAiKACRAQVICKoABFBBYgIKkBEUAEiggoQEVSAiKACRAQVICKoABFBBYgIKkBEUAEiggoQEVSAiKACRAQVICKoABFBBYgIKkBEUAEiggoQEVSAiKACRAQVICKoABFBBYgIKkBEUAEiggoQEVSAiKACRAQVICKoABFBBYgIKkBEUAEiggoQEVSAiKACRAQVICKoABFBBYgIKkBEUAEiggoQEVSAiKACRAQVICKoABFBBYgIKkBEUAEiggoQEVSAiKACRAQVICKoABFBBYgIKkBEUAEiggoQEVSAiKACRAQVICKoABFBBYgIKkBEUAEiggoQEVSAiKACRAQVICKoABFBBYgIKkBEUAEiggoQEVSAiKACRAQVICKoABFBBYgcRy8APuL3+eGf199Oj4NWAi/ZobIbz2P61jkYRVDZDbtRtk5QASKCyu657WcrBBUgIqgAEUFlVwym2DJBBYgIKkBEUJmCST9bIKgAEUEFiAgqu2PSz1YJKkBEUJmGwRSjCSpARFABIoLKLhlMsUWCChARVICIoDIVk35GElSAiKACRASV3TLpZ2sEFSAiqEzHYIpRBBUgIqgAEUFl1wym2BJBBYgIKlMymGIEQWX33PazFYIKEBFUpuW2n7UJKkBEUAEigsoUDKbYAkEFiAgqQERQmZpJP2sSVICIoAJEBJVpmPQzmqACRASV6RlMsRZBBYgIKkBEUJmKwRQjCSpARFABIoLKXTDpZw2CChARVICIoDIdk35GEVSAiKByNwymuDVBBYgIKkBEUJmSwRQjCCq7syzLfx2fee+1z4C3CCp35dfjafQSmJigMq3vD+fD4XA4/Hw6/T3glgSVqT2PqKhyS4LK3XHbz60IKkBEUJnaj6/nq6+htFwul2vXr16EEdZ8nOmd3wf369UvoR0qQERQASKCChARVICIoAJEBBUgIqgAEUEFiAgqQOQ4egHwUf69xFbZoQJEBBUgIqgAEUEFiAgqQERQASKCChARVICIoAJEBBUgIqgAEUEFiAgqQERQASKCChARVICIoAJEBBUgIqgAEUEFiAgqQERQASKCChARVICIoAJEBBUgIqgAEUEFiAgqQERQASKCChARVICIoAJEBBUgIqgAEUEFiAgqQERQASKCChARVICIoAJEBBUgIqgAEUEFiAgqQOT4zvVllVUATMAOFSAiqAARQQWICCpARFABIoIKEPkD2PdJq7Q0AngAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot_animation(frames)\n",
    "#system is unstable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"dense_3/Elu:0\", shape=(?, 1), dtype=float32)\n",
      "Tensor(\"concat_1:0\", shape=(?, 2), dtype=float32)\n",
      "log:  Tensor(\"Log_2:0\", shape=(?, 2), dtype=float32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nin thsi particular env, the apst actions and observation can be safely ignored\\nsince each observation contains the environment full state. If there were some hidden state\\nthen you may need to consider past actions and observations in order to try ot\\ninfer the hidden state of the environemtn for example if environment only reveealed the position and not the velocit\\nwe would have to consider past and presnt.\\n\\nanother exampleis if observations are noisy\\n\\nwe are picking one random action \\nYou may wonder why we are picking a random action \\nbased on the probability given by the policy network,\\nrather than just picking the action with the highest probability. \\nThis approach lets the agent find the right balance between exploring new actions and exploiting the actions \\nthat are known to work well. \\nHere's an analogy: suppose you go to a restaurant for the first time, \\nand all the dishes look equally appealing so you randomly pick one. If it turns out to be good, you can increase the probability to order it next time, but you shouldn't increase that probability to 100%, or else you will never try out the other dishes, some of which may be even better than the one you tried.\\n\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets create a neral network that will take observation as inputs and output the action to take for\n",
    "# each observation. To choose an action the network will first estimate\n",
    "# a probability for each action then select an action randomly according to etimated probabilities\n",
    "# in case of cart pole environment\n",
    "# there are two action left or right so we onyl need one output neuton\n",
    "# it will output a p probability of p and 1 - p will be the probability for the right\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# specify the network architechture\n",
    "n_inputs = 4 # == env.observation_space.shape[0]\n",
    "n_hidden = 4 # it a simple task, we dont need more than this\n",
    "n_outputs = 1 # only outputs the probability of accelerating left\n",
    "initializer = tf.variance_scaling_initializer()\n",
    "\n",
    "# build the network\n",
    "X = tf.placeholder(tf.float32, shape=[None,n_inputs])\n",
    "hidden = tf.layers.dense(X, n_hidden, activation=tf.nn.elu,\n",
    "                         kernel_initializer = initializer)\n",
    "outputs = tf.layers.dense(hidden, n_outputs, activation=tf.nn.elu,\n",
    "                          kernel_initializer=initializer)\n",
    "# if there were more twopossible actions we would have used softmax\n",
    "\n",
    "# 3 select a random action based on estimated probailities\n",
    "p_left_and_right = tf.concat(axis=1, values=[outputs, 1- outputs])\n",
    "# print(outputs)\n",
    "# print(p_left_and_right)\n",
    "# print(\"log: \", tf.log(p_left_and_right))\n",
    "\n",
    "\"\"\"\n",
    "Lastly, we call the multinomial() function to pick a random action. This func‐\n",
    "tion independently samples one (or more) integers, given the log probability of\n",
    "each integer. For example, if you call it with the array [np.log(0.5),\n",
    "np.log(0.2), np.log(0.3)] and with num_samples=5, then it will output five\n",
    "integers, each of which will have a 50% probability of being 0, 20% of being 1,\n",
    "and 30% of being 2. In our case we just need one integer representing the action\n",
    "to take. Since the outputs tensor only contains the probability of going left, we\n",
    "must first concatenate 1-outputs to it to have a tensor containing the probability\n",
    "of both left and right actions. Note that if there were more than two possible\n",
    "actions, the neural network would have to output one probability per action so\n",
    "you would not need the concatenation step\n",
    "\"\"\"\n",
    "\n",
    "action = tf.multinomial(tf.log(p_left_and_right), num_samples=1)\n",
    "\n",
    "init = tf.global_variables_initializer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly playing onegame\n",
    "# without training the network\n",
    "\n",
    "n_max_steps = 1000\n",
    "frames = []\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    obs = env.reset()\n",
    "    for step in range(n_max_steps):\n",
    "        img = render_cart_pole(env, obs)\n",
    "        frames.append(img)\n",
    "        action_val = action.eval(feed_dict={X: obs.reshape(1,n_inputs )})\n",
    "        # this line choses the action with most probability at 0,0\n",
    "        \n",
    "        obs, reward, done, info = env.step(action_val[0][0])\n",
    "        if done:\n",
    "            break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVQAAADnCAYAAABBu67aAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAADp0lEQVR4nO3b7UnDUBiAUSNZwjlcwznqGq5h53AN53CM+MsPtLRiH3treg4ESi6U90d4uBeSaVmWKwCOdz16AIC1EFSAiKACRAQVICKoAJH5wLpXAAC+m3bdtEMFiAgqQERQASKCChARVICIoAJEBBUgIqgAEUEFiAgqQERQASKCChARVICIoAJEBBUgIqgAEUEFiAgqQERQASKCChARVICIoAJEBBUgIqgAEUEFiAgqQERQASKCChARVICIoAJEBBUgIqgAEUEFiAgqQERQASKCChARVICIoAJEBBUgIqgAEUEFiAgqQERQASKCChARVICIoAJEBBUgIqgAEUEFiAgqQERQASKCChARVICIoAJEBBUgIqgAEUEFiAgqQERQASKCChARVICIoAJEBBUgIqgAEUEFiAgqQERQASKCChARVICIoAJEBBUgIqgAEUEFiAgqQERQASKCChARVICIoAJEBBUgIqgAEUEFiAgqQERQASKCChARVICIoAJEBBUgIqgAEUEFiAgqQERQASKCChARVIDIPHoALtfz9n7n/dvN44kngYYdKkBEUAEiggoQEVSAiKACRAQVICKoABFBBYgIKkBEUAEiggoQEVSAiKACRAQVICKoABFBBYgIKkBEUAEiggoQEVSAiKACRAQVICKoABFBBYgIKkBEUAEiggoQEVSAiKACRAQVICKoABFBBYgIKkBEUAEiggoQEVSAiKACRAQVICKoABFBBYgIKkBEUAEiggoQEVSAiKACRAQVICKoABFBBYgIKkBEUAEiggoQEVSAiKACRAQVICKoABFBBYgIKkBEUAEiggoQEVSAiKCSmqbpx9ex/wHnRlABIvPoAbhsTy+b9993N9uBk8DxBJWz8RFXYeV/cuRnmM+7U1gDQWUYR3zWxpGfs/EW2IexY8CvTcuy7FvfuwhfnfJ1pgPPLvylnQ+6Iz9ARFABIoIKEBFUgIigAkQEFSAiqAARQQWICCpAxKenpHy9xCWzQwWICCpARFABIoIKEBFUgIigAkQEFSAiqAARQQWICCpARFABIoIKEBFUgIigAkQEFSAiqAARQQWICCpARFABIoIKEBFUgIigAkQEFSAiqAARQQWICCpARFABIoIKEBFUgIigAkQEFSAiqAARQQWICCpARFABIoIKEBFUgIigAkQEFSAiqAARQQWICCpARFABIoIKEBFUgMh8YH06yRQAK2CHChARVICIoAJEBBUgIqgAEUEFiLwCHM0dLDq63/wAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot_animation(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training theneural network\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "tf.reset_default_graph()\n",
    "tf.set_random_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "n_inputs = 4\n",
    "n_hidden = 4\n",
    "n_outputs = 1\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "initializer = tf.variance_scaling_initializer()\n",
    "\n",
    "X = tf.placeholder(tf.float32,shape=[None, n_inputs])\n",
    "y = tf.placeholder(tf.float32, shape=[None, n_outputs])\n",
    "\n",
    "hidden = tf.layers.dense(X, n_hidden, activation=tf.nn.elu, kernel_initializer=initializer )\n",
    "logits = tf.layers.dense(hidden,n_outputs)\n",
    "outputs =tf.nn.sigmoid(logits)\n",
    "# probability of 0 is left\n",
    "\n",
    "p_left_and_right = tf.concat(axis=1, values=[outputs, 1-outputs])\n",
    "action = tf.multinomial(tf.log(p_left_and_right), num_samples=1)\n",
    "# wtf is this p_left_and_right\n",
    "# it is the combined arraay of output and 1 - ouput\n",
    "\n",
    "cross_entropy = tf.nn.sigmoid_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "training_op = optimizer.minimize(cross_entropy)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_environments = 10\n",
    "n_iterations = 1000\n",
    "\n",
    "envs = [gym.make(\"CartPole-v0\") for _ in range(n_environments)]\n",
    "observations = [env.reset() for env in envs]\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for iteration in range(n_iterations):\n",
    "        target_probas = np.array([([1.] if obs[2] < 0 else [0.]) for obs in observations]) \n",
    "        # if angle<0 we want proba(left)=1., or else proba(left)=0.\n",
    "        action_val, _ = sess.run([action, training_op], feed_dict={X: np.array(observations), y: target_probas})\n",
    "        for env_index, env in enumerate(envs):\n",
    "            obs, reward, done, info = env.step(action_val[env_index][0])\n",
    "            observations[env_index] = obs if not done else env.reset()\n",
    "    saver.save(sess, \"./my_policy_net_basic.ckpt\")\n",
    "\n",
    "for env in envs:\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_policy_net(model_path, aciton, X, n_max_steps=1000):\n",
    "    frames = []\n",
    "    env = gym.make(\"CartPole-v0\")\n",
    "    obs = env.reset()\n",
    "    with tf.Session() as sess:\n",
    "        saver.restore(sess, model_path)\n",
    "        for step in range(n_max_steps):\n",
    "            img = render_cart_pole(env, obs)\n",
    "            frames.append(img)\n",
    "            action_val = action.eval(feed_dict={X:obs.reshape(1, n_inputs )})\n",
    "            obs, reward, done, info = env.step(action_val[0][0])\n",
    "            if done:\n",
    "                break\n",
    "    env.close()\n",
    "    return frames\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVQAAADnCAYAAABBu67aAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAEDklEQVR4nO3cwU1bQRRA0f8jN5E6QhmpA9qgDagjZZA6UoazygKCYhA3/jPDOTuwkWaBrt73G9jP5/MGwMd9OfoAAKsQVICIoAJEBBUgIqgAkdOF110BAPjb/to3TagAEUEFiAgqQERQASKCChARVICIoAJEBBUgIqgAEUEFiAgqQERQASKCChARVICIoAJEBBUgIqgAEUEFiAgqQERQASKCChARVICIoAJEBBUgIqgAEUEFiAgqQERQASKCChARVICIoAJEBBUgIqgAEUEFiAgqQERQASKCChARVICIoAJEBBUgIqgAEUEFiAgqQERQASKCChARVICIoAJEBBUgIqgAEUEFiAgqQERQASKCChARVICIoAJEBBUgIqgAEUEFiAgqQERQASKCChARVICIoAJEBBUgIqgAEUEFiAgqQERQASKCChARVICIoAJEBBUgIqgAEUEFiAgqQERQASKCChARVICIoAJEBBUgIqgAEUEFiAgqQERQASKCChARVICIoAJEBBUgIqgAEUEFiAgqQERQASKCChARVIDI6egDwCU/H++eff3t9uGgk8C/mVAZ2suYwsgEFSAiqEzH1MqoBBUgIqgAEUFlaDb6zERQASKCChARVKZk08+IBBUgIqgMz2KKWQgqQERQmZbPURmNoAJEBBUgIqgAEUFlCjb9zEBQASKCyjRem1Jt+hmJoAJEBBUgIqhMz2M/oxBUgIigAkQElam4j8rIBBUgIqgswWKKEQgqQERQASKCynQsphiVoAJEBBUgIqgsw6afowkqQERQASKCypRs+hmRoAJEBJWlWExxJEEFiAgqQERQmZbFFKMRVICIoLIciymOIqgAEUEFiAgqU7OYYiSCChARVJZkMcURBJXpeexnFIIKEBFUluWxn2sTVICIoAJEBJUlWEwxAkEFiAgqQERQWZpNP9ckqAARQQWInI4+AFyy7/ub3/v0cPuhn9+2bTufz+96P/xhQgWImFBZzo9fz6fUp4dtu7l7POg0fCYmVJb3MrDwvwgqSxFPjiSoLOX+/uboI/CJCSrL+/7V56dcx37hioj7IxzuvdeePsq1Kd7g1V9KEypARFABIoIKEBFUgIigAkQEFSAiqAARQQWICCpAxL/vY3j+colZmFABIoIKEBFUgIigAkQEFSAiqAARQQWICCpARFABIoIKEBFUgIigAkQEFSAiqAARQQWICCpARFABIoIKEBFUgIigAkQEFSAiqAARQQWICCpARFABIoIKEBFUgIigAkQEFSAiqAARQQWICCpARFABIoIKEBFUgIigAkQEFSAiqAARQQWICCpARFABIoIKEBFUgIigAkQEFSByuvD6fpVTACzAhAoQEVSAiKACRAQVICKoABFBBYj8Bg2XRwuZYfwnAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "frames =render_policy_net(\"./my_policy_net_basic.ckpt\", action, X)\n",
    "#plot_animation(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "we need to define the target probabilities y\n",
    "If an action is good we must increaseits probability\n",
    "and if its bad then decrease its probability. But how do we know if\n",
    "an action is good or bad\n",
    "The problem is that most actions hae delayed effects so \n",
    "if we win and lose points in a game\n",
    "it is not clear whcic actions have delayed effects so when we los and win it si\n",
    "not clear that which actions cnotributed to us winning so theis is called credit assignment problem\n",
    "\n",
    "The policy gradients algorithm takckles this problem by first playing\n",
    "multiple games then making the action isngood games sloghltr more likely while actions in bad\n",
    "games are made slightly less likely\n",
    "fiirst we plat then we go back and thngk what we did right and bad\n",
    "\"\"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.reset_default_graph()\n",
    "tf.set_random_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "n_inputs = 4 # no of states of the environment\n",
    "n_hidden =4\n",
    "n_outputs = 1 #only one action selected\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "initializer = tf.variance_scaling_initializer()\n",
    "\n",
    "X = tf.placeholder(tf.float32,shape=[None, n_inputs])\n",
    "\n",
    "hidden = tf.layers.dense(X, n_hidden,activation=tf.nn.elu,\n",
    "                         kernel_initializer=initializer)\n",
    "logits = tf.layers.dense(hidden, n_outputs)\n",
    "outputs = tf.nn.sigmoid(logits)\n",
    "p_left_and_right = tf.concat(axis=1,values=[outputs, 1-outputs])\n",
    "action  = tf.multinomial(tf.log(p_left_and_right), num_samples=1)\n",
    "\n",
    "y = 1. - tf.to_float(action)\n",
    "# since we are saying the left is the desired action\n",
    "\n",
    "\n",
    "cross_entropy = tf.nn.sigmoid_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "\n",
    "# we are using compute gradients instead of minimizing loss\n",
    "# compute _gradients returns a list of gradient vecot/ variable pairs (one pair per traineable variable)\n",
    "grads_and_vars = optimizer.compute_gradients(cross_entropy)\n",
    "\n",
    "# putting all the gradients in a list\n",
    "gradients = [grad for grad, variable in grads_and_vars]\n",
    "\n",
    "# during the execution part the algoritm will run the policy and at each step evaluate theier values\n",
    "# after a number of episodes it will tweak the values\n",
    "# and then compute the mean of tweaked gradients as explained earlier\n",
    "# and wwe compute thte mean of tweaked gradients\n",
    "gradient_placeholders = []\n",
    "grads_and_vars_feed = []\n",
    "\n",
    "# we need one placeholder  per gradient vector\n",
    "for grad, variable in grads_and_vars:\n",
    "    gradient_placeholder = tf.placeholder(tf.float32, shape=grad.get_shape())\n",
    "    gradient_placeholders.append(gradient_placeholder)\n",
    "    grads_and_vars_feed.append((gradient_placeholder, variable))\n",
    "\n",
    "# apply _gradients to do the training op\n",
    "# instead of giving it the original vectors we will give it updated vectors\n",
    "training_op = optimizer.apply_gradients(grads_and_vars_feed)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to return the total discounted rewards \n",
    "# given raw rewards\n",
    "def discount_rewards(rewards, discount_rate):\n",
    "    discounted_rewards = np.zeros(len(rewards))\n",
    "    cumulative_rewards = 0\n",
    "    for step in reversed(range(len(rewards))):\n",
    "        cumulative_rewards = rewards[step] + cumulative_rewards * discount_rate\n",
    "        discounted_rewards[step] = cumulative_rewards\n",
    "    return discounted_rewards\n",
    "\n",
    "# discount and normalize rewards as the name says\n",
    "def discount_and_normalize_rewards(all_rewards, discount_rate):\n",
    "    all_discounted_rewards = [discount_rewards(rewards, discount_rate) for rewards in all_rewards]\n",
    "    flat_rewards = np.concatenate(all_discounted_rewards)\n",
    "    reward_mean = flat_rewards.mean()\n",
    "    reward_std = flat_rewards.std()\n",
    "    return [(discounted_rewards -reward_mean) / reward_std for discounted_rewards in all_discounted_rewards]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-22., -40., -50.])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "discount_rewards([10,0,-50],discount_rate=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([-0.65618612, -0.98126915, -1.16187083]),\n",
       " array([0.42742398, 0.82474769, 1.54715442])]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "discount_and_normalize_rewards([[10,0,-50], [-10,-20,100]], discount_rate=0.8)\n",
    "# first episode was badder than the second that is why\n",
    "# all discount rate is negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 249"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v0\")\n",
    "\n",
    "n_games_per_update = 10 #train the policy every 10 episodes\n",
    "n_max_steps = 1000 # max steps per episode\n",
    "n_iterations = 250 # no of training iteration\n",
    "\n",
    "# so total time the gaem is run = n_iterations * n_games_per_update * n_ max_steps\n",
    "# 1000 * 10 * 250\n",
    "\n",
    "save_iterations = 10 # save model ever 10 iterations\n",
    "discount_rate = 0.95\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for iteration in range(n_iterations):\n",
    "        print(\"\\rIteration: {}\".format(iteration), end=\"\")\n",
    "        all_rewards = [] # all sequence og rewards in eachiteration\n",
    "        all_gradients = [] # gradients saved at each step of each episode\n",
    "        \n",
    "        for game in range(n_games_per_update):\n",
    "        # each episode has 10 games    \n",
    "            current_rewards = [] #all rewards in current episode\n",
    "            current_gradients = [] # all gradients from the current episode\n",
    "            obs = env.reset()\n",
    "            for step in range(n_max_steps):\n",
    "                action_val, gradients_val = sess.run([action, gradients], feed_dict={X: obs.reshape(1, n_inputs)})\n",
    "                obs, reward, done, info = env.step(action_val[0][0])\n",
    "                current_rewards.append(reward)\n",
    "                current_gradients.append(gradients_val)\n",
    "                if done:\n",
    "                    break\n",
    "            all_rewards.append(current_rewards)\n",
    "            all_gradients.append(current_gradients)\n",
    "\n",
    "        all_rewards = discount_and_normalize_rewards(all_rewards, discount_rate=discount_rate)\n",
    "        feed_dict = {}\n",
    "        \"\"\"\n",
    "        we go through each trainable variable, across all episodes\n",
    "        and all steps, to multiply each gradient vector by its corresponding action score; and\n",
    "        we compute the mean of the resulting gradients\n",
    "        \"\"\"\n",
    "        for var_index, gradient_placeholder in enumerate(gradient_placeholders):\n",
    "            mean_gradients = np.mean([reward * all_gradients[game_index][step][var_index]\n",
    "                                      for game_index, rewards in enumerate(all_rewards)\n",
    "                                          for step, reward in enumerate(rewards)], axis=0)\n",
    "            feed_dict[gradient_placeholder] = mean_gradients\n",
    "        \n",
    "        # the training operation is run feeding it mean gradients\n",
    "        sess.run(training_op, feed_dict=feed_dict)\n",
    "        if iteration % save_iterations == 0:\n",
    "            saver.save(sess, \"./my_policy_net_pg.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVQAAADnCAYAAABBu67aAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAEBElEQVR4nO3c0W3aQACA4bhiiq7RrtE10jGijNGs0TXaNbqG+xJVKiEGwm9IfN8nIQEW6MTDr+Ps8zTP8x0Al/t06wEAbIWgAkQEFSAiqAARQQWI7I4cdwkAwEvToTfNUAEiggoQEVSAiKACRAQVICKoABFBBYgIKkBEUAEiggoQEVSAiKACRAQVICKoABFBBYgIKkBEUAEiggoQEVSAiKACRAQVICKoABFBBYgIKkBEUAEiggoQEVSAiKACRAQVICKoABFBBYgIKkBEUAEiggoQEVSAiKACRAQVICKoABFBBYgIKkBEUAEiggoQEVSAiKACRAQVICKoABFBBYgIKkBEUAEiggoQEVSAiKACRAQVICKoABFBBYgIKkBEUAEiggoQEVSAiKACRAQVICKoABFBBYgIKkBEUAEiggoQEVSAiKACRAQVICKoABFBBYgIKkBEUAEiggoQEVSAiKACRAQVICKoABFBBYgIKkBEUAEiggoQEVSAiKACRAQVICKoABFBBYgIKkBEUAEiggoQEVSAiKACRAQVILK79QCu7ffT9/9ef7n/caORAFsz1Ax1P6YApaGCCrAmQQWIDB9UywBAZfigAlQEFSAiqAARQQWIDBVUF/EDaxoqqABrElSAiKACRAQVICKod3ZLAQ1BBYgIKkBEUAEiggoQGS6odksBaxkuqABrEVSAiKACRAQVICKoz+yWAi4lqAARQQWICCpARFABIkMG1W4pYA1DBhVgDYIKEBFUgIigAkSGDeqhE1N2SwGXGDaoADVBBYgIKkBEUAEigrrHiSngrQQVICKoABFBBYgIKkBk6KC6jR9QGjqoACVBBYgIKkBEUAEignqA3VLAWwgqQERQASKCChARVIDI8EG1WwqoDB9UgIqgAkQEFSAiqAARQX2F3VLAuQQVILLpoE7TdNLj0s8vfQcwjk0HFeCadrcewHvy88/9v+ffPj/dcCTARzTN87x0fPHge3fOX/GHh18v3nt8/Hry54/8jsC2HIyLv/wAEUEFiAjqs/01U2uowLmsoUasocJQDsZl8Sy/6ytP57eCcbw2gVoM6kefdZmhAtdkDRUgIqgAEUEFiAgqQERQASKCChARVIDIpm/f59pQ4JrMUAEiggoQEVSAiKACRAQVICKoABFBBYgIKkBEUAEiggoQEVSAiKACRAQVICKoABFBBYgIKkBEUAEiggoQEVSAiKACRAQVICKoABFBBYgIKkBEUAEiggoQEVSAiKACRAQVICKoABFBBYgIKkBEUAEiggoQEVSAiKACRAQVICKoABFBBYgIKkBEUAEiggoQEVSAiKACRAQVILI7cny6yigANsAMFSAiqAARQQWICCpARFABIoIKEPkL46FArKZ+c5QAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "frames = render_policy_net(\"./my_policy_net_pg.ckpt\", action,X, n_max_steps=1000 )\n",
    "#plot_animation(frames)\n",
    "\n",
    "# there are two ways to loose the game either the\n",
    "# teh pole tilts\n",
    "# or the agent goes off the screen\n",
    "# now the agent is off the screen\n",
    "# thats why it looses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "States: 0 0 3 \n",
      "States: 0 1 2 1 2 1 2 1 2 1 3 \n",
      "States: 0 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 3 \n",
      "States: 0 3 \n",
      "States: 0 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 3 \n",
      "States: 0 1 3 \n",
      "States: 0 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 ...\n",
      "States: 0 0 3 \n",
      "States: 0 0 0 1 2 1 2 1 3 \n",
      "States: 0 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 3 \n"
     ]
    }
   ],
   "source": [
    "transition_probabilities = [\n",
    "    [0.7, 0.2, 0.0,0.1],\n",
    "    [0.0,0.0,0.9,0.1],\n",
    "    [0.0,1.0,0.0,0.0],\n",
    "    [0.0,0.0,0.0,1.0],\n",
    "]\n",
    "\n",
    "n_max_steps = 50\n",
    "\n",
    "def print_sequence(start_state=0):\n",
    "    current_state = start_state\n",
    "    print(\"States:\", end=\" \")\n",
    "    for step in range(n_max_steps):\n",
    "        print(current_state, end=\" \")\n",
    "        if current_state == 3:\n",
    "            break\n",
    "        current_state = np.random.choice(range(4), p=transition_probabilities[current_state])\n",
    "    else:\n",
    "        print(\"...\",end=\"\")\n",
    "    print()\n",
    "\n",
    "for _ in range(10):\n",
    "    print_sequence()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov decision process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](trial_by_fire.png)\n",
    "\n",
    "Markov decision for thes states given below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy_fire\n",
      "states (+rewards):  0 1 (-50) 2 2 (40) 0 1 (-50) 2 (40) 0 (10) 0 (10) 0 (10) ... Total rewards =  100\n",
      "states (+rewards):  0 1 (-50) 2 (40) 0 1 (-50) 2 (40) 0 (10) 0 1 (-50) 2 ... Total rewards =  -40\n",
      "states (+rewards):  0 (10) 0 (10) 0 (10) 0 1 (-50) 2 (40) 0 (10) 0 (10) 0 (10) 0 (10) ... Total rewards =  80\n",
      "states (+rewards):  0 (10) 0 1 (-50) 2 (40) 0 (10) 0 (10) 0 (10) 0 (10) 0 (10) 0 (10) ... Total rewards =  -100\n",
      "states (+rewards):  0 1 (-50) 2 2 2 1 (-50) 2 (40) 0 (10) 0 (10) 0 (10) ... Total rewards =  420\n",
      "Summary: mean=123.0,std=139.888955, min=-400, max=500\n",
      "\n",
      "policy_random\n",
      "states (+rewards):  0 1 1 1 (-50) 2 (40) 0 0 0 (10) 0 0 (10) ... Total rewards =  30\n",
      "states (+rewards):  0 0 0 (10) 0 (10) 0 0 (10) 0 0 (10) 0 0 ... Total rewards =  50\n",
      "states (+rewards):  0 1 (-50) 2 (40) 0 0 0 1 1 (-50) 2 (40) 0 ... Total rewards =  -130\n",
      "states (+rewards):  0 0 1 1 (-50) 2 (40) 0 0 (10) 0 (10) 0 0 ... Total rewards =  -20\n",
      "states (+rewards):  0 (10) 0 (10) 0 (10) 0 (10) 0 (10) 0 0 (10) 0 0 0 ... Total rewards =  30\n",
      "Summary: mean=-26.3,std=90.971492, min=-360, max=210\n",
      "\n",
      "policy_safe\n",
      "states (+rewards):  0 1 1 1 1 1 1 1 1 1 ... Total rewards =  0\n",
      "states (+rewards):  0 1 1 1 1 1 1 1 1 1 ... Total rewards =  0\n",
      "states (+rewards):  0 (10) 0 (10) 0 (10) 0 (10) 0 (10) 0 (10) 0 (10) 0 (10) 0 (10) 0 (10) ... Total rewards =  160\n",
      "states (+rewards):  0 (10) 0 (10) 0 (10) 0 (10) 0 1 1 1 1 1 ... Total rewards =  40\n",
      "states (+rewards):  0 1 1 1 1 1 1 1 1 1 ... Total rewards =  0\n",
      "Summary: mean=23.0,std=28.363341, min=0, max=210\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transition_probabilities = [\n",
    "    [[0.7,0.3,0.0], [1.0,0.0,0.0], [0.8,0.2,0.0]],\n",
    "    [[0.0,1.0,0.0], None, [0.0,0.0,1.0]],\n",
    "    [None,[0.8,0.1,0.1], None],\n",
    "] \n",
    "\n",
    "rewards = [\n",
    "    [[+10,0,0],[0,0,0],[0,0,0]],\n",
    "    [[0,0,0],[0,0,0],[0,0,-50]],\n",
    "    [[0,0,0],[+40,0,0],[0,0,0]],\n",
    "]\n",
    "\n",
    "possible_actions = [[0,1,2],[0,2],[1]]\n",
    "\n",
    "def policy_fire(state):\n",
    "    return [0,2,1][state]\n",
    "\n",
    "def policy_random(state):\n",
    "    return np.random.choice(possible_actions[state])\n",
    "\n",
    "def policy_safe(state):\n",
    "    return [0,0,1][state]\n",
    "\n",
    "class MDPEnvironment(object):\n",
    "    def __init__(self, start_state=0):\n",
    "        self.start_state = start_state\n",
    "        self.reset()\n",
    "    def reset(self):\n",
    "        self.total_rewards = 0\n",
    "        self.state = self.start_state\n",
    "    def step(self,action):\n",
    "        next_state = np.random.choice(range(3), p=transition_probabilities[self.state][action])\n",
    "        reward = rewards[self.state][action][next_state]\n",
    "        self.state = next_state\n",
    "        self.total_rewards += reward\n",
    "        return self.state, reward\n",
    "    \n",
    "def run_episode(policy, n_steps,start_state=0, display=True):\n",
    "    env = MDPEnvironment()\n",
    "    if display:\n",
    "        print(\"states (+rewards): \", end=\" \")\n",
    "    for step in range(n_steps):\n",
    "        if display:\n",
    "            if step == 10:\n",
    "                print(\"...\", end=\" \")\n",
    "            elif step < 10:\n",
    "                print(env.state, end=\" \")\n",
    "        action = policy(env.state)\n",
    "        state, reward = env.step(action)\n",
    "        if display and step <10:\n",
    "            if reward:\n",
    "                print(\"({})\".format(reward), end=\" \")\n",
    "    if display:\n",
    "        print(\"Total rewards = \", env.total_rewards)\n",
    "    return env.total_rewards\n",
    "\n",
    "for policy in (policy_fire, policy_random,policy_safe):\n",
    "    all_totals = []\n",
    "    print(policy.__name__)\n",
    "    for episode in range(1000):\n",
    "        all_totals.append(run_episode(policy, n_steps=100, display=(episode<5)))\n",
    "    print(\"Summary: mean={:.1f},std={:1f}, min={}, max={}\".format(np.mean(all_totals), np.std(all_totals), np.min(all_totals),np.max(all_totals) ))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q value iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# q learning works by watching an agent play, and gradually improving its \n",
    "# estimates of the Q values, Once it has accurate Q value then the optimal policy consists in choosing the action\n",
    "# that has highest Q vale greedy_policy\n",
    "\n",
    "n_states = 3\n",
    "n_actions = 3\n",
    "n_steps = 20000\n",
    "alpha = 0.01\n",
    "gamma = 0.99\n",
    "exploration_policy = policy_random\n",
    "q_values = np.full((n_states, n_actions),-np.inf)\n",
    "for state,actions in enumerate(possible_actions):\n",
    "    q_values[state][actions]=0\n",
    "\n",
    "env = MDPEnvironment()\n",
    "for step in range(n_steps):\n",
    "    action = exploration_policy(env.state)\n",
    "    state = env.state\n",
    "    next_state, reward = env.step(action)\n",
    "    next_value = np.max(q_values[next_state])\n",
    "    q_values[state, action] = (1-alpha) * q_values[state, action] + alpha *(reward + gamma * next_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimal_policy(state):\n",
    "    return np.argmax(q_values[state])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[39.04049867, 38.57875719, 34.75451786],\n",
       "       [19.15207712,        -inf, 21.1316577 ],\n",
       "       [       -inf, 73.73600019,        -inf]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "states (+rewards):  0 (10) 0 (10) 0 (10) 0 (10) 0 1 (-50) 2 2 (40) 0 (10) 0 (10) ... Total rewards =  170\n",
      "states (+rewards):  0 1 (-50) 2 (40) 0 (10) 0 (10) 0 (10) 0 (10) 0 (10) 0 1 (-50) ... Total rewards =  190\n",
      "states (+rewards):  0 1 (-50) 2 (40) 0 1 (-50) 2 2 (40) 0 1 (-50) 2 (40) ... Total rewards =  170\n",
      "states (+rewards):  0 (10) 0 1 (-50) 2 (40) 0 (10) 0 (10) 0 (10) 0 1 (-50) 2 (40) ... Total rewards =  40\n",
      "states (+rewards):  0 1 (-50) 2 (40) 0 (10) 0 1 (-50) 2 (40) 0 (10) 0 (10) 0 (10) ... Total rewards =  250\n",
      "summary: mean=120.3, std=135.001496, min=-430, max=520\n",
      "\n"
     ]
    }
   ],
   "source": [
    "all_totals = []\n",
    "for episode in range(1000):\n",
    "    all_totals.append(run_episode(optimal_policy, n_steps=100, display=(episode<5)))\n",
    "print(\"summary: mean={:.1f}, std={:1f}, min={}, max={}\".format(np.mean(all_totals), np.std(all_totals), np.min(all_totals), np.max(all_totals)))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying Q learning to the trail by fire example\n",
    "![](trail_by_fire.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we already have transition_probabilities \n",
    "# and rewards\n",
    "import numpy as np\n",
    "\n",
    "nan=np.nan # represents impossible actions\n",
    "T = np.array([ # shape=[s, a, s']\n",
    "    [[0.7, 0.3, 0.0], [1.0, 0.0, 0.0], [0.8, 0.2, 0.0]],\n",
    "    [[0.0, 1.0, 0.0], [nan, nan, nan], [0.0, 0.0, 1.0]],\n",
    "    [[nan, nan, nan], [0.8, 0.1, 0.1], [nan, nan, nan]],\n",
    "])\n",
    "\n",
    "R = np.array([ # shape=[s, a, s']\n",
    "[[10., 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0, 0.0]],\n",
    "[[10., 0.0, 0.0], [nan, nan, nan], [0.0, 0.0, -50.]],\n",
    "[[nan, nan, nan], [40., 0.0, 0.0], [nan, nan, nan]],\n",
    "])\n",
    "\n",
    "possible_actions =[[0,1,2],[0,2],[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Q value iteration algorithm\n",
    "Q = np.full((3,3),-np.inf)\n",
    "for state, actions in enumerate(possible_actions):\n",
    "    Q[state, actions] = 0.0 # initial value = 0.0. forall posssible actions\n",
    "\n",
    "learning_rate = 0.01 \n",
    "discount_rate = 0.95\n",
    "n_iterations = 100\n",
    "\n",
    "for iteration in range(n_iterations):\n",
    "    Q_prev = Q.copy()\n",
    "    for s in range(3):\n",
    "        for a in possible_actions[s]:\n",
    "                Q[s,a] = np.sum([\n",
    "                    T[s,a,sp] * (R[s,a,sp] + discount_rate * np.max(Q_prev[sp]))\n",
    "                    for sp in range(3)\n",
    "                ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  7.52688172,   0.75268817,   0.60215054],\n",
       "       [  0.        ,         -inf, -46.70685348],\n",
       "       [        -inf,  32.93146519,         -inf]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Q\n",
    "\n",
    "# dsicount rate affects the policy being chosen\n",
    "# discount .1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[21.88646117, 20.79149867, 16.854807  ],\n",
       "       [ 1.10804034,        -inf,  1.16703135],\n",
       "       [       -inf, 53.8607061 ,        -inf]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q\n",
    "\n",
    "# discount rate 0.95"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q learning simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy.random as rnd\n",
    "\n",
    "learning_rate0 = 0.05\n",
    "learning_rate_decay = 0.1\n",
    "n_iterations = 20000\n",
    "\n",
    "s = 0\n",
    "\n",
    "\n",
    "learning_rate = 0.01 \n",
    "discount_rate = 0.95\n",
    "n_iterations = 100\n",
    "\n",
    "Q = np.full((3,3), -np.inf)\n",
    "\n",
    "for state, actions in enumerate(possible_actions):\n",
    "    Q[state, actions] = 0.0 \n",
    "\n",
    "for iteration in range(n_iterations):\n",
    "    a = rnd.choice(possible_actions[s])\n",
    "    sp = rnd.choice(range(3), p=T[s,a]) # pick next state using T[s,a]\n",
    "    reward = R[s,a,sp]\n",
    "    learning_rate = learning_rate0/ (1+ iteration * learning_rate_decay)\n",
    "    Q[s,a] = learning_rate * Q[s,a] + (1 + learning_rate) * (\n",
    "        reward + discount_rate * np.max(Q[sp])\n",
    "    )\n",
    "    s=sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 99.01853501,  88.97008276,  64.95550041],\n",
       "       [ 35.83794861,         -inf,  67.63445532],\n",
       "       [        -inf, 123.32379116,         -inf]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q\n",
    "# given enough operations this algorithm will converge to optimal Q value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN for MS. PacMan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import tensorflow as tf\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(210, 160, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating he pacman environemnt\n",
    "env = gym.make(\"MsPacman-v0\")\n",
    "obs = env.reset()\n",
    "obs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(9)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#It greatly sppeds up trainign\n",
    "mspacman_color = 210 + 164 + 74\n",
    "\n",
    "def preprocess_observation(obs):\n",
    "    img = obs[1:176:2, ::2] # crop and downsize\n",
    "    img = img.sum(axis=2) # to greyscale\n",
    "    img[img == mspacman_color] = 0 # improve contrast\n",
    "    img = (img // 3 - 128).astype(np.int8) # normalise from -128 to 128\n",
    "    return img.reshape(88, 80, 1)\n",
    "\n",
    "img = preprocess_observation(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 792x504 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "\n",
    "plt.figure(figsize=(11,7))\n",
    "plt.subplot(131)\n",
    "plt.title(\"Original Observation(160 X 210 RGB)\")\n",
    "plt.imshow(obs)\n",
    "plt.axis(\"off\")\n",
    "plt.subplot(132)\n",
    "plt.title(\"Preprocessed observation (88x80 greyscale)\")\n",
    "plt.imshow(preprocess_observation(obs).reshape(88,80))\n",
    "plt.subplot(133)\n",
    "plt.imshow(img.reshape(88,80), interpolation=\"nearest\",cmap=\"gray\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Build the DQN\n",
    "# the training algorithm uses two DQNs with the same architecture but different\n",
    "#parameters one to drive Ms Pac man during the training the online\n",
    "# and other will watch the actor from its trials , the target\n",
    "\n",
    "tf.reset_default_graph()\n",
    "tf.set_random_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "input_height = 88\n",
    "input_width = 80\n",
    "input_channels = 1\n",
    "conv_n_maps = [32,64,64]\n",
    "conv_kernel_size = [(8,8), (4,4), (3,3)]\n",
    "conv_strides = [4,2,1]\n",
    "conv_paddings = [\"SAME\"] * 3\n",
    "conv_activation = [tf.nn.relu] * 3\n",
    "#multiply by three because a zip has been used to encapsulate the values\n",
    "\n",
    "n_hidden_in  = 64 * 11* 10 # conv3 has 64 maps of 11x10 each\n",
    "n_hidden = 512\n",
    "hidden_activation = tf.nn.relu\n",
    "n_outputs = env.action_space.n # 9 discrete actions are available\n",
    "initializer = tf.variance_scaling_initializer()\n",
    "# this defines the hyperparameters\n",
    "\n",
    "# The DQn takes the X_state as input and name of the variable scope\n",
    "def q_network(X_state, name):\n",
    "    prev_layer = X_state / 128.0 # scale pixel intensities to the [-1.0,1.0]\n",
    "    with tf.variable_scope(name) as scope:\n",
    "        for n_maps, kernel_size, strides, padding,activation in zip(\n",
    "                conv_n_maps,conv_kernel_size, conv_strides,\n",
    "                conv_paddings, conv_activation):\n",
    "            prev_layer = tf.layers.conv2d(\n",
    "                prev_layer, filters=n_maps, kernel_size=kernel_size,\n",
    "                strides = strides, padding=padding, activation=activation,\n",
    "                kernel_initializer=initializer\n",
    "            )\n",
    "            # why are we not appending the prev layers\n",
    "            # prev layer only stands for the alst layer here\n",
    "            \n",
    "        last_conv_layer_flat = tf.reshape(prev_layer, shape=[-1,n_hidden_in])\n",
    "        hidden = tf.layers.dense(last_conv_layer_flat, n_hidden,activation=hidden_activation, kernel_initializer=initializer)\n",
    "        outputs = tf.layers.dense(hidden, n_outputs, kernel_initializer=initializer)\n",
    "    \n",
    "    trainable_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,\n",
    "                                      scope=scope.name)\n",
    "    trainable_vars_by_name = {var.name[len(scope.name):]: var \n",
    "                             for var in trainable_vars}\n",
    "    return outputs, trainable_vars_by_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0629 06:26:09.426785  8912 deprecation.py:323] From <ipython-input-6-ef1cea899b99>:37: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.keras.layers.Conv2D` instead.\n",
      "W0629 06:26:09.429777  8912 deprecation.py:323] From C:\\installs\\Anaconda\\envs\\tf_gpu\\lib\\site-packages\\tensorflow_core\\python\\layers\\convolutional.py:424: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n",
      "W0629 06:26:09.545496  8912 deprecation.py:323] From <ipython-input-6-ef1cea899b99>:43: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.Dense instead.\n"
     ]
    }
   ],
   "source": [
    "X_state = tf.placeholder(tf.float32, shape=[None, input_height, input_width, input_channels])\n",
    "online_q_values, online_vars = q_network(X_state, name=\"q_networks/online\")\n",
    "target_q_values, target_vars = q_network(X_state, name=\"q_networks/target\")\n",
    "# defining two DQNs\n",
    "\n",
    "copy_ops = [target_var.assign(online_vars[var_name])\n",
    "            for var_name, target_var in target_vars.items()]\n",
    "\n",
    "copy_online_to_target = tf.group(*copy_ops)\n",
    "# this operation is supposed to copy yhr online network to target network but why"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'/conv2d/kernel:0': <tf.Variable 'q_networks/online/conv2d/kernel:0' shape=(8, 8, 1, 32) dtype=float32_ref>,\n",
       " '/conv2d/bias:0': <tf.Variable 'q_networks/online/conv2d/bias:0' shape=(32,) dtype=float32_ref>,\n",
       " '/conv2d_1/kernel:0': <tf.Variable 'q_networks/online/conv2d_1/kernel:0' shape=(4, 4, 32, 64) dtype=float32_ref>,\n",
       " '/conv2d_1/bias:0': <tf.Variable 'q_networks/online/conv2d_1/bias:0' shape=(64,) dtype=float32_ref>,\n",
       " '/conv2d_2/kernel:0': <tf.Variable 'q_networks/online/conv2d_2/kernel:0' shape=(3, 3, 64, 64) dtype=float32_ref>,\n",
       " '/conv2d_2/bias:0': <tf.Variable 'q_networks/online/conv2d_2/bias:0' shape=(64,) dtype=float32_ref>,\n",
       " '/dense/kernel:0': <tf.Variable 'q_networks/online/dense/kernel:0' shape=(7040, 512) dtype=float32_ref>,\n",
       " '/dense/bias:0': <tf.Variable 'q_networks/online/dense/bias:0' shape=(512,) dtype=float32_ref>,\n",
       " '/dense_1/kernel:0': <tf.Variable 'q_networks/online/dense_1/kernel:0' shape=(512, 9) dtype=float32_ref>,\n",
       " '/dense_1/bias:0': <tf.Variable 'q_networks/online/dense_1/bias:0' shape=(9,) dtype=float32_ref>}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "online_vars\n",
    "# trainable var by name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0629 06:26:22.306785  8912 deprecation.py:323] From C:\\installs\\Anaconda\\envs\\tf_gpu\\lib\\site-packages\\tensorflow_core\\python\\ops\\math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.001\n",
    "momentum =0.95\n",
    "\n",
    "with tf.variable_scope(\"train\"):\n",
    "    X_action = tf.placeholder(tf.int32, shape=[None])\n",
    "    y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "    \n",
    "    # this will zero out all Q values except forth eone corresponding to the \n",
    "    # memorized action.Then we just sum overthe first axis to obtain the desired \n",
    "    # Q value prediction for each memory\n",
    "    q_value = tf.reduce_sum(online_q_values * tf.one_hot(X_action, n_outputs),\n",
    "                            axis=1, keepdims=True)\n",
    "    error = tf.abs(y-q_value)\n",
    "    clipped_error = tf.clip_by_value(error, 0.0,1.0)\n",
    "    linear_error = 2 * (error -clipped_error)\n",
    "    # more like a quadratic error function\n",
    "    loss= tf.reduce_mean(tf.square(clipped_error) + linear_error)\n",
    "    \"\"\"\n",
    "    Note: in the first version of the book, the loss function was simply the squared error \n",
    "    between the target Q-Values (y) and the estimated Q-Values (q_value). \n",
    "    However, because the experiences are very noisy, \n",
    "    it is better to use a quadratic loss only for small errors (below 1.0) \n",
    "    and a linear loss (twice the absolute error) for larger errors,\n",
    "    which is what the code above computes. \n",
    "    This way large errors don't push the model parameters around as much. \n",
    "    \"\"\"\n",
    "    \n",
    "    global_step = tf.Variable(0, trainable=False, name='global_step')\n",
    "    optimizer = tf.train.MomentumOptimizer (learning_rate, momentum, use_nesterov=True)\n",
    "    training_op = optimizer.minimize(loss, global_step=global_step)\n",
    "    # gloabl step initialised to 0\n",
    "    \n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use a replay memory than a deque because it is much faster for random acces\n",
    "# we default to sampling with replacement\n",
    "\n",
    "class ReplayMemory:\n",
    "    def __init__(self, maxlen):\n",
    "        self.maxlen = maxlen\n",
    "        self.buf = np.empty(shape=maxlen, dtype=np.object)\n",
    "        self.index = 0 \n",
    "        self.length = 0\n",
    "        \n",
    "    def append(self, data):\n",
    "        self.buf[self.index] = data\n",
    "        self.length = min(self.length + 1, self.maxlen)\n",
    "        self.index = (self.index + 1) % self.maxlen\n",
    "    \n",
    "    def sample(self, batch_size, with_replacement=True):\n",
    "        if with_replacement:\n",
    "            indices = np.random.randint(self.length, size=batch_size)\n",
    "        else:\n",
    "            indices = np.random.permutation(self.length)[:batch_size]\n",
    "        return self.buf[indices]\n",
    "\n",
    "# this stores data (defined in sample memory) in buffer to be sampled and returned when required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_memory_size =500000\n",
    "replay_memory = ReplayMemory(replay_memory_size)\n",
    "# initalising replay _memory object\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_memories(batch_size):\n",
    "    cols = [[],[],[],[],[]]\n",
    "    # state, action, reward, next_state, contine\n",
    "    for memory in replay_memory.sample(batch_size):\n",
    "        for col, value in zip(cols, memory):\n",
    "            col.append(value)\n",
    "    \n",
    "    cols = [np.array(col) for col in cols]\n",
    "    return cols[0], cols[1], cols[2].reshape(-1,1), cols[3], cols[4].reshape(-1,1)\n",
    "\n",
    "# wraooer function for replay memory and we also realise that\n",
    "# teh data is state, action, reward, next_stae and continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps_min = 0.1\n",
    "eps_max = 1.0\n",
    "eps_decay_steps = 2000000\n",
    "\n",
    "#will gradually decrease from 1.0 to 0.1 in 20000000 steps\n",
    "def epsilon_greedy(q_values, step):\n",
    "    epsilon = max(eps_min, eps_max - (eps_max - eps_min) * step/eps_decay_steps)\n",
    "    if np.random.rand() < epsilon:\n",
    "        return np.random.randint(n_outputs)\n",
    "    else:\n",
    "        return np.argmax(q_values)\n",
    "    \n",
    "    # the function that chooses the greedy action it \n",
    "    # choose the best value argmax(q_value) if the cahnce desires it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 4000000 # total training steps\n",
    "training_start = 10000 # start training after 10,000 iterations\n",
    "training_interval = 4 # run a training step every 4 iterations\n",
    "save_steps = 1000 # save model every 1000 steps\n",
    "copy_steps = 10000 # copy online DQN to target DQN every 10,000 training steps\n",
    "discount_rate =0.99 \n",
    "skip_start = 90 # skip the start of every game when it is just waiting\n",
    "batch_size = 50\n",
    "iteration = 0 # GAMEiterations\n",
    "checkpoint_path = \"./my_dqn.ckpt\"\n",
    "done = True # env needs to reset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variables to track the progress\n",
    "\n",
    "loss_val = np.infty\n",
    "game_length = 0\n",
    "total_max_q = 0\n",
    "mean_max_q = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 658089\tTraining step 242024/4000000 (6.1)%\tLoss 2.048075\tMean Max-Q 11.415875  "
     ]
    }
   ],
   "source": [
    "# The main traiing loop\n",
    "import os\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    if os.path.isfile(checkpoint_path + \".index\"):\n",
    "        saver.restore(sess, checkpoint_path)\n",
    "    else:\n",
    "        init.run()\n",
    "        copy_online_to_target.run()\n",
    "    while True:\n",
    "        step = global_step.eval()\n",
    "        if step >= n_steps:\n",
    "            break\n",
    "        iteration += 1\n",
    "        print(\"\\rIteration {}\\tTraining step {}/{} ({:.1f})%\\tLoss {:5f}\\tMean Max-Q {:5f} \".format(\n",
    "            iteration, step, n_steps, step * 100 / n_steps, loss_val, mean_max_q\n",
    "        ), end=\"\")\n",
    "        \n",
    "        if done: # it means game over, start again\n",
    "            obs = env.reset()\n",
    "            for skip in range(skip_start):\n",
    "                # skip to the start of each game\n",
    "                obs, reward, done, info = env.step(0)\n",
    "            state = preprocess_observation(obs)\n",
    "        \n",
    "        # online DQn evaluates what to do\n",
    "        q_values = online_q_values.eval(feed_dict={X_state: [state]})\n",
    "        action = epsilon_greedy(q_values, step)\n",
    "        # action can be argmax(Q) or some random value\n",
    "        \n",
    "        # online DQn evaluates what to do\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        next_state = preprocess_observation(obs)\n",
    "        \n",
    "        # lets memorize what happened\n",
    "        replay_memory.append((state, action, reward, next_state, 1.0- done))\n",
    "        state = next_state\n",
    "        \n",
    "        # compute statistics for tracking progress\n",
    "        total_max_q += q_values.max()\n",
    "        game_length += 1\n",
    "        if done:\n",
    "            mean_max_q = total_max_q /game_length\n",
    "            total_max_q = 0.0\n",
    "            game_length = 0\n",
    "        \n",
    "        if iteration < training_start or iteration % training_interval != 0:\n",
    "            continue # only train after warmup period and atregular intervals\n",
    "        \n",
    "        # sample memories and use the target DQN to produce the target Q-value\n",
    "        X_state_val, X_action_val, rewards, X_next_state_val,continues = (\n",
    "            sample_memories(batch_size))\n",
    "        next_q_values = target_q_values.eval(\n",
    "            feed_dict ={X_state:X_next_state_val}\n",
    "        )\n",
    "        max_next_q_values = np.max(next_q_values, axis=1, keepdims=True)\n",
    "        y_val = rewards + continues * discount_rate * max_next_q_values\n",
    "        # The only tricky part here is that we must multiply the next state’s Q-Values by the\n",
    "        # continues vector to zero out the Q-Values corresponding to memories where the\n",
    "        # game was over.\n",
    "        \n",
    "        # train teh online SQn\n",
    "        _, loss_val = sess.run([training_op, loss], feed_dict={\n",
    "            X_state: X_state_val, X_action:X_action_val, y:y_val\n",
    "        })\n",
    "        \n",
    "        # regularly copy the DQN to target DQn\n",
    "        if step %copy_steps == 0:\n",
    "            copy_online_to_target.run()\n",
    "            \n",
    "        # save regularly\n",
    "        if step% save_steps == 0:\n",
    "            saver.save(sess, checkpoint_path)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing the agent\n",
    "frames = []\n",
    "n_max_steps = 10000\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, checkpoint_path)\n",
    "    \n",
    "    obs = env.reset()\n",
    "    for step in range(n_max_steps):\n",
    "        state = preprocess_observation(obs)\n",
    "        \n",
    "        # online DWn evluates what to do\n",
    "        q_values = online_q_values.eval(feed_dict={X_state:[state]})\n",
    "        action = np.argmax(q_values)\n",
    "        \n",
    "        #online dqn plays\n",
    "        obs, reward, done,info = env.step(action)\n",
    "        \n",
    "        img = env.render(mode=\"rgb_array\")\n",
    "        frames.append(img)\n",
    "        \n",
    "        if done:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# at 9865\n",
    "# plot_animation(frames)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf_gpu)",
   "language": "python",
   "name": "tf_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
