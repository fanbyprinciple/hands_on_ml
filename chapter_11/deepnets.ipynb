{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "def reset_graph():\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(42)\n",
    "    np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_batch(X, y, batch_size):\n",
    "    rnd_idx = np.random.permutation(len(X))\n",
    "    n_batches = len(X) // batch_size\n",
    "    for batch_idx in np.array_split(rnd_idx, n_batches):\n",
    "        X_batch, y_batch = X[batch_idx], y[batch_idx]\n",
    "        yield X_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the dataset tf.examples.tutorials.mnist is depreciated\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "X_train = X_train.astype(np.float32).reshape(-1,28 *28)/ 255.0\n",
    "X_test = X_test.astype(np.float32).reshape(-1, 28*28)/ 255.0\n",
    "\n",
    "y_train = y_train.astype(np.int32)\n",
    "y_test = y_test.astype(np.int32)\n",
    "\n",
    "X_valid, X_train = X_train[:5000], X_train[5000:]\n",
    "y_valid, y_train = y_train[:5000], y_train[5000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanishing/exploding gradients problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "def logit(z):\n",
    "    return 1/ (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "z = np.linspace(-5,5,200)\n",
    "\n",
    "plt.plot([-5,-5], [0,0], 'k-')\n",
    "plt.plot([-5,5], [1,1], 'k--')\n",
    "plt.plot([0,0], [-0.2,1.2], 'k-')\n",
    "plt.plot([-5,5], [-3/4, 7/4], 'g--')\n",
    "plt.plot(z, logit(z), 'b-', linewidth=2)\n",
    "props= dict(facecolor='black',shrink=0.1)\n",
    "\n",
    "plt.annotate('Saturating', xytext=(3.5,0.7), xy=(5,1), arrowprops=props, fontsize=14, ha=\"center\")\n",
    "plt.annotate('Saturating', xytext=(-3.5,0.3), xy=(-5,0), arrowprops=props, fontsize=14, ha=\"center\")\n",
    "plt.annotate('Linear', xytext=(2, 0.2), xy=(0, 0.5),arrowprops=props, fontsize=14, ha=\"center\")\n",
    "plt.grid(True)\n",
    "plt.title(\"Sigmoid activation function\", fontsize=14)\n",
    "plt.axis([-5, 5, -0.2, 1.2])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Xavier and He Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The book uses fully_connected isntaead of dense but it is preferable to use dense\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "#reset_graph()\n",
    "tf.reset_default_graph()\n",
    "tf.set_random_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "n_inputs = 28 *28\n",
    "n_hidden1 = 300\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0530 10:39:02.405937 13420 deprecation.py:323] From <ipython-input-7-da109dac52d3>:3: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.Dense instead.\n",
      "W0530 10:39:02.410925 13420 deprecation.py:323] From C:\\installs\\Anaconda\\envs\\tf_gpu\\lib\\site-packages\\tensorflow_core\\python\\layers\\core.py:187: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n"
     ]
    }
   ],
   "source": [
    "he_init = tf.variance_scaling_initializer()\n",
    "hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu,\n",
    "                          kernel_initializer=he_init, name=\"hidden1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non Saturating Activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leaky_relu(z, alpha=0.01):\n",
    "    return np.maximum(alpha*z, z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plotting leaky relu\n",
    "\n",
    "plt.plot(z, leaky_relu(z, 0.05), \"b-\",linewidth=2)\n",
    "plt.plot([-5,5],[0,0], 'k-')\n",
    "plt.plot([0,0], [-0.5, 4.2],'k-')\n",
    "plt.grid(True)\n",
    "props = dict(facecolor='black', shrink=0.1)\n",
    "\n",
    "plt.annotate('Leak', xytext=(-3.5, 0.5), xy=(-0.5, -0.2), arrowprops=props, fontsize=14, ha=\"center\")\n",
    "plt.title(\"Leaky relu activation function\", fontsize=14)\n",
    "plt.axis([-5,5,-0.5,4.2])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing leaky relu in tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "tf.set_random_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs),name=\"X\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leaky_relu(z, name=None):\n",
    "    return tf.maximum(0.01 * z, z, name=name)\n",
    "\n",
    "hidden1 = tf.layers.dense(X, n_hidden1, activation=leaky_relu, name=\"hidden1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "tf.set_random_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "n_inputs = 28 * 28\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32,shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=leaky_relu, name=\"hidden1\")\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=leaky_relu, name=\"hidden2\")\n",
    "    logits = tf.layers.dense(hidden2, n_outputs, name=\"outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0530 10:39:21.877292 13420 deprecation.py:323] From C:\\installs\\Anaconda\\envs\\tf_gpu\\lib\\site-packages\\tensorflow_core\\python\\ops\\math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.01\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the dataset tf.examples.tutorials.mnist is depreciated\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "X_train = X_train.astype(np.float32).reshape(-1,28 *28)/ 255.0\n",
    "X_test = X_test.astype(np.float32).reshape(-1, 28*28)/ 255.0\n",
    "\n",
    "y_train = y_train.astype(np.int32)\n",
    "y_test = y_test.astype(np.int32)\n",
    "\n",
    "X_valid, X_train = X_train[:5000], X_train[5000:]\n",
    "y_valid, y_train = y_train[:5000], y_train[5000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_batch(X, y, batch_size):\n",
    "    rnd_idx = np.random.permutation(len(X))\n",
    "    n_batches = len(X) // batch_size\n",
    "    for batch_idx in np.array_split(rnd_idx, n_batches):\n",
    "        X_batch, y_batch = X[batch_idx], y[batch_idx]\n",
    "        yield X_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.86 0.9048\n",
      "5 0.94 0.9496\n",
      "10 0.92 0.9656\n",
      "15 0.94 0.9714\n",
      "20 1.0 0.9762\n",
      "25 1.0 0.9774\n",
      "30 0.98 0.978\n",
      "35 1.0 0.9786\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 40\n",
    "batch_size =50\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X:X_batch, y: y_batch})\n",
    "        if epoch % 5 == 0:\n",
    "            acc_batch = accuracy.eval(feed_dict={X:X_batch, y:y_batch})\n",
    "            acc_valid = accuracy.eval(feed_dict={X:X_valid, y:y_valid})\n",
    "            print(epoch, acc_batch, acc_valid)\n",
    "    \n",
    "    save_path = saver.save(sess, \"./my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ELU function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elu(z, alpha=1):\n",
    "    return np.where(z < 0, alpha * (np.exp(z) - 1), z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(z, elu(z),\"b-\", linewidth=2)\n",
    "plt.plot([-5,5], [0,0], 'k-')\n",
    "plt.plot([-5,5], [-1,1], 'k--')\n",
    "plt.plot([0,0], [-2.2,3.2], 'k-')\n",
    "plt.grid(True)\n",
    "plt.title(r\"ELU activation function ($\\alpha=1$)\", fontsize=14)\n",
    "plt.axis([-5,5,-2.2,3.2])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "def reset_graph():\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(42)\n",
    "    np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementing ELU in tensorflow\n",
    "reset_graph()\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden1 = tf.layers.dense(X, n_hidden1,activation=tf.nn.elu, name=\"hidden1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SELU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function tends to preserve the same mean and variance \n",
    "# during training of every layer\n",
    "# During training, a neural network composed exclusively of a stack of dense layers using the SELU activation function \n",
    "# and LeCun initialization will self-normalize\n",
    "# In practice it works very well with CNN\n",
    "\n",
    "# unfortunately you canot yse l1 or l2 regularisation\n",
    "\n",
    "from scipy.special import erfc\n",
    "\n",
    "alpha_0_1 = -np.sqrt(2 / np.pi) / (erfc(1/np.sqrt(2)) * np.exp(1/2) - 1)\n",
    "scale_0_1 = (1 - erfc(1 / np.sqrt(2)) * np.sqrt(np.e)) * np.sqrt(2 * np.pi) * (2 * erfc(np.sqrt(2))*np.e**2 + np.pi*erfc(1/np.sqrt(2))**2*np.e - 2*(2+np.pi)*erfc(1/np.sqrt(2))*np.sqrt(np.e)+np.pi+2)**(-1/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def selu(z, scale=scale_0_1, alpha=alpha_0_1):\n",
    "    return scale * elu(z, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(z, selu(z), \"b-\", linewidth=2)\n",
    "plt.plot([-5, 5], [0, 0], 'k-')\n",
    "plt.plot([-5, 5], [-1.758, -1.758], 'k--')\n",
    "plt.plot([0, 0], [-2.2, 3.2], 'k-')\n",
    "plt.grid(True)\n",
    "plt.title(r\"SELU activation function\", fontsize=14)\n",
    "plt.axis([-5, 5, -2.2, 3.2])\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# the selu function hyperparameters are tuned in such a way that mean output of each\n",
    "# neiron remains close to 0 \n",
    "# and the standrd deviation remains close to 1\n",
    "# even if therewere about 1000 layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer:  0 mean:  -0.0018323191229082014 deviation:  0.9993326848140984\n",
      "layer:  100 mean:  0.022381401728658136 deviation:  0.9624307594850072\n",
      "layer:  200 mean:  0.009595136169400975 deviation:  0.9044881262732755\n",
      "layer:  300 mean:  -0.01519457019486652 deviation:  0.9236924543338241\n",
      "layer:  400 mean:  0.0541343380033697 deviation:  0.8916028369516269\n",
      "layer:  500 mean:  0.005853113458524639 deviation:  0.9286462949083308\n",
      "layer:  600 mean:  0.022268641456820597 deviation:  0.9166416040576405\n",
      "layer:  700 mean:  -0.01825907594144632 deviation:  0.9033554369309249\n",
      "layer:  800 mean:  0.053814955568643616 deviation:  0.8250223490618902\n",
      "layer:  900 mean:  0.01989578336047298 deviation:  0.9952796987739755\n"
     ]
    }
   ],
   "source": [
    "# we can verify for 1000 layers\n",
    "\n",
    "np.random.seed(42)\n",
    "Z = np.random.normal(size=(500,100))\n",
    "for layer in range(1000):\n",
    "    W = np.random.normal(size=(100,100), scale=np.sqrt(1/100))\n",
    "    Z = selu(np.dot(Z, W))\n",
    "    means = np.mean(Z, axis=0).mean()\n",
    "    stds = np.std(Z, axis=0).mean()\n",
    "    if layer % 100 == 0:\n",
    "        print(\"layer: \", layer,\"mean: \", means,\"deviation: \", stds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selufunction for tensorflow<1.14\n",
    "\n",
    "def selu(z, scale=alpha_0_1, alpha=scale_0_1):\n",
    "    return scale * tf.where(z>=0.0, z,alpha*tf.nn.elu(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=selu, name=\"hidden1\")\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=selu, name=\"hidden2\")\n",
    "    logits = tf.layers.dense(hidden2, n_outputs, name=\"outputs\")\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "n_epochs = 40\n",
    "batch_size = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Batch accurac:  0.94 validation accuracy:  0.9388\n",
      "5 Batch accurac:  0.98 validation accuracy:  0.9634\n",
      "10 Batch accurac:  1.0 validation accuracy:  0.9694\n",
      "15 Batch accurac:  1.0 validation accuracy:  0.9704\n",
      "20 Batch accurac:  1.0 validation accuracy:  0.9692\n",
      "25 Batch accurac:  1.0 validation accuracy:  0.9692\n",
      "30 Batch accurac:  1.0 validation accuracy:  0.9708\n",
      "35 Batch accurac:  1.0 validation accuracy:  0.9706\n"
     ]
    }
   ],
   "source": [
    "means = X_train.mean(axis=0, keepdims=True)\n",
    "stds = X_train.std(axis=0, keepdims=True) + 1e-10\n",
    "X_val_scaled = (X_valid - means) / stds\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            X_batch_scaled = (X_batch - means) / stds\n",
    "            sess.run(training_op, feed_dict={X:X_batch_scaled, y:y_batch})\n",
    "        if epoch % 5 ==0:\n",
    "            acc_batch = accuracy.eval(feed_dict={X:X_batch_scaled, y:y_batch})\n",
    "            acc_valid = accuracy.eval(feed_dict={X:X_val_scaled, y:y_valid})\n",
    "            print(epoch, \"Batch accurac: \", acc_batch, \"validation accuracy: \", acc_valid)\n",
    "    \n",
    "    save_path = saver.save(sess, \"./my_model_final_selu.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Batch Normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0530 11:24:38.453265 13420 deprecation.py:323] From <ipython-input-40-dd5d141516b5>:38: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.BatchNormalization instead.  In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.batch_normalization` documentation).\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "reset_graph()\n",
    "\n",
    "from tensorflow.contrib.layers import batch_norm\n",
    "\n",
    "n_inputs = 28*28\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None,n_inputs), name=\"X\")\n",
    "\n",
    "# decay is renamed as momentum\n",
    "# is_training parameter would either be true or false\n",
    "# tells what means to be used in testing ans trainaing phase\n",
    "# in testing we need to use the entire dataset mean and standard deviation\n",
    "\n",
    "#is_training = tf.placeholder(tf.bool,shape=(), name=\"is training\")\n",
    "\n",
    "training = tf.placeholder_with_default(False, shape=(), name='training')\n",
    "\n",
    "# bn_params is a dictionary that defines the parameters that will be passed to batch norm()\n",
    "# algorithm uses exponential decay to compute the running averages\n",
    "# a good decay is close to one\n",
    "# Update collection to be set to None if we want the batch_norm()\n",
    "# function to update the running averages right before it perform\n",
    "#batch normalisation during training\n",
    "\n",
    "# bn_params = {\n",
    "#     \"is_training\" : is_training,\n",
    "#     \"decay\": 0.99,\n",
    "#     \"updates_collections\": None\n",
    "# }\n",
    "\n",
    "hidden1 = tf.layers.dense(X, n_hidden1, name=\"hidden1\")\n",
    "bn1 = tf.layers.batch_normalization(hidden1, training=training, momentum=0.90)  \n",
    "bn1_act = tf.nn.elu(bn1)\n",
    "\n",
    "hidden2 = tf.layers.dense(bn1_act, n_hidden2,name=\"hidden2\")\n",
    "bn2 = tf.layers.batch_normalization(hidden2, training=training, momentum=0.9)\n",
    "bn2_act = tf.nn.elu(bn2)\n",
    "\n",
    "logits_before_bn = tf.layers.dense(bn2_act, n_outputs, name=\"outputs\")\n",
    "logits = tf.layers.batch_normalization(logits_before_bn, training=training, momentum=0.9)\n",
    "\n",
    "# normalizer_fn = batch_norm, normalizer_params=bn_params)\n",
    "# hidden2 = dense(hidden1, n_hidden2 , scope=\"hidden2\",\n",
    "#                 normalizer_fn =batch_norm, normalizer_paramas=bn_params)\n",
    "# logits = dense(hidden2, n_outputs,activation_fn=None, scope=\"outputs\",\n",
    "#                normalizer_fn =batch_norm, normalizer_params=bn_params)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None,n_inputs), name=\"X\")\n",
    "training = tf.placeholder_with_default(False,shape=(), name=\"training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to avoid repeating the same function again and again we can call the partil function\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "my_batch_norm_layer = partial(tf.layers.batch_normalization, training=training, momentum=0.90)\n",
    "\n",
    "hidden1 = tf.layers.dense(X,n_hidden1, name=\"hidden1\")\n",
    "bn1 = my_batch_norm_layer(hidden1)\n",
    "bn1_act = tf.nn.elu(bn1)\n",
    "hidden2 = tf.layers.dense(bn1_act,n_hidden2,name=\"hidden2\")\n",
    "bn2 = my_batch_norm_layer(hidden2)\n",
    "bn2_act = tf.nn.elu(bn2)\n",
    "\n",
    "logits_before_bn = tf.layers.dense(bn2_act, n_outputs, name=\"outputs\")\n",
    "logits = my_batch_norm_layer(logits_before_bn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "batch_norm_momentum = 0.9\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "training = tf.placeholder_with_default(False, shape=(), name='training')\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    he_init = tf.variance_scaling_initializer()\n",
    "\n",
    "    my_batch_norm_layer = partial(\n",
    "            tf.layers.batch_normalization,\n",
    "            training=training,\n",
    "            momentum=batch_norm_momentum)\n",
    "\n",
    "    my_dense_layer = partial(\n",
    "            tf.layers.dense,\n",
    "            kernel_initializer=he_init)\n",
    "\n",
    "    hidden1 = my_dense_layer(X, n_hidden1, name=\"hidden1\")\n",
    "    bn1 = tf.nn.elu(my_batch_norm_layer(hidden1))\n",
    "    hidden2 = my_dense_layer(bn1, n_hidden2, name=\"hidden2\")\n",
    "    bn2 = tf.nn.elu(my_batch_norm_layer(hidden2))\n",
    "    logits_before_bn = my_dense_layer(bn2, n_outputs, name=\"outputs\")\n",
    "    logits = my_batch_norm_layer(logits_before_bn)\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    \n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 20\n",
    "batch_size = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Validation accuracy:  0.8952\n",
      "1 Validation accuracy:  0.9202\n",
      "2 Validation accuracy:  0.9318\n",
      "3 Validation accuracy:  0.9422\n",
      "4 Validation accuracy:  0.9468\n",
      "5 Validation accuracy:  0.954\n",
      "6 Validation accuracy:  0.9568\n",
      "7 Validation accuracy:  0.96\n",
      "8 Validation accuracy:  0.962\n",
      "9 Validation accuracy:  0.9638\n",
      "10 Validation accuracy:  0.9662\n",
      "11 Validation accuracy:  0.9682\n",
      "12 Validation accuracy:  0.9672\n",
      "13 Validation accuracy:  0.9696\n",
      "14 Validation accuracy:  0.9706\n",
      "15 Validation accuracy:  0.9704\n",
      "16 Validation accuracy:  0.9718\n",
      "17 Validation accuracy:  0.9726\n",
      "18 Validation accuracy:  0.9738\n",
      "19 Validation accuracy:  0.9742\n"
     ]
    }
   ],
   "source": [
    "extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run([training_op, extra_update_ops],\n",
    "                     feed_dict={training:True, X:X_batch, y:y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X:X_valid, y:y_valid})\n",
    "        print(epoch, \"Validation accuracy: \", accuracy_val)\n",
    "    save_path = saver.save(sess, \"./my_model_final.ckpt\")\n",
    "\n",
    "# its not great accuracy for mnist\n",
    "# butwe can get better result if we train it for longer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hidden1/kernel:0',\n",
       " 'hidden1/bias:0',\n",
       " 'batch_normalization/gamma:0',\n",
       " 'batch_normalization/beta:0',\n",
       " 'hidden2/kernel:0',\n",
       " 'hidden2/bias:0',\n",
       " 'batch_normalization_1/gamma:0',\n",
       " 'batch_normalization_1/beta:0',\n",
       " 'outputs/kernel:0',\n",
       " 'outputs/bias:0',\n",
       " 'batch_normalization_2/gamma:0',\n",
       " 'batch_normalization_2/beta:0']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can make the training operation depend on the update operations\n",
    "# with tf.name_scope(\"Train\"):\n",
    "#  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "#  extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "# with tf.control_dependencies(extra_update_ops):\n",
    "#  training_op = optimizer.minimize(loss)\n",
    "# the during sess.run when training_op is called the tensorflow will aitomatically run update as well\n",
    "\n",
    "#looking at the trainable variable\n",
    "\n",
    "[v.name for v in tf.trainable_variables()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hidden1/kernel:0',\n",
       " 'hidden1/bias:0',\n",
       " 'batch_normalization/gamma:0',\n",
       " 'batch_normalization/beta:0',\n",
       " 'batch_normalization/moving_mean:0',\n",
       " 'batch_normalization/moving_variance:0',\n",
       " 'hidden2/kernel:0',\n",
       " 'hidden2/bias:0',\n",
       " 'batch_normalization_1/gamma:0',\n",
       " 'batch_normalization_1/beta:0',\n",
       " 'batch_normalization_1/moving_mean:0',\n",
       " 'batch_normalization_1/moving_variance:0',\n",
       " 'outputs/kernel:0',\n",
       " 'outputs/bias:0',\n",
       " 'batch_normalization_2/gamma:0',\n",
       " 'batch_normalization_2/beta:0',\n",
       " 'batch_normalization_2/moving_mean:0',\n",
       " 'batch_normalization_2/moving_variance:0']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[v.name for v in tf.global_variables()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient clipping for MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "\n",
    "# we are using extra layers to demonstrate using pretrained models\n",
    "n_inputs = 28*28\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 50\n",
    "n_hidden3 = 50\n",
    "n_hidden4 = 50\n",
    "n_hidden5 = 50\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu,name=\"hidden1\")\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, name=\"hidden2\")\n",
    "    hidden3 = tf.layers.dense(hidden2, n_hidden3, activation=tf.nn.relu, name=\"hidden3\")\n",
    "    hidden4 = tf.layers.dense(hidden3, n_hidden4, activation=tf.nn.relu, name=\"hidden4\")\n",
    "    hidden5 = tf.layers.dense(hidden4, n_hidden5, activation=tf.nn.relu, name=\"hidden5\")\n",
    "    logits = tf.layers.dense(hidden5, n_outputs, name=\"outputs\")\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate =0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we apply clip by value()\n",
    "\n",
    "threshold = 1.0\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "grads_and_vars = optimizer.compute_gradients(loss)\n",
    "capped_gvs = [(tf.clip_by_value(grad, -threshold, threshold), var) for grad, var in grads_and_vars]\n",
    "training_op = optimizer.apply_gradients(capped_gvs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k (logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32),name=\"accuracy\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 20\n",
    "batch_Size =200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 validation accuracy:  0.288\n",
      "1 validation accuracy:  0.794\n",
      "2 validation accuracy:  0.8796\n",
      "3 validation accuracy:  0.9058\n",
      "4 validation accuracy:  0.9164\n",
      "5 validation accuracy:  0.9222\n",
      "6 validation accuracy:  0.929\n",
      "7 validation accuracy:  0.9356\n",
      "8 validation accuracy:  0.938\n",
      "9 validation accuracy:  0.9416\n",
      "10 validation accuracy:  0.946\n",
      "11 validation accuracy:  0.9472\n",
      "12 validation accuracy:  0.9474\n",
      "13 validation accuracy:  0.9534\n",
      "14 validation accuracy:  0.9566\n",
      "15 validation accuracy:  0.9566\n",
      "16 validation accuracy:  0.9578\n",
      "17 validation accuracy:  0.9586\n",
      "18 validation accuracy:  0.9622\n",
      "19 validation accuracy:  0.9616\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X:X_batch, y:y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X:X_valid, y:y_valid})\n",
    "        print(epoch, \"validation accuracy: \", accuracy_val)\n",
    "        \n",
    "    save_path = saver.save(sess, \"./my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reusing a TensorFlow model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# we will first load the meta file\n",
    "\n",
    "saver = tf.train.import_meta_graph(\"./my_model_final.ckpt.meta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X\n",
      "y\n",
      "hidden1/kernel/Initializer/random_uniform/shape\n",
      "hidden1/kernel/Initializer/random_uniform/min\n",
      "hidden1/kernel/Initializer/random_uniform/max\n",
      "hidden1/kernel/Initializer/random_uniform/RandomUniform\n",
      "hidden1/kernel/Initializer/random_uniform/sub\n",
      "hidden1/kernel/Initializer/random_uniform/mul\n",
      "hidden1/kernel/Initializer/random_uniform\n",
      "hidden1/kernel\n",
      "hidden1/kernel/Assign\n",
      "hidden1/kernel/read\n",
      "hidden1/bias/Initializer/zeros\n",
      "hidden1/bias\n",
      "hidden1/bias/Assign\n",
      "hidden1/bias/read\n",
      "dnn/hidden1/MatMul\n",
      "dnn/hidden1/BiasAdd\n",
      "dnn/hidden1/Relu\n",
      "hidden2/kernel/Initializer/random_uniform/shape\n",
      "hidden2/kernel/Initializer/random_uniform/min\n",
      "hidden2/kernel/Initializer/random_uniform/max\n",
      "hidden2/kernel/Initializer/random_uniform/RandomUniform\n",
      "hidden2/kernel/Initializer/random_uniform/sub\n",
      "hidden2/kernel/Initializer/random_uniform/mul\n",
      "hidden2/kernel/Initializer/random_uniform\n",
      "hidden2/kernel\n",
      "hidden2/kernel/Assign\n",
      "hidden2/kernel/read\n",
      "hidden2/bias/Initializer/zeros\n",
      "hidden2/bias\n",
      "hidden2/bias/Assign\n",
      "hidden2/bias/read\n",
      "dnn/hidden2/MatMul\n",
      "dnn/hidden2/BiasAdd\n",
      "dnn/hidden2/Relu\n",
      "hidden3/kernel/Initializer/random_uniform/shape\n",
      "hidden3/kernel/Initializer/random_uniform/min\n",
      "hidden3/kernel/Initializer/random_uniform/max\n",
      "hidden3/kernel/Initializer/random_uniform/RandomUniform\n",
      "hidden3/kernel/Initializer/random_uniform/sub\n",
      "hidden3/kernel/Initializer/random_uniform/mul\n",
      "hidden3/kernel/Initializer/random_uniform\n",
      "hidden3/kernel\n",
      "hidden3/kernel/Assign\n",
      "hidden3/kernel/read\n",
      "hidden3/bias/Initializer/zeros\n",
      "hidden3/bias\n",
      "hidden3/bias/Assign\n",
      "hidden3/bias/read\n",
      "dnn/hidden3/MatMul\n",
      "dnn/hidden3/BiasAdd\n",
      "dnn/hidden3/Relu\n",
      "hidden4/kernel/Initializer/random_uniform/shape\n",
      "hidden4/kernel/Initializer/random_uniform/min\n",
      "hidden4/kernel/Initializer/random_uniform/max\n",
      "hidden4/kernel/Initializer/random_uniform/RandomUniform\n",
      "hidden4/kernel/Initializer/random_uniform/sub\n",
      "hidden4/kernel/Initializer/random_uniform/mul\n",
      "hidden4/kernel/Initializer/random_uniform\n",
      "hidden4/kernel\n",
      "hidden4/kernel/Assign\n",
      "hidden4/kernel/read\n",
      "hidden4/bias/Initializer/zeros\n",
      "hidden4/bias\n",
      "hidden4/bias/Assign\n",
      "hidden4/bias/read\n",
      "dnn/hidden4/MatMul\n",
      "dnn/hidden4/BiasAdd\n",
      "dnn/hidden4/Relu\n",
      "hidden5/kernel/Initializer/random_uniform/shape\n",
      "hidden5/kernel/Initializer/random_uniform/min\n",
      "hidden5/kernel/Initializer/random_uniform/max\n",
      "hidden5/kernel/Initializer/random_uniform/RandomUniform\n",
      "hidden5/kernel/Initializer/random_uniform/sub\n",
      "hidden5/kernel/Initializer/random_uniform/mul\n",
      "hidden5/kernel/Initializer/random_uniform\n",
      "hidden5/kernel\n",
      "hidden5/kernel/Assign\n",
      "hidden5/kernel/read\n",
      "hidden5/bias/Initializer/zeros\n",
      "hidden5/bias\n",
      "hidden5/bias/Assign\n",
      "hidden5/bias/read\n",
      "dnn/hidden5/MatMul\n",
      "dnn/hidden5/BiasAdd\n",
      "dnn/hidden5/Relu\n",
      "outputs/kernel/Initializer/random_uniform/shape\n",
      "outputs/kernel/Initializer/random_uniform/min\n",
      "outputs/kernel/Initializer/random_uniform/max\n",
      "outputs/kernel/Initializer/random_uniform/RandomUniform\n",
      "outputs/kernel/Initializer/random_uniform/sub\n",
      "outputs/kernel/Initializer/random_uniform/mul\n",
      "outputs/kernel/Initializer/random_uniform\n",
      "outputs/kernel\n",
      "outputs/kernel/Assign\n",
      "outputs/kernel/read\n",
      "outputs/bias/Initializer/zeros\n",
      "outputs/bias\n",
      "outputs/bias/Assign\n",
      "outputs/bias/read\n",
      "dnn/outputs/MatMul\n",
      "dnn/outputs/BiasAdd\n",
      "loss/SparseSoftmaxCrossEntropyWithLogits/Shape\n",
      "loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits\n",
      "loss/Const\n",
      "loss/loss\n",
      "gradients/Shape\n",
      "gradients/grad_ys_0\n",
      "gradients/Fill\n",
      "gradients/loss/loss_grad/Reshape/shape\n",
      "gradients/loss/loss_grad/Reshape\n",
      "gradients/loss/loss_grad/Shape\n",
      "gradients/loss/loss_grad/Tile\n",
      "gradients/loss/loss_grad/Shape_1\n",
      "gradients/loss/loss_grad/Shape_2\n",
      "gradients/loss/loss_grad/Const\n",
      "gradients/loss/loss_grad/Prod\n",
      "gradients/loss/loss_grad/Const_1\n",
      "gradients/loss/loss_grad/Prod_1\n",
      "gradients/loss/loss_grad/Maximum/y\n",
      "gradients/loss/loss_grad/Maximum\n",
      "gradients/loss/loss_grad/floordiv\n",
      "gradients/loss/loss_grad/Cast\n",
      "gradients/loss/loss_grad/truediv\n",
      "gradients/zeros_like\n",
      "gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/PreventGradient\n",
      "gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/ExpandDims/dim\n",
      "gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/ExpandDims\n",
      "gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/mul\n",
      "gradients/dnn/outputs/BiasAdd_grad/BiasAddGrad\n",
      "gradients/dnn/outputs/BiasAdd_grad/tuple/group_deps\n",
      "gradients/dnn/outputs/BiasAdd_grad/tuple/control_dependency\n",
      "gradients/dnn/outputs/BiasAdd_grad/tuple/control_dependency_1\n",
      "gradients/dnn/outputs/MatMul_grad/MatMul\n",
      "gradients/dnn/outputs/MatMul_grad/MatMul_1\n",
      "gradients/dnn/outputs/MatMul_grad/tuple/group_deps\n",
      "gradients/dnn/outputs/MatMul_grad/tuple/control_dependency\n",
      "gradients/dnn/outputs/MatMul_grad/tuple/control_dependency_1\n",
      "gradients/dnn/hidden5/Relu_grad/ReluGrad\n",
      "gradients/dnn/hidden5/BiasAdd_grad/BiasAddGrad\n",
      "gradients/dnn/hidden5/BiasAdd_grad/tuple/group_deps\n",
      "gradients/dnn/hidden5/BiasAdd_grad/tuple/control_dependency\n",
      "gradients/dnn/hidden5/BiasAdd_grad/tuple/control_dependency_1\n",
      "gradients/dnn/hidden5/MatMul_grad/MatMul\n",
      "gradients/dnn/hidden5/MatMul_grad/MatMul_1\n",
      "gradients/dnn/hidden5/MatMul_grad/tuple/group_deps\n",
      "gradients/dnn/hidden5/MatMul_grad/tuple/control_dependency\n",
      "gradients/dnn/hidden5/MatMul_grad/tuple/control_dependency_1\n",
      "gradients/dnn/hidden4/Relu_grad/ReluGrad\n",
      "gradients/dnn/hidden4/BiasAdd_grad/BiasAddGrad\n",
      "gradients/dnn/hidden4/BiasAdd_grad/tuple/group_deps\n",
      "gradients/dnn/hidden4/BiasAdd_grad/tuple/control_dependency\n",
      "gradients/dnn/hidden4/BiasAdd_grad/tuple/control_dependency_1\n",
      "gradients/dnn/hidden4/MatMul_grad/MatMul\n",
      "gradients/dnn/hidden4/MatMul_grad/MatMul_1\n",
      "gradients/dnn/hidden4/MatMul_grad/tuple/group_deps\n",
      "gradients/dnn/hidden4/MatMul_grad/tuple/control_dependency\n",
      "gradients/dnn/hidden4/MatMul_grad/tuple/control_dependency_1\n",
      "gradients/dnn/hidden3/Relu_grad/ReluGrad\n",
      "gradients/dnn/hidden3/BiasAdd_grad/BiasAddGrad\n",
      "gradients/dnn/hidden3/BiasAdd_grad/tuple/group_deps\n",
      "gradients/dnn/hidden3/BiasAdd_grad/tuple/control_dependency\n",
      "gradients/dnn/hidden3/BiasAdd_grad/tuple/control_dependency_1\n",
      "gradients/dnn/hidden3/MatMul_grad/MatMul\n",
      "gradients/dnn/hidden3/MatMul_grad/MatMul_1\n",
      "gradients/dnn/hidden3/MatMul_grad/tuple/group_deps\n",
      "gradients/dnn/hidden3/MatMul_grad/tuple/control_dependency\n",
      "gradients/dnn/hidden3/MatMul_grad/tuple/control_dependency_1\n",
      "gradients/dnn/hidden2/Relu_grad/ReluGrad\n",
      "gradients/dnn/hidden2/BiasAdd_grad/BiasAddGrad\n",
      "gradients/dnn/hidden2/BiasAdd_grad/tuple/group_deps\n",
      "gradients/dnn/hidden2/BiasAdd_grad/tuple/control_dependency\n",
      "gradients/dnn/hidden2/BiasAdd_grad/tuple/control_dependency_1\n",
      "gradients/dnn/hidden2/MatMul_grad/MatMul\n",
      "gradients/dnn/hidden2/MatMul_grad/MatMul_1\n",
      "gradients/dnn/hidden2/MatMul_grad/tuple/group_deps\n",
      "gradients/dnn/hidden2/MatMul_grad/tuple/control_dependency\n",
      "gradients/dnn/hidden2/MatMul_grad/tuple/control_dependency_1\n",
      "gradients/dnn/hidden1/Relu_grad/ReluGrad\n",
      "gradients/dnn/hidden1/BiasAdd_grad/BiasAddGrad\n",
      "gradients/dnn/hidden1/BiasAdd_grad/tuple/group_deps\n",
      "gradients/dnn/hidden1/BiasAdd_grad/tuple/control_dependency\n",
      "gradients/dnn/hidden1/BiasAdd_grad/tuple/control_dependency_1\n",
      "gradients/dnn/hidden1/MatMul_grad/MatMul\n",
      "gradients/dnn/hidden1/MatMul_grad/MatMul_1\n",
      "gradients/dnn/hidden1/MatMul_grad/tuple/group_deps\n",
      "gradients/dnn/hidden1/MatMul_grad/tuple/control_dependency\n",
      "gradients/dnn/hidden1/MatMul_grad/tuple/control_dependency_1\n",
      "clip_by_value/Minimum/y\n",
      "clip_by_value/Minimum\n",
      "clip_by_value/y\n",
      "clip_by_value\n",
      "clip_by_value_1/Minimum/y\n",
      "clip_by_value_1/Minimum\n",
      "clip_by_value_1/y\n",
      "clip_by_value_1\n",
      "clip_by_value_2/Minimum/y\n",
      "clip_by_value_2/Minimum\n",
      "clip_by_value_2/y\n",
      "clip_by_value_2\n",
      "clip_by_value_3/Minimum/y\n",
      "clip_by_value_3/Minimum\n",
      "clip_by_value_3/y\n",
      "clip_by_value_3\n",
      "clip_by_value_4/Minimum/y\n",
      "clip_by_value_4/Minimum\n",
      "clip_by_value_4/y\n",
      "clip_by_value_4\n",
      "clip_by_value_5/Minimum/y\n",
      "clip_by_value_5/Minimum\n",
      "clip_by_value_5/y\n",
      "clip_by_value_5\n",
      "clip_by_value_6/Minimum/y\n",
      "clip_by_value_6/Minimum\n",
      "clip_by_value_6/y\n",
      "clip_by_value_6\n",
      "clip_by_value_7/Minimum/y\n",
      "clip_by_value_7/Minimum\n",
      "clip_by_value_7/y\n",
      "clip_by_value_7\n",
      "clip_by_value_8/Minimum/y\n",
      "clip_by_value_8/Minimum\n",
      "clip_by_value_8/y\n",
      "clip_by_value_8\n",
      "clip_by_value_9/Minimum/y\n",
      "clip_by_value_9/Minimum\n",
      "clip_by_value_9/y\n",
      "clip_by_value_9\n",
      "clip_by_value_10/Minimum/y\n",
      "clip_by_value_10/Minimum\n",
      "clip_by_value_10/y\n",
      "clip_by_value_10\n",
      "clip_by_value_11/Minimum/y\n",
      "clip_by_value_11/Minimum\n",
      "clip_by_value_11/y\n",
      "clip_by_value_11\n",
      "GradientDescent/learning_rate\n",
      "GradientDescent/update_hidden1/kernel/ApplyGradientDescent\n",
      "GradientDescent/update_hidden1/bias/ApplyGradientDescent\n",
      "GradientDescent/update_hidden2/kernel/ApplyGradientDescent\n",
      "GradientDescent/update_hidden2/bias/ApplyGradientDescent\n",
      "GradientDescent/update_hidden3/kernel/ApplyGradientDescent\n",
      "GradientDescent/update_hidden3/bias/ApplyGradientDescent\n",
      "GradientDescent/update_hidden4/kernel/ApplyGradientDescent\n",
      "GradientDescent/update_hidden4/bias/ApplyGradientDescent\n",
      "GradientDescent/update_hidden5/kernel/ApplyGradientDescent\n",
      "GradientDescent/update_hidden5/bias/ApplyGradientDescent\n",
      "GradientDescent/update_outputs/kernel/ApplyGradientDescent\n",
      "GradientDescent/update_outputs/bias/ApplyGradientDescent\n",
      "GradientDescent\n",
      "eval/in_top_k/InTopKV2/k\n",
      "eval/in_top_k/InTopKV2\n",
      "eval/Cast\n",
      "eval/Const\n",
      "eval/accuracy\n",
      "init\n",
      "save/filename/input\n",
      "save/filename\n",
      "save/Const\n",
      "save/SaveV2/tensor_names\n",
      "save/SaveV2/shape_and_slices\n",
      "save/SaveV2\n",
      "save/control_dependency\n",
      "save/RestoreV2/tensor_names\n",
      "save/RestoreV2/shape_and_slices\n",
      "save/RestoreV2\n",
      "save/Assign\n",
      "save/Assign_1\n",
      "save/Assign_2\n",
      "save/Assign_3\n",
      "save/Assign_4\n",
      "save/Assign_5\n",
      "save/Assign_6\n",
      "save/Assign_7\n",
      "save/Assign_8\n",
      "save/Assign_9\n",
      "save/Assign_10\n",
      "save/Assign_11\n",
      "save/restore_all\n",
      "eval_1/in_top_k/InTopKV2/k\n",
      "eval_1/in_top_k/InTopKV2\n",
      "eval_1/Cast\n",
      "eval_1/Const\n",
      "eval_1/accuracy\n",
      "init_1\n",
      "save_1/filename/input\n",
      "save_1/filename\n",
      "save_1/Const\n",
      "save_1/SaveV2/tensor_names\n",
      "save_1/SaveV2/shape_and_slices\n",
      "save_1/SaveV2\n",
      "save_1/control_dependency\n",
      "save_1/RestoreV2/tensor_names\n",
      "save_1/RestoreV2/shape_and_slices\n",
      "save_1/RestoreV2\n",
      "save_1/Assign\n",
      "save_1/Assign_1\n",
      "save_1/Assign_2\n",
      "save_1/Assign_3\n",
      "save_1/Assign_4\n",
      "save_1/Assign_5\n",
      "save_1/Assign_6\n",
      "save_1/Assign_7\n",
      "save_1/Assign_8\n",
      "save_1/Assign_9\n",
      "save_1/Assign_10\n",
      "save_1/Assign_11\n",
      "save_1/restore_all\n"
     ]
    }
   ],
   "source": [
    "# we need a handle on all the operations we need for the training.\n",
    "\n",
    "for op in tf.get_default_graph().get_operations():\n",
    "    print(op.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from tensorflow_graph_in_jupyter import show_graph\n",
    "# need to define this firts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.get_default_graph().get_tensor_by_name(\"X:0\")\n",
    "y = tf.get_default_graph().get_tensor_by_name(\"y:0\")\n",
    "\n",
    "accuracy = tf.get_default_graph().get_tensor_by_name(\"eval/accuracy:0\")\n",
    "training_op = tf.get_default_graph().get_operation_by_name(\"GradientDescent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# another approach\n",
    "for op in (X, y, accuracy, training_op):\n",
    "    tf.add_to_collection(\"my_important_ops\",op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# such that\n",
    "X,y, accuracy,training_op=tf.get_collection(\"my_important_ops\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# starting a session\n",
    "# restoring model state \n",
    "# training on our data\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 validation accuracy 0.9644\n",
      "1 validation accuracy 0.9628\n",
      "2 validation accuracy 0.9654\n",
      "3 validation accuracy 0.9652\n",
      "4 validation accuracy 0.9642\n",
      "5 validation accuracy 0.965\n",
      "6 validation accuracy 0.9686\n",
      "7 validation accuracy 0.9686\n",
      "8 validation accuracy 0.9682\n",
      "9 validation accuracy 0.9688\n",
      "10 validation accuracy 0.97\n",
      "11 validation accuracy 0.9716\n",
      "12 validation accuracy 0.9672\n",
      "13 validation accuracy 0.97\n",
      "14 validation accuracy 0.971\n",
      "15 validation accuracy 0.9724\n",
      "16 validation accuracy 0.9718\n",
      "17 validation accuracy 0.9714\n",
      "18 validation accuracy 0.9712\n",
      "19 validation accuracy 0.971\n"
     ]
    }
   ],
   "source": [
    "# testing\n",
    "\n",
    "n_epochs = 20\n",
    "batch_size =200\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"./my_model_final.ckpt\")\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch, in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X:X_batch, y:y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X:X_valid, y:y_valid})\n",
    "        print(epoch, \"validation accuracy\", accuracy_val)\n",
    "    \n",
    "    save_path = saver.save(sess, \"./my_new_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we need to reuse only the lower layers\n",
    "# in this example we add a new 4th layer on top of a third layer\n",
    "#we will also build a new output layer the loss of this new output and an optimiser to minimise it\n",
    "# and then we save the new graph and an initilisation up\n",
    "\n",
    "reset_graph()\n",
    "\n",
    "n_hidden4 = 20\n",
    "n_outputs = 10\n",
    "\n",
    "saver = tf.train.import_meta_graph(\"./my_model_final.ckpt.meta\")\n",
    "\n",
    "X = tf.get_default_graph().get_tensor_by_name(\"X:0\")\n",
    "y = tf.get_default_graph().get_tensor_by_name(\"y:0\")\n",
    "\n",
    "hidden3 = tf.get_default_graph().get_tensor_by_name(\"dnn/hidden3/Relu:0\")\n",
    "\n",
    "new_hidden4 = tf.layers.dense(hidden3, n_hidden4, activation=tf.nn.relu, name=\"new_hidden4\")\n",
    "new_logits = tf.layers.dense(new_hidden4, n_outputs, name=\"new_outputs\")\n",
    "\n",
    "with tf.name_scope(\"new_loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=new_logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"new_eval\"):\n",
    "    correct = tf.nn.in_top_k(new_logits,y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "\n",
    "with tf.name_scope(\"new_train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "new_saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Validation accuracy:  0.916\n",
      "1 Validation accuracy:  0.932\n",
      "2 Validation accuracy:  0.9446\n",
      "3 Validation accuracy:  0.947\n",
      "4 Validation accuracy:  0.9498\n",
      "5 Validation accuracy:  0.9534\n",
      "6 Validation accuracy:  0.9542\n",
      "7 Validation accuracy:  0.9562\n",
      "8 Validation accuracy:  0.9602\n",
      "9 Validation accuracy:  0.9586\n",
      "10 Validation accuracy:  0.9626\n",
      "11 Validation accuracy:  0.9622\n",
      "12 Validation accuracy:  0.9602\n",
      "13 Validation accuracy:  0.9652\n",
      "14 Validation accuracy:  0.9648\n",
      "15 Validation accuracy:  0.9668\n",
      "16 Validation accuracy:  0.9656\n",
      "17 Validation accuracy:  0.967\n",
      "18 Validation accuracy:  0.9674\n",
      "19 Validation accuracy:  0.9668\n"
     ]
    }
   ],
   "source": [
    "# training the new model\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    saver.restore(sess, \"./my_model_final.ckpt\")\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch,y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X:X_batch, y:y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X:X_valid, y:y_valid})\n",
    "        print(epoch, \"Validation accuracy: \", accuracy_val)\n",
    "        \n",
    "        save_path = new_saver.save(sess, \"./my_new_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can reuse the parts we need in the old python code\n",
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28 #MNIST\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 50\n",
    "n_hidden3 = 50\n",
    "n_hidden4 = 20\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, name=\"hidden2\")\n",
    "    hidden3 = tf.layers.dense(hidden2, n_hidden3, activation=tf.nn.relu, name=\"hidden3\")\n",
    "    hidden4 = tf.layers.dense(hidden3, n_hidden4, activation=tf.nn.relu, name=\"hidden4\")\n",
    "    logits = tf.layers.dense(hidden4,n_outputs, name=\"outputs\") \n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y,logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy,name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Validation accuracy:  0.9022\n",
      "1 Validation accuracy:  0.933\n",
      "2 Validation accuracy:  0.943\n",
      "3 Validation accuracy:  0.947\n",
      "4 Validation accuracy:  0.952\n",
      "5 Validation accuracy:  0.9534\n",
      "6 Validation accuracy:  0.9556\n",
      "7 Validation accuracy:  0.959\n",
      "8 Validation accuracy:  0.9588\n",
      "9 Validation accuracy:  0.9606\n",
      "10 Validation accuracy:  0.9622\n",
      "11 Validation accuracy:  0.9622\n",
      "12 Validation accuracy:  0.9638\n",
      "13 Validation accuracy:  0.9658\n",
      "14 Validation accuracy:  0.9662\n",
      "15 Validation accuracy:  0.9662\n",
      "16 Validation accuracy:  0.9672\n",
      "17 Validation accuracy:  0.9672\n",
      "18 Validation accuracy:  0.9682\n",
      "19 Validation accuracy:  0.9674\n"
     ]
    }
   ],
   "source": [
    "# we have to create one Saver to restore the pretrained moodel\n",
    "# and another saver for new model\n",
    "\n",
    "reuse_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,\n",
    "                               scope=\"hidden[123]\")# regex to get layers 1 to 3\n",
    "restore_saver = tf.train.Saver(reuse_vars) # restore layer 1-3\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    restore_saver.restore(sess, \"./my_model_final.ckpt\")\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict = {X:X_batch, y:y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X:X_valid, y:y_valid})\n",
    "        print(epoch, \"Validation accuracy: \", accuracy_val)\n",
    "    save_path = saver.save(sess, \"./my_new_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reusing models from other frameworks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# following code shows how you would copy the weight and biases from the \n",
    "# first hidden layer of  a model tranined using another framework\n",
    "\n",
    "reset_graph()\n",
    "n_inputs = 2\n",
    "n_hidden1 = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 61.  83. 105.]]\n"
     ]
    }
   ],
   "source": [
    "original_w = [[1.,2.,3.], [4.,5.,6.]] # weights from the other framework\n",
    "original_b = [7.,8.,9.] # loading the biases fromother frameworks\n",
    "\n",
    "X= tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")\n",
    "\n",
    "graph = tf.get_default_graph()\n",
    "assign_kernel = graph.get_operation_by_name(\"hidden1/kernel/Assign\")\n",
    "assign_bias = graph.get_operation_by_name(\"hidden1/bias/Assign\")\n",
    "init_kernel = assign_kernel.inputs[1]\n",
    "init_bias = assign_bias.inputs[1]\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init, feed_dict={init_kernel:original_w, init_bias: original_b})\n",
    "    print(hidden1.eval(feed_dict={X: [[10.0,11.0]]}))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 61.  83. 105.]]\n"
     ]
    }
   ],
   "source": [
    "# the book uses a more verbose version\n",
    "\n",
    "reset_graph()\n",
    "\n",
    "n_inputs =2\n",
    "n_hidden1 = 3\n",
    "\n",
    "original_w = [[1.,2.,3.], [4.,5.,6.]]\n",
    "original_b = [7.,8.,9.]\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs),name=\"X\")\n",
    "hidden1 = tf.layers.dense(X, n_hidden1,activation=tf.nn.relu, name=\"hidden1\")\n",
    "\n",
    "\n",
    "with tf.variable_scope(\"\", default_name=\"\", reuse=True):\n",
    "    hidden1_weights = tf.get_variable(\"hidden1/kernel\")\n",
    "    hidden1_biases = tf.get_variable(\"hidden1/bias\")\n",
    "\n",
    "# creating dedicated placeholder and assignment nodes\n",
    "original_weights = tf.placeholder(tf.float32, shape=(n_inputs, n_hidden1))\n",
    "original_biases = tf.placeholder(tf.float32, shape=n_hidden1)\n",
    "assign_hidden1_weights = tf.assign(hidden1_weights, original_weights)\n",
    "assign_hidden1_biases = tf.assign(hidden1_biases, original_biases)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    sess.run(assign_hidden1_weights,feed_dict={original_weights: original_w})\n",
    "    sess.run(assign_hidden1_biases, feed_dict={original_biases: original_b})\n",
    "    print(hidden1.eval(feed_dict={X: [[10.0,11.0]]}))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'hidden1/kernel:0' shape=(2, 3) dtype=float32_ref>,\n",
       " <tf.Variable 'hidden1/bias:0' shape=(3,) dtype=float32_ref>]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we could have gtten the handle iusin getcollection and specifying scope\n",
    "tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=\"hidden1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'hidden1/kernel:0' shape=(2, 3) dtype=float32_ref>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#or the get_tensor_by_name() method\n",
    "tf.get_default_graph().get_tensor_by_name(\"hidden1/kernel:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'hidden1/bias:0' shape=(3,) dtype=float32_ref>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.get_default_graph().get_tensor_by_name(\"hidden1/bias:0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Freezing the lower layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 50\n",
    "n_hidden3 = 50\n",
    "n_hidden4 = 20\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\" )\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, name=\"hidden2\")\n",
    "    hidden3 = tf.layers.dense(hidden2, n_hidden3, activation=tf.nn.relu, name=\"hidden3\")\n",
    "    hidden4 = tf.layers.dense(hidden3, n_hidden4, activation=tf.nn.relu, name=\"hidden4\")\n",
    "    logits  = tf.layers.dense(hidden4, n_outputs, name=\"outputs\")\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32),name=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    train_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,\n",
    "                                   scope = \"hidden[34]| outputs\")\n",
    "    training_op = optimizer.minimize(loss, var_list = train_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Validation accuracy:  0.7462\n",
      "1 Validation accuracy:  0.8754\n",
      "2 Validation accuracy:  0.9056\n",
      "3 Validation accuracy:  0.9168\n",
      "4 Validation accuracy:  0.9234\n",
      "5 Validation accuracy:  0.9282\n",
      "6 Validation accuracy:  0.9336\n",
      "7 Validation accuracy:  0.9354\n",
      "8 Validation accuracy:  0.9364\n",
      "9 Validation accuracy:  0.9398\n",
      "10 Validation accuracy:  0.9398\n",
      "11 Validation accuracy:  0.9418\n",
      "12 Validation accuracy:  0.9426\n",
      "13 Validation accuracy:  0.9442\n",
      "14 Validation accuracy:  0.946\n",
      "15 Validation accuracy:  0.9458\n",
      "16 Validation accuracy:  0.948\n",
      "17 Validation accuracy:  0.9478\n",
      "18 Validation accuracy:  0.9498\n",
      "19 Validation accuracy:  0.9464\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 20\n",
    "batch_size = 200\n",
    "\n",
    "reuse_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,\n",
    "                               scope=\"hidden[123]\")\n",
    "restore_saver = tf.train.Saver(reuse_vars)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    restore_saver.restore(sess, \"./my_model_final.ckpt\")\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X:X_batch, y:y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X:X_valid, y:y_valid})\n",
    "        print(epoch, \"Validation accuracy: \", accuracy_val)\n",
    "    \n",
    "    save_path = saver.save(sess, \"./my_new_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### faster optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "## momentum \n",
    "optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate,\n",
    "                                       momentum=0.9)\n",
    "\n",
    "# Nestrov Accelerated Gradient\n",
    "optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate,\n",
    "                                      momentum=0.9,use_nesterov=True)\n",
    "\n",
    "#AdaGrad\n",
    "optimizer = tf.train.AdagradOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "#RMS_prop\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate,\n",
    "                                      momentum=0.9, decay=0.9, epsilon=1e-10 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adam optimisation\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning rate scheduling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 50\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, name=\"hidden2\")\n",
    "    logits = tf.layers.dense(hidden2, n_outputs, name=\"outputs\")\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"train\"):\n",
    "    initial_learning_rate = 0.1\n",
    "    decay_steps = 10000\n",
    "    decay_rate = 1/10\n",
    "    global_step = tf.Variable(0, trainable=False, name=\"global_step\")\n",
    "    learning_rate = tf.train.exponential_decay(initial_learning_rate,\n",
    "                                               global_step,\n",
    "                                               decay_steps, decay_rate)\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate, momentum=0.9)\n",
    "    training_op = optimizer.minimize(loss, global_step=global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 validation accuracy:  0.9566\n",
      "1 validation accuracy:  0.9724\n",
      "2 validation accuracy:  0.9764\n",
      "3 validation accuracy:  0.9784\n",
      "4 validation accuracy:  0.9838\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 5\n",
    "batch_size = 50\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X:X_batch, y:y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X:X_valid, y:y_valid})\n",
    "        print(epoch, \"validation accuracy: \", accuracy_val)\n",
    "    \n",
    "    save_path = saver.save(sess, \"./my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### l1 and l2 regularisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manual implementation\n",
    "\n",
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28\n",
    "n_hidden1 = 300\n",
    "n_outputs =10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1,activation=tf.nn.relu, name=\"hidden1\")\n",
    "    logits = tf.layers.dense(hidden1, n_outputs, name=\"outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next we get a handle on layer weights and then compute thte total loss which\n",
    "# is equal to the usal cros entropy loss and the l1 loss computed on\n",
    "# the absolute value of weights\n",
    "\n",
    "W1 = tf.get_default_graph().get_tensor_by_name(\"hidden1/kernel:0\")\n",
    "W2 = tf.get_default_graph().get_tensor_by_name(\"outputs/kernel:0\")\n",
    "\n",
    "scale = 0.001\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y,\n",
    "                                                              logits=logits)\n",
    "    base_loss = tf.reduce_mean(xentropy, name=\"avg_xentropy\")\n",
    "    reg_losses = tf.reduce_sum(tf.abs(W1)) + tf.reduce_sum(tf.abs(W2))\n",
    "    loss = tf.add(base_loss, scale * reg_losses, name=\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy= tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "learning_rate = 0.01\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 validation accuracy:  0.831\n",
      "1 validation accuracy:  0.871\n",
      "2 validation accuracy:  0.8838\n",
      "3 validation accuracy:  0.8934\n",
      "4 validation accuracy:  0.8966\n",
      "5 validation accuracy:  0.8988\n",
      "6 validation accuracy:  0.9016\n",
      "7 validation accuracy:  0.9044\n",
      "8 validation accuracy:  0.9058\n",
      "9 validation accuracy:  0.906\n",
      "10 validation accuracy:  0.9068\n",
      "11 validation accuracy:  0.9054\n",
      "12 validation accuracy:  0.907\n",
      "13 validation accuracy:  0.9084\n",
      "14 validation accuracy:  0.9088\n",
      "15 validation accuracy:  0.9064\n",
      "16 validation accuracy:  0.9066\n",
      "17 validation accuracy:  0.9066\n",
      "18 validation accuracy:  0.9066\n",
      "19 validation accuracy:  0.9052\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 20\n",
    "batch_size =200\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X:X_batch, y:y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X:X_valid, y:y_valid})\n",
    "        print(epoch, \"validation accuracy: \", accuracy_val)\n",
    "        \n",
    "    save_path = saver.save(sess, \"./my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternatively we can pass regularisation function to the tf.layers.dense()\n",
    "\n",
    "reset_graph()\n",
    "n_inputs=28 *28\n",
    "n_hidden1 =300\n",
    "n_hidden2 =50\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "# we will use the Pythons partial function to avoid repeating same arguments\n",
    "from functools import partial\n",
    "\n",
    "# kernel regulariser argumetn \n",
    "scale =0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "my_dense_layer = partial(\n",
    "    tf.layers.dense, activation=tf.nn.relu,\n",
    "    kernel_regularizer=tf.contrib.layers.l1_regularizer(scale)\n",
    ")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = my_dense_layer(X, n_hidden1, name=\"hidden1\")\n",
    "    hidden2 = my_dense_layer(hidden1, n_hidden2, name=\"hidden2\")\n",
    "    logits = my_dense_layer(hidden2, n_outputs, activation=None,\n",
    "                           name=\"outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regularisation loss\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "        labels =y, logits=logits\n",
    "    )\n",
    "    base_loss= tf.reduce_mean(xentropy, name=\"avg_xentropy\")\n",
    "    reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "    loss = tf.add_n([base_loss] + reg_losses, name=\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"eval\"):\n",
    "    # what is this\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "\n",
    "learning_rate =0.01\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Validation accuracy:  0.8274\n",
      "1 Validation accuracy:  0.8766\n",
      "2 Validation accuracy:  0.8952\n",
      "3 Validation accuracy:  0.9016\n",
      "4 Validation accuracy:  0.908\n",
      "5 Validation accuracy:  0.9096\n",
      "6 Validation accuracy:  0.9124\n",
      "7 Validation accuracy:  0.9154\n",
      "8 Validation accuracy:  0.9178\n",
      "9 Validation accuracy:  0.919\n",
      "10 Validation accuracy:  0.92\n",
      "11 Validation accuracy:  0.9224\n",
      "12 Validation accuracy:  0.9212\n",
      "13 Validation accuracy:  0.9228\n",
      "14 Validation accuracy:  0.9222\n",
      "15 Validation accuracy:  0.9218\n",
      "16 Validation accuracy:  0.9218\n",
      "17 Validation accuracy:  0.9228\n",
      "18 Validation accuracy:  0.9216\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 20\n",
    "batch_size =200\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X:X_batch, y:y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X:X_valid, y:y_valid})\n",
    "        print(epoch, \"Validation accuracy: \", accuracy_val)\n",
    "    save_path = saver.save(sess, \"./my_model_final.ckpt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf_gpu)",
   "language": "python",
   "name": "tf_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
