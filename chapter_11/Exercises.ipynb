{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creatinga DNN with five hidden layers of 100 neuron each\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "def reset_graph():\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(42)\n",
    "    np.random.seed(42)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct: Tensor(\"eval/in_top_k/InTopKV2:0\", shape=(?,), dtype=bool)\n"
     ]
    }
   ],
   "source": [
    "# reset_graph()\n",
    "\n",
    "# n_inputs =28*28\n",
    "# n_hidden1 = 300\n",
    "# n_hidden2 = 150\n",
    "# n_hidden3 = 50\n",
    "# n_outputs = 10\n",
    "\n",
    "# X = tf.placeholder(tf.float32,shape=(None,n_inputs),name=\"X\")\n",
    "# y = tf.placeholder(tf.int32, shape=(None),name=\"y\")\n",
    "\n",
    "# def elu(z, alpha=1):\n",
    "#     return np.where(z<0, alpha * (np.exp(z) - 1), z)\n",
    "\n",
    "# with tf.name_scope(\"dnn\"):\n",
    "#     hidden1= tf.layers.dense(X, n_hidden1,activation=tf.nn.elu,name=\"hidden1\")\n",
    "#     hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.elu, name=\"hidden2\")\n",
    "#     hidden3 = tf.layers.dense(hidden2, n_hidden3, activation=tf.nn.elu,name=\"hidden3\")\n",
    "#     logits = tf.layers.dense(hidden3, n_outputs, name=\"outputs\"\n",
    "#                             )\n",
    "\n",
    "# with tf.name_scope(\"loss\"):\n",
    "#     xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "#     loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "# learning_rate = 0.01\n",
    "# with tf.name_scope(\"train\"):\n",
    "#     optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "#     training_op = optimizer.minimize(loss)\n",
    "                   \n",
    "# with tf.name_scope(\"eval\"):\n",
    "#     correct = tf.nn.in_top_k(logits,y, 1)\n",
    "#     print(\"correct:\",correct)\n",
    "#     accuracy = tf.reduce_mean(tf.cast(correct,tf.float32))\n",
    "    \n",
    "# init = tf.global_variables_initializer()\n",
    "# saver = tf.train.Saver()\n",
    "# n_epochs = 40\n",
    "# batch_size = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the dataset tf.examples.tutorials.mnist is depreciated\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "X_train = X_train.astype(np.float32).reshape(-1,28 *28)/ 255.0\n",
    "X_test = X_test.astype(np.float32).reshape(-1, 28*28)/ 255.0\n",
    "\n",
    "y_train = y_train.astype(np.int32)\n",
    "y_test = y_test.astype(np.int32)\n",
    "\n",
    "X_valid, X_train = X_train[:5000], X_train[5000:]\n",
    "y_valid, y_train = y_train[:5000], y_train[5000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "he_init = tf.variance_scaling_initializer()\n",
    "\n",
    "def dnn(inputs, n_hidden_layers=5, n_neurons=100, name=None,\n",
    "        activation=tf.nn.elu, initializer=he_init):\n",
    "    with tf.variable_scope(name, \"dnn\"):\n",
    "        for layer in range(n_hidden_layers):\n",
    "            inputs = tf.layers.dense(inputs, n_neurons, activation=activation,\n",
    "                                     kernel_initializer=initializer,\n",
    "                                     name=\"hidden%d\" % (layer + 1))\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_inputs = 28 * 28\n",
    "n_outputs = 5\n",
    "reset_graph()\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32,shape=(None),name=\"y\")\n",
    "\n",
    "dnn_outputs = dnn(X)\n",
    "\n",
    "logits = tf.layers.dense(dnn_outputs, n_outputs, kernel_initializer=he_init, name=\"logits\")\n",
    "Y_proba = tf.nn.softmax(logits, name=\"Y_proba\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using adam optimization for early stopping,on digits 0 to 3 and then using transfer for 5 to 0\n",
    "# need softmaxwith 5 nerons\n",
    "# make checkpoints and so tha we can resuse the mdoel later\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "training_op = optimizer.minimize(loss, name=\"training_op\")\n",
    "\n",
    "correct =tf.nn.in_top_k(logits, y, 1)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating th training set validation and teseting set\n",
    "# the validation needs toimpolement early stopping\n",
    "\n",
    "X_train1 = X_train[y_train < 5]\n",
    "y_train1 = y_train[y_train < 5]\n",
    "X_valid1 = X_valid[y_valid < 5]\n",
    "y_valid1 = y_valid[y_valid < 5]\n",
    "X_test1  = X_test[y_test < 5]\n",
    "y_test1 = y_test[y_test < 5] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 current loss:  0.17804934 best loss: 0.17804934 95.03518342971802\n",
      "1 current loss:  0.119493365 best loss: 0.119493365 96.71618342399597\n",
      "2 current loss:  0.09292569 best loss: 0.09292569 98.31900000572205\n",
      "3 current loss:  0.26109308 best loss: 0.09292569 94.64425444602966\n",
      "4 current loss:  0.44506058 best loss: 0.09292569 92.57231950759888\n",
      "5 current loss:  0.20861483 best loss: 0.09292569 96.87255620956421\n",
      "6 current loss:  0.10406159 best loss: 0.09292569 98.00625443458557\n",
      "7 current loss:  0.17517713 best loss: 0.09292569 96.71618342399597\n",
      "8 current loss:  1.63139 best loss: 0.09292569 22.009381651878357\n",
      "9 current loss:  1.7801597 best loss: 0.09292569 22.009381651878357\n",
      "10 current loss:  1.6627414 best loss: 0.09292569 18.725566565990448\n",
      "11 current loss:  1.630582 best loss: 0.09292569 20.914776623249054\n",
      "12 current loss:  1.6505957 best loss: 0.09292569 20.914776623249054\n",
      "13 current loss:  1.6361288 best loss: 0.09292569 20.914776623249054\n",
      "14 current loss:  1.718902 best loss: 0.09292569 19.077403843402863\n",
      "15 current loss:  1.6824518 best loss: 0.09292569 19.27286982536316\n",
      "16 current loss:  1.6753669 best loss: 0.09292569 18.725566565990448\n",
      "17 current loss:  1.645803 best loss: 0.09292569 19.077403843402863\n",
      "18 current loss:  1.722334 best loss: 0.09292569 22.009381651878357\n",
      "19 current loss:  1.6564153 best loss: 0.09292569 22.009381651878357\n",
      "20 current loss:  1.643529 best loss: 0.09292569 18.725566565990448\n",
      "21 current loss:  1.6442338 best loss: 0.09292569 19.27286982536316\n",
      "22 current loss:  1.6900325 best loss: 0.09292569 18.725566565990448\n",
      "Early stopping\n",
      "Final accuracy: 98.42382073402405\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 1000\n",
    "batch_size =20\n",
    "\n",
    "max_checks_without_progress = 20\n",
    "checks_without_progress = 0\n",
    "best_loss = np.infty\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        rnd_idx = np.random.permutation(len(X_train1))\n",
    "        for rnd_indices in np.array_split(rnd_idx,len(X_train1)//batch_size):\n",
    "            X_batch, y_batch = X_train1[rnd_indices], y_train1[rnd_indices]\n",
    "            sess.run(training_op, feed_dict={X:X_batch, y:y_batch})\n",
    "        loss_val, acc_val = sess.run([loss,accuracy], feed_dict={X:X_valid1, y:y_valid1})\n",
    "        if loss_val < best_loss:\n",
    "            save_path = saver.save(sess, \"./my_mnist_0_to_4.ckpt\")\n",
    "            best_loss = loss_val\n",
    "            checks_without_progress = 0\n",
    "        else :\n",
    "            checks_without_progress +=1\n",
    "            if checks_without_progress > max_checks_without_progress:\n",
    "                print(\"Early stopping\")\n",
    "                break\n",
    "        print(epoch,\"current loss: \", loss_val, \"best loss:\", best_loss, acc_val * 100)\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"./my_mnist_0_to_4.ckpt\")\n",
    "    acc_test = accuracy.eval(feed_dict={X:X_test1, y:y_test1})\n",
    "    print(\"Final accuracy:\", acc_test * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## tuning the hyperparameters using cross validation\n",
    "## __init__() method just creates the instance variables\n",
    "## fit() method creates the graph, starts the session and trains\n",
    "## build_graoh() to build eht egraph\n",
    "## _dnn() for hidden layers\n",
    "## fit () for validation set and implementing eaarly stopping\n",
    "## it keeps the session open \n",
    "#  predic_proba for using trained model to predict class probabilirtis\n",
    "# predictforcalling predcit proba and returing the class with highest probablility\n",
    "\n",
    "from sklearn.base import BaseEstimator, ClassfierMixin\n",
    "from sklearn.exeptions import NotFittedError\n",
    "\n",
    "class DNNClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, n_hidden_layers=5, n_neirons=100, optimizer_class=tf.train.AdamOptimizer,\n",
    "                 learning_rate = 0.01, batch_szie=20, activation=tf.nn.elu, initializer=he_init,\n",
    "                 batch_norm_momentum = None,dropout_rate=None, random_state=None):\n",
    "        self.n_hidden_layers = n_hidden_layers\n",
    "        self.n_neurons = n_neurons\n",
    "        self.optimizer_class = optimizer_class\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.activation = activation\n",
    "        self.initializer = initializer\n",
    "        self.batch_norm_momentum = batch_norm_momentum\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.random_state = random_state\n",
    "        self._session = None\n",
    "    \n",
    "    def _dnn(self, inputs):\n",
    "        # build hidden layer and batch normalisation\n",
    "        if layer in range(self.n_hidden_layers):\n",
    "            if self.dropout_rate:\n",
    "                inputs=tf.layers.dropout(inputs, self.dropout_rate, training=self._training)\n",
    "            inputs = tf.layer.dense(inputs, self.n_neurons,\n",
    "                                    kernel_initializer=self.initializer,\n",
    "                                    name=\"hidden%d\" % (layer + 1))\n",
    "            if (self.batch_norm_momentum):\n",
    "                inputs = self.activation(inputs, name=\"hidden%d_out\" % (layer + 1))\n",
    "        return inputs\n",
    "    \n",
    "    def _build_graph(self, n_inputs, n_outputs):\n",
    "        if self.random_state is i=not None:\n",
    "            tf.set_random_seed(self.random_state)\n",
    "            np.random.seed(self.random_state)\n",
    "        \n",
    "        X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "        y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "        \n",
    "        if self.batch_norm_momrntum or self.dropour_rate:\n",
    "            self._training = tf.placeholder_with_default(false, shape=(), name=\"training\")\n",
    "        else:\n",
    "            self._training = None\n",
    "        \n",
    "        dnn_outputs = self._dnn(X)\n",
    "        \n",
    "        logits =tf.layers.dense(dnn_outputs, n_outputs, kernel_initializer=he_inti, name=\"logits\")\n",
    "        Y_proba = tf.nn.softmax(logits, name=\"Y_proba\")\n",
    "        \n",
    "        xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "        loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "        \n",
    "        optimizer = self.optimizer_calss(learning_rate=self.learning_rate)\n",
    "        training_op = optimizer.minimize(loss)\n",
    "        \n",
    "        correct = tf.nn.in_top_k(logits, y, 1)\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "        \n",
    "        init = tf.gloabl_variables_initializer()\n",
    "        saver = tf.train.Saver()\n",
    "        \n",
    "        # making importatnt operations avaialble through class\n",
    "        \n",
    "        self._X , self._y = X, y\n",
    "        self._Y_proba, self._loss = Y_proba, loss\n",
    "        self._training_op, self._accuracy = training_op, accuracy\n",
    "        self._init,self._saver = init, saver\n",
    "        \n",
    "    def close_session(self):\n",
    "        if self._session:\n",
    "            self.session.close()\n",
    "            \n",
    "    def _get_model_params(self):\n",
    "        with self._graph.as_default():\n",
    "            gvats = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\n",
    "        return (gvat.op.name: value for gvat, value in zip(gvars,self._session.run(gvars)))\n",
    "    \n",
    "    def"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf_gpu)",
   "language": "python",
   "name": "tf_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
